<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Aide-mÃ©moire NLP Python : workflows pratiques avec Spacy, Transformers, BERTopic et LangChain.">
    <title>NLP Python - IT Cheatsheets</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body class="dark-theme text-slate-200">

    <!-- Header -->
    <header class="bg-slate-900/50 border-b border-white/5 py-8 px-4 relative overflow-hidden header-glow">
        <div class="max-w-4xl mx-auto relative z-10">
            <div class="flex items-center justify-between mb-4">
                <a href="../index.html" class="nav-back inline-flex items-center text-slate-400 hover:text-sky-400 transition">
                    <i class="fas fa-arrow-left mr-2"></i>Retour
                </a>
                <a href="../index.html" class="inline-flex items-center text-slate-400 hover:text-sky-400 transition">
                    <i class="fas fa-home mr-2"></i>Accueil
                </a>
            </div>
            <div class="text-center">
                <div class="inline-flex items-center justify-center w-16 h-16 rounded-xl bg-violet-500/20 mb-4 icon-glow">
                    <i class="fas fa-language text-3xl text-violet-400"></i>
                </div>
                <h1 class="text-3xl font-bold mb-2 gradient-text">NLP avec Python</h1>
                <p class="text-slate-400">Workflows pratiques : Spacy, Transformers, BERTopic, LangChain</p>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="max-w-4xl mx-auto p-4 relative z-10">
        <div class="mb-8 relative">
            <input type="text" id="searchInput" placeholder="Rechercher un workflow (ex: sentiment, topic, RAG)..."
                   class="search-dark w-full p-4 pl-12 rounded-lg outline-none transition">
            <i class="fas fa-search absolute left-4 top-1/2 transform -translate-y-1/2 text-slate-500"></i>
        </div>
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6" id="categoriesGrid"></div>
    </main>

    <!-- Modal -->
    <div id="detailModal" class="fixed inset-0 bg-black/70 hidden items-center justify-center z-50 p-4 modal-overlay" onclick="closeModal(event)">
        <div class="modal-content-dark rounded-xl max-w-2xl w-full max-h-[90vh] overflow-y-auto shadow-2xl modal-content" onclick="event.stopPropagation()">
            <div id="modalContent"></div>
        </div>
    </div>

    <!-- Footer -->
    <footer class="border-t border-white/5 text-center text-slate-500 py-8 text-sm relative z-10">
        <p>Â© 2026 - Dr FENOHASINA Toto Jean Felicien</p>
    </footer>

    <!-- DonnÃ©es NLP -->
    <script>
        const cheatsheetData = [
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 1: INSTALLATION & CONFIGURATION
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'setup',
                title: 'ğŸš€ Installation & Configuration',
                icon: 'fa-download',
                color: 'border-l-4 border-blue-500',
                commands: [
                    {
                        cmd: 'Installer Spacy et charger un modÃ¨le',
                        desc: 'Framework NLP rapide pour le franÃ§ais',
                        details: {
                            explanation: 'Spacy est un framework NLP industriel. Les modÃ¨les franÃ§ais incluent sm (petit), md (moyen avec vectors) et lg (grand).',
                            syntax: 'pip install spacy\npython -m spacy download fr_core_news_md',
                            options: [
                                { flag: 'fr_core_news_sm', desc: 'Petit modÃ¨le (~15MB), rapide' },
                                { flag: 'fr_core_news_md', desc: 'Moyen (~50MB), avec word vectors' },
                                { flag: 'fr_core_news_lg', desc: 'Grand (~550MB), meilleure prÃ©cision' },
                                { flag: 'en_core_web_sm', desc: 'ModÃ¨le anglais' }
                            ],
                            examples: [
                                { code: `# Installation
pip install spacy

# TÃ©lÃ©charger le modÃ¨le franÃ§ais (moyen)
python -m spacy download fr_core_news_md

# Utilisation
import spacy
nlp = spacy.load("fr_core_news_md")

# Traiter un texte
doc = nlp("Paris est la capitale de la France.")
for token in doc:
    print(f"{token.text}: {token.pos_}")`, desc: 'Installation et premier usage' }
                            ],
                            tips: ['Les modÃ¨les _md et _lg incluent des word vectors pour la similaritÃ©', 'Utilisez _sm pour la vitesse, _lg pour la prÃ©cision'],
                            warnings: ['TÃ©lÃ©chargez le modÃ¨le AVANT de l\'utiliser dans Python']
                        }
                    },
                    {
                        cmd: 'Installer Transformers (Hugging Face)',
                        desc: 'AccÃ¨s aux modÃ¨les BERT, GPT, CamemBERT...',
                        details: {
                            explanation: 'La bibliothÃ¨que transformers de Hugging Face donne accÃ¨s Ã  des milliers de modÃ¨les prÃ©-entraÃ®nÃ©s.',
                            syntax: 'pip install transformers torch',
                            options: [
                                { flag: 'transformers', desc: 'BibliothÃ¨que principale' },
                                { flag: 'torch', desc: 'Backend PyTorch (recommandÃ©)' },
                                { flag: 'tensorflow', desc: 'Alternative Ã  PyTorch' },
                                { flag: 'accelerate', desc: 'Optimisation GPU/multi-GPU' }
                            ],
                            examples: [
                                { code: `# Installation de base
pip install transformers torch

# Avec accÃ©lÃ©ration GPU
pip install transformers torch accelerate

# Test rapide
from transformers import pipeline

# Pipeline sentiment (tÃ©lÃ©charge le modÃ¨le automatiquement)
classifier = pipeline("sentiment-analysis")
result = classifier("I love this!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.99}]`, desc: 'Installation et test' }
                            ],
                            tips: ['Premier appel Ã  pipeline() tÃ©lÃ©charge le modÃ¨le (~500MB)', 'Utilisez device=0 pour GPU si disponible'],
                            warnings: ['Les modÃ¨les sont volumineux, prÃ©voyez de l\'espace disque']
                        }
                    },
                    {
                        cmd: 'Installer les packages NLP essentiels',
                        desc: 'BERTopic, Sentence-Transformers, LangChain...',
                        details: {
                            explanation: 'Packages complÃ©mentaires pour des tÃ¢ches NLP avancÃ©es.',
                            syntax: 'pip install bertopic sentence-transformers langchain',
                            options: [
                                { flag: 'bertopic', desc: 'Topic modeling moderne' },
                                { flag: 'sentence-transformers', desc: 'Embeddings de phrases' },
                                { flag: 'langchain', desc: 'Framework RAG et LLM' },
                                { flag: 'nltk', desc: 'Toolkit NLP classique' }
                            ],
                            examples: [
                                { code: `# Installation complÃ¨te NLP
pip install spacy transformers torch
pip install sentence-transformers bertopic
pip install langchain langchain-community
pip install nltk

# TÃ©lÃ©charger ressources NLTK
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')`, desc: 'Setup complet' },
                                { code: `# Installation minimale (analyse de texte)
pip install spacy transformers torch
python -m spacy download fr_core_news_md`, desc: 'Setup minimal' }
                            ],
                            tips: ['Installez progressivement selon vos besoins'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Configurer GPU / CPU',
                        desc: 'AccÃ©lÃ©rer les traitements avec CUDA',
                        details: {
                            explanation: 'Les modÃ¨les Transformers sont beaucoup plus rapides sur GPU. VÃ©rifiez votre configuration.',
                            syntax: 'torch.cuda.is_available()',
                            options: [
                                { flag: 'device=0', desc: 'Utiliser le premier GPU' },
                                { flag: 'device=-1', desc: 'Forcer CPU' },
                                { flag: 'device="mps"', desc: 'GPU Apple Silicon (M1/M2)' }
                            ],
                            examples: [
                                { code: `import torch

# VÃ©rifier la disponibilitÃ© GPU
print(f"CUDA disponible: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# DÃ©finir le device pour les pipelines
device = 0 if torch.cuda.is_available() else -1

# Utilisation avec Transformers
from transformers import pipeline
classifier = pipeline("sentiment-analysis", device=device)`, desc: 'Configuration GPU' },
                                { code: `# Apple Silicon (M1/M2)
import torch
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Device: {device}")`, desc: 'Configuration Mac M1/M2' }
                            ],
                            tips: ['GPU accÃ©lÃ¨re 10-100x les traitements', 'Apple M1/M2 supportent MPS'],
                            warnings: ['VÃ©rifiez la compatibilitÃ© CUDA avec votre version de PyTorch']
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 2: ANALYSER UN TEXTE (FONDAMENTAUX)
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'text-analysis',
                title: 'ğŸ“ Analyser un texte',
                icon: 'fa-magnifying-glass',
                color: 'border-l-4 border-emerald-500',
                commands: [
                    {
                        cmd: 'Tokeniser et lemmatiser un texte',
                        desc: 'DÃ©couper en mots et trouver les formes canoniques',
                        details: {
                            explanation: 'La tokenisation dÃ©coupe le texte en unitÃ©s (tokens). La lemmatisation rÃ©duit les mots Ã  leur forme de base (lemme) : "mangeons" â†’ "manger".',
                            syntax: 'doc = nlp(texte)\nfor token in doc: token.lemma_',
                            options: [
                                { flag: '.text', desc: 'Texte original du token' },
                                { flag: '.lemma_', desc: 'Forme canonique (lemme)' },
                                { flag: '.is_stop', desc: 'True si mot vide (le, de, et...)' },
                                { flag: '.is_punct', desc: 'True si ponctuation' }
                            ],
                            examples: [
                                { code: `import spacy
nlp = spacy.load("fr_core_news_md")

texte = "Les chats mangent des souris dans le jardin."
doc = nlp(texte)

# Tokenisation + Lemmatisation
for token in doc:
    print(f"{token.text:12} â†’ {token.lemma_:12} (stop: {token.is_stop})")

# RÃ©sultat:
# Les          â†’ le           (stop: True)
# chats        â†’ chat         (stop: False)
# mangent      â†’ manger       (stop: False)
# ...`, desc: 'Tokenisation avec Spacy' },
                                { code: `# Filtrer les stop words et la ponctuation
tokens_utiles = [
    token.lemma_.lower()
    for token in doc
    if not token.is_stop and not token.is_punct
]
print(tokens_utiles)
# ['chat', 'manger', 'souris', 'jardin']`, desc: 'Filtrer les mots utiles' }
                            ],
                            tips: ['Spacy gÃ¨re automatiquement la ponctuation et les contractions', 'Utilisez lemma_.lower() pour normaliser la casse'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Extraire les entitÃ©s (noms, lieux, dates)',
                        desc: 'Named Entity Recognition (NER)',
                        details: {
                            explanation: 'Le NER identifie et classifie les entitÃ©s nommÃ©es : personnes, organisations, lieux, dates, etc.',
                            syntax: 'for ent in doc.ents: ent.text, ent.label_',
                            options: [
                                { flag: 'PER', desc: 'Personne' },
                                { flag: 'ORG', desc: 'Organisation' },
                                { flag: 'LOC', desc: 'Lieu' },
                                { flag: 'DATE', desc: 'Date' },
                                { flag: 'MISC', desc: 'Divers' }
                            ],
                            examples: [
                                { code: `import spacy
nlp = spacy.load("fr_core_news_md")

texte = "Emmanuel Macron a rencontrÃ© Joe Biden Ã  Paris le 15 juin 2024."
doc = nlp(texte)

# Extraire les entitÃ©s
for ent in doc.ents:
    print(f"{ent.text:20} â†’ {ent.label_}")

# RÃ©sultat:
# Emmanuel Macron      â†’ PER
# Joe Biden            â†’ PER
# Paris                â†’ LOC
# 15 juin 2024         â†’ DATE`, desc: 'NER avec Spacy' },
                                { code: `# NER plus prÃ©cis avec CamemBERT
from transformers import pipeline

ner = pipeline(
    "ner",
    model="Jean-Baptiste/camembert-ner",
    aggregation_strategy="simple"
)

result = ner("Emmanuel Macron est Ã  l'Ã‰lysÃ©e.")
for entity in result:
    print(f"{entity['word']:20} â†’ {entity['entity_group']} ({entity['score']:.2f})")`, desc: 'NER avec CamemBERT (plus prÃ©cis)' },
                                { code: `# Visualiser les entitÃ©s
from spacy import displacy

# Dans Jupyter Notebook
displacy.render(doc, style="ent", jupyter=True)

# Ou serveur web local
# displacy.serve(doc, style="ent")`, desc: 'Visualisation des entitÃ©s' }
                            ],
                            tips: ['CamemBERT NER est plus prÃ©cis que Spacy pour le franÃ§ais', 'aggregation_strategy="simple" regroupe les sous-tokens'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Voir la structure grammaticale (POS tags)',
                        desc: 'Part-of-Speech tagging et dÃ©pendances',
                        details: {
                            explanation: 'Le POS tagging identifie la nature grammaticale de chaque mot (nom, verbe, adjectif...). L\'analyse de dÃ©pendances montre les relations syntaxiques.',
                            syntax: 'token.pos_, token.dep_, token.head',
                            options: [
                                { flag: '.pos_', desc: 'Part-of-speech (NOUN, VERB, ADJ...)' },
                                { flag: '.tag_', desc: 'POS tag dÃ©taillÃ©' },
                                { flag: '.dep_', desc: 'Relation de dÃ©pendance (nsubj, dobj...)' },
                                { flag: '.head', desc: 'Token parent dans l\'arbre' }
                            ],
                            examples: [
                                { code: `import spacy
nlp = spacy.load("fr_core_news_md")

doc = nlp("Le chat noir mange une souris.")

# POS Tagging
for token in doc:
    print(f"{token.text:10} {token.pos_:6} {token.dep_:10} â† {token.head.text}")

# RÃ©sultat:
# Le         DET    det        â† chat
# chat       NOUN   nsubj      â† mange
# noir       ADJ    amod       â† chat
# mange      VERB   ROOT       â† mange
# une        DET    det        â† souris
# souris     NOUN   dobj       â† mange
# .          PUNCT  punct      â† mange`, desc: 'Analyse grammaticale' },
                                { code: `# Visualiser l'arbre de dÃ©pendances
from spacy import displacy

doc = nlp("Le chat noir mange une souris.")

# Dans Jupyter
displacy.render(doc, style="dep", jupyter=True)

# Comprendre un tag
print(spacy.explain("nsubj"))  # "nominal subject"
print(spacy.explain("VERB"))   # "verb"`, desc: 'Visualisation des dÃ©pendances' }
                            ],
                            tips: ['spacy.explain("TAG") donne la signification d\'un tag', 'displacy gÃ©nÃ¨re des visualisations interactives'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Nettoyer et prÃ©traiter du texte',
                        desc: 'Supprimer URLs, caractÃ¨res spÃ©ciaux, normaliser',
                        details: {
                            explanation: 'Le prÃ©traitement prÃ©pare le texte pour l\'analyse : suppression des URLs, emails, normalisation des espaces, etc.',
                            syntax: 'import re\nre.sub(pattern, replacement, text)',
                            options: [
                                { flag: 'r"http\\S+"', desc: 'Pattern pour URLs' },
                                { flag: 'r"@\\w+"', desc: 'Pattern pour mentions' },
                                { flag: 'r"#\\w+"', desc: 'Pattern pour hashtags' },
                                { flag: 'r"\\s+"', desc: 'Espaces multiples' }
                            ],
                            examples: [
                                { code: `import re

def clean_text(text):
    """Nettoyage complet d'un texte."""
    # Minuscules
    text = text.lower()

    # Supprimer URLs
    text = re.sub(r"http\\S+|www\\S+", "", text)

    # Supprimer emails
    text = re.sub(r"\\S+@\\S+", "", text)

    # Supprimer mentions et hashtags
    text = re.sub(r"@\\w+|#\\w+", "", text)

    # Garder uniquement lettres et espaces
    text = re.sub(r"[^a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã§\\s]", " ", text)

    # Normaliser les espaces
    text = re.sub(r"\\s+", " ", text).strip()

    return text

# Test
texte = "Visitez https://example.com ! @user #NLP C'est gÃ©nial!!! ğŸ˜€"
print(clean_text(texte))
# "visitez user nlp c est gÃ©nial"`, desc: 'Fonction de nettoyage complÃ¨te' },
                                { code: `# Avec Spacy (plus intelligent)
import spacy
nlp = spacy.load("fr_core_news_md")

def clean_with_spacy(text):
    doc = nlp(text)
    tokens = [
        token.lemma_.lower()
        for token in doc
        if not token.is_stop
        and not token.is_punct
        and not token.like_url
        and not token.like_email
        and len(token.text) > 2
    ]
    return " ".join(tokens)

print(clean_with_spacy("Les chats mangent des souris!"))
# "chat manger souris"`, desc: 'Nettoyage intelligent avec Spacy' }
                            ],
                            tips: ['Combinez regex pour le nettoyage basique et Spacy pour la lemmatisation', 'Adaptez le nettoyage Ã  votre tÃ¢che'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'DÃ©tecter la langue d\'un texte',
                        desc: 'Identifier automatiquement la langue',
                        details: {
                            explanation: 'DÃ©tecte la langue d\'un texte pour choisir le bon modÃ¨le ou filtrer les donnÃ©es.',
                            syntax: 'from langdetect import detect',
                            options: [
                                { flag: 'langdetect', desc: 'Simple et rapide' },
                                { flag: 'lingua', desc: 'Plus prÃ©cis sur textes courts' },
                                { flag: 'fasttext', desc: 'TrÃ¨s rapide, 176 langues' }
                            ],
                            examples: [
                                { code: `# Avec langdetect (simple)
from langdetect import detect, detect_langs

# DÃ©tection simple
print(detect("Bonjour le monde"))  # "fr"
print(detect("Hello world"))       # "en"
print(detect("Hola mundo"))        # "es"

# Avec probabilitÃ©s
probs = detect_langs("Bonjour le monde")
print(probs)  # [fr:0.999...]`, desc: 'langdetect basique' },
                                { code: `# Avec lingua (plus prÃ©cis sur textes courts)
from lingua import Language, LanguageDetectorBuilder

# CrÃ©er le dÃ©tecteur
detector = LanguageDetectorBuilder.from_languages(
    Language.FRENCH,
    Language.ENGLISH,
    Language.SPANISH
).build()

# DÃ©tecter
lang = detector.detect_language_of("Bonjour")
print(lang)  # Language.FRENCH

# Avec confiance
confidence = detector.compute_language_confidence_values("Bonjour")
for lang, score in confidence:
    print(f"{lang.name}: {score:.2f}")`, desc: 'lingua (plus prÃ©cis)' }
                            ],
                            tips: ['pip install langdetect ou pip install lingua-language-detector', 'lingua est meilleur sur les textes courts (<50 caractÃ¨res)'],
                            warnings: ['langdetect peut Ãªtre imprÃ©cis sur des textes trÃ¨s courts']
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 3: DÃ‰TERMINER LE SENTIMENT
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'sentiment',
                title: 'â¤ï¸ DÃ©terminer le sentiment',
                icon: 'fa-heart',
                color: 'border-l-4 border-pink-500',
                commands: [
                    {
                        cmd: 'Analyser le sentiment (positif/nÃ©gatif)',
                        desc: 'Classification binaire en franÃ§ais',
                        details: {
                            explanation: 'Utilise CamemBERT fine-tunÃ© sur AllocinÃ© pour classifier le sentiment en franÃ§ais.',
                            syntax: 'classifier = pipeline("sentiment-analysis", model="tblard/tf-allocine")',
                            options: [
                                { flag: 'tblard/tf-allocine', desc: 'FranÃ§ais, binaire (pos/neg)' },
                                { flag: 'nlptown/bert-multilingual', desc: 'Multilingue, 5 Ã©toiles' },
                                { flag: 'cardiffnlp/twitter-roberta', desc: 'Anglais Twitter' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline
import torch

# Device GPU si disponible
device = 0 if torch.cuda.is_available() else -1

# Charger le modÃ¨le franÃ§ais
classifier = pipeline(
    "sentiment-analysis",
    model="tblard/tf-allocine",
    device=device
)

# Analyser un texte
result = classifier("Ce film est vraiment excellent !")
print(result)
# [{'label': 'POSITIVE', 'score': 0.98}]

# Analyser plusieurs textes
textes = [
    "J'ai adorÃ© ce restaurant !",
    "Service terrible, je ne reviendrai jamais.",
    "C'Ã©tait correct, sans plus."
]
results = classifier(textes)
for texte, res in zip(textes, results):
    print(f"{res['label']:10} ({res['score']:.2f}) - {texte[:40]}")`, desc: 'Sentiment franÃ§ais avec CamemBERT' }
                            ],
                            tips: ['tblard/tf-allocine est entraÃ®nÃ© sur des critiques de films franÃ§ais', 'Premier appel tÃ©lÃ©charge le modÃ¨le (~500MB)'],
                            warnings: ['Utilisez truncation=True pour les textes longs']
                        }
                    },
                    {
                        cmd: 'Analyser sur une Ã©chelle 1-5 Ã©toiles',
                        desc: 'Notation fine du sentiment',
                        details: {
                            explanation: 'Retourne une note de 1 Ã  5 Ã©toiles, utile pour les avis clients.',
                            syntax: 'pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")',
                            options: [
                                { flag: '1 star', desc: 'TrÃ¨s nÃ©gatif' },
                                { flag: '2 stars', desc: 'NÃ©gatif' },
                                { flag: '3 stars', desc: 'Neutre' },
                                { flag: '4 stars', desc: 'Positif' },
                                { flag: '5 stars', desc: 'TrÃ¨s positif' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# ModÃ¨le multilingue 5 Ã©toiles
classifier = pipeline(
    "sentiment-analysis",
    model="nlptown/bert-base-multilingual-uncased-sentiment"
)

# Test
textes = [
    "Absolument incroyable, le meilleur !",
    "TrÃ¨s bien, je recommande.",
    "Correct, rien de spÃ©cial.",
    "Pas terrible...",
    "Horrible, Ã  Ã©viter !"
]

for texte in textes:
    result = classifier(texte)[0]
    stars = int(result['label'].split()[0])
    print(f"{'â­' * stars} ({result['score']:.2f}) - {texte}")

# â­â­â­â­â­ (0.85) - Absolument incroyable...
# â­â­â­â­ (0.72) - TrÃ¨s bien...
# â­â­â­ (0.65) - Correct...`, desc: 'Notation 5 Ã©toiles' }
                            ],
                            tips: ['Ce modÃ¨le est multilingue, fonctionne en franÃ§ais et anglais', 'Utile pour analyser des avis clients'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'DÃ©tecter les Ã©motions (joie, colÃ¨re...)',
                        desc: 'Classification multi-Ã©motions',
                        details: {
                            explanation: 'Classifie le texte selon plusieurs Ã©motions : joie, tristesse, colÃ¨re, peur, surprise, dÃ©goÃ»t.',
                            syntax: 'pipeline("text-classification", model="..._emotion")',
                            options: [
                                { flag: 'joy', desc: 'Joie' },
                                { flag: 'sadness', desc: 'Tristesse' },
                                { flag: 'anger', desc: 'ColÃ¨re' },
                                { flag: 'fear', desc: 'Peur' },
                                { flag: 'surprise', desc: 'Surprise' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# ModÃ¨le d'Ã©motions (anglais)
emotion = pipeline(
    "text-classification",
    model="j-hartmann/emotion-english-distilroberta-base",
    top_k=None  # Retourne toutes les Ã©motions
)

# Test (en anglais)
result = emotion("I am so happy today!")
for emo in result[0]:
    print(f"{emo['label']:12} {emo['score']:.2f}")

# joy          0.95
# surprise     0.02
# neutral      0.01
# ...`, desc: 'DÃ©tection d\'Ã©motions (anglais)' },
                                { code: `# Pour le franÃ§ais : traduire d'abord
from transformers import pipeline
from deep_translator import GoogleTranslator

# Pipelines
translator = GoogleTranslator(source='fr', target='en')
emotion = pipeline(
    "text-classification",
    model="j-hartmann/emotion-english-distilroberta-base"
)

# Analyser un texte franÃ§ais
texte_fr = "Je suis tellement content aujourd'hui !"
texte_en = translator.translate(texte_fr)
result = emotion(texte_en)[0]
print(f"{texte_fr} â†’ {result['label']} ({result['score']:.2f})")`, desc: 'Ã‰motions pour le franÃ§ais (via traduction)' }
                            ],
                            tips: ['Peu de modÃ¨les d\'Ã©motions franÃ§ais, utilisez la traduction', 'top_k=None retourne toutes les Ã©motions avec leur score'],
                            warnings: ['Les modÃ¨les d\'Ã©motions anglais peuvent perdre des nuances en traduction']
                        }
                    },
                    {
                        cmd: 'Analyse simple avec TextBlob',
                        desc: 'Sentiment rapide sans GPU',
                        details: {
                            explanation: 'TextBlob fournit une analyse basique avec polaritÃ© (-1 Ã  1) et subjectivitÃ© (0 Ã  1). Plus rapide mais moins prÃ©cis.',
                            syntax: 'from textblob import TextBlob\nblob = TextBlob(text)',
                            options: [
                                { flag: '.sentiment.polarity', desc: '-1 (nÃ©gatif) Ã  1 (positif)' },
                                { flag: '.sentiment.subjectivity', desc: '0 (objectif) Ã  1 (subjectif)' }
                            ],
                            examples: [
                                { code: `from textblob import TextBlob

# Anglais
blob = TextBlob("I love this product, it's amazing!")
print(f"Polarity: {blob.sentiment.polarity:.2f}")      # 0.62
print(f"Subjectivity: {blob.sentiment.subjectivity:.2f}") # 0.76`, desc: 'TextBlob (anglais)' },
                                { code: `# Pour le franÃ§ais, utilisez textblob-fr
# pip install textblob-fr

from textblob import TextBlob
from textblob_fr import PatternTagger, PatternAnalyzer

blob = TextBlob(
    "J'adore ce produit, il est incroyable !",
    pos_tagger=PatternTagger(),
    analyzer=PatternAnalyzer()
)
print(f"Polarity: {blob.sentiment[0]:.2f}")
print(f"Subjectivity: {blob.sentiment[1]:.2f}")`, desc: 'TextBlob franÃ§ais' }
                            ],
                            tips: ['pip install textblob textblob-fr', 'TrÃ¨s rapide, idÃ©al pour du prototypage'],
                            warnings: ['Moins prÃ©cis que les modÃ¨les Transformers']
                        }
                    },
                    {
                        cmd: 'Analyser un corpus entier par batch',
                        desc: 'Traitement efficace de grands volumes',
                        details: {
                            explanation: 'Pour analyser des milliers de textes, utilisez le traitement par batch avec une barre de progression.',
                            syntax: 'classifier(texts, batch_size=32, truncation=True)',
                            options: [
                                { flag: 'batch_size', desc: 'Taille des lots (16-64)' },
                                { flag: 'truncation', desc: 'Tronquer les textes longs' },
                                { flag: 'max_length', desc: 'Longueur max (512 tokens)' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline
from tqdm import tqdm
import pandas as pd

# Charger le modÃ¨le
classifier = pipeline(
    "sentiment-analysis",
    model="tblard/tf-allocine",
    device=0  # GPU
)

# Charger les donnÃ©es
df = pd.read_csv("reviews.csv")
texts = df["text"].tolist()

# Traitement par batch avec progression
def analyze_batch(texts, batch_size=32):
    results = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Analyse"):
        batch = texts[i:i+batch_size]
        # Nettoyer et tronquer
        batch_clean = [str(t)[:1000] if pd.notna(t) else "" for t in batch]
        try:
            batch_results = classifier(batch_clean, truncation=True)
            results.extend(batch_results)
        except Exception as e:
            results.extend([{"label": "ERROR", "score": 0}] * len(batch))
    return results

# Analyser
results = analyze_batch(texts, batch_size=32)

# Ajouter au DataFrame
df["sentiment"] = [r["label"] for r in results]
df["confidence"] = [r["score"] for r in results]

print(df["sentiment"].value_counts())`, desc: 'Analyse batch avec progression' }
                            ],
                            tips: ['batch_size=32 est un bon compromis vitesse/mÃ©moire', 'GÃ©rez les erreurs pour les textes problÃ©matiques'],
                            warnings: ['Textes > 512 tokens seront tronquÃ©s']
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 4: CLASSIFIER DES TEXTES
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'classification',
                title: 'ğŸ·ï¸ Classifier des textes',
                icon: 'fa-tags',
                color: 'border-l-4 border-purple-500',
                commands: [
                    {
                        cmd: 'Classifier sans donnÃ©es (zero-shot)',
                        desc: 'Classification sans entraÃ®nement',
                        details: {
                            explanation: 'Le zero-shot permet de classifier du texte dans des catÃ©gories personnalisÃ©es sans aucune donnÃ©e d\'entraÃ®nement.',
                            syntax: 'pipeline("zero-shot-classification", candidate_labels=[...])',
                            options: [
                                { flag: 'candidate_labels', desc: 'Liste des catÃ©gories possibles' },
                                { flag: 'multi_label=True', desc: 'Plusieurs labels possibles' },
                                { flag: 'hypothesis_template', desc: 'Template personnalisÃ©' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# Charger le modÃ¨le
classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli"
)

# Classifier un texte
texte = "Apple lance un nouveau MacBook avec puce M3"
categories = ["technologie", "sport", "politique", "finance"]

result = classifier(texte, candidate_labels=categories)

print(f"Texte: {texte}")
for label, score in zip(result['labels'], result['scores']):
    print(f"  {label}: {score:.2f}")

# technologie: 0.92
# finance: 0.05
# politique: 0.02
# sport: 0.01`, desc: 'Classification thÃ©matique' },
                                { code: `# Multi-label (plusieurs catÃ©gories possibles)
texte = "Bug urgent dans le module de paiement"
categories = ["bug", "feature", "urgent", "question"]

result = classifier(
    texte,
    candidate_labels=categories,
    multi_label=True
)

for label, score in zip(result['labels'], result['scores']):
    if score > 0.5:
        print(f"âœ“ {label}: {score:.2f}")

# âœ“ bug: 0.95
# âœ“ urgent: 0.87`, desc: 'Classification multi-label' }
                            ],
                            tips: ['Parfait pour le prototypage rapide', 'Personnalisez les catÃ©gories selon vos besoins'],
                            warnings: ['Plus lent que les classifieurs entraÃ®nÃ©s']
                        }
                    },
                    {
                        cmd: 'Classifier avec peu d\'exemples (SetFit)',
                        desc: 'Few-shot learning avec 8-16 exemples par classe',
                        details: {
                            explanation: 'SetFit entraÃ®ne un classifieur performant avec trÃ¨s peu d\'exemples (8-16 par classe) grÃ¢ce au contrastive learning.',
                            syntax: 'from setfit import SetFitModel, SetFitTrainer',
                            options: [
                                { flag: 'num_iterations', desc: 'ItÃ©rations contrastive (20 par dÃ©faut)' },
                                { flag: 'num_epochs', desc: 'Ã‰poques classification (1 par dÃ©faut)' }
                            ],
                            examples: [
                                { code: `from setfit import SetFitModel, SetFitTrainer
from datasets import Dataset

# DonnÃ©es d'entraÃ®nement (peu d'exemples suffisent!)
train_data = Dataset.from_dict({
    "text": [
        "Super produit, je recommande !",
        "Excellent service client",
        "TrÃ¨s satisfait de mon achat",
        "Nul, ne fonctionne pas",
        "DÃ©Ã§u par la qualitÃ©",
        "Livraison en retard, produit cassÃ©"
    ],
    "label": [1, 1, 1, 0, 0, 0]  # 1=positif, 0=nÃ©gatif
})

# Charger le modÃ¨le
model = SetFitModel.from_pretrained(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# EntraÃ®ner (trÃ¨s rapide!)
trainer = SetFitTrainer(
    model=model,
    train_dataset=train_data,
    num_iterations=20
)
trainer.train()

# PrÃ©dire
predictions = model.predict([
    "Ce produit est gÃ©nial !",
    "TrÃ¨s mauvaise expÃ©rience"
])
print(predictions)  # [1, 0]`, desc: 'Classification few-shot avec SetFit' }
                            ],
                            tips: ['pip install setfit', 'IdÃ©al quand vous avez peu de donnÃ©es labellisÃ©es', 'Fonctionne bien avec seulement 8 exemples par classe'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Classification ML rapide (TF-IDF)',
                        desc: 'Approche classique sans deep learning',
                        details: {
                            explanation: 'TF-IDF + Logistic Regression est une approche classique rapide et efficace, sans besoin de GPU.',
                            syntax: 'TfidfVectorizer() + LogisticRegression()',
                            options: [
                                { flag: 'max_features', desc: 'Nombre max de features' },
                                { flag: 'ngram_range', desc: '(1,2) pour uni+bigrammes' },
                                { flag: 'min_df', desc: 'FrÃ©quence minimum' }
                            ],
                            examples: [
                                { code: `from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# DonnÃ©es
texts = ["Super produit", "Nul", "Excellent", "Horrible", ...]
labels = [1, 0, 1, 0, ...]

# Split
X_train, X_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2
)

# Pipeline
pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),  # Uni et bigrammes
        min_df=2
    )),
    ("clf", LogisticRegression(max_iter=1000))
])

# EntraÃ®ner
pipeline.fit(X_train, y_train)

# Ã‰valuer
accuracy = pipeline.score(X_test, y_test)
print(f"Accuracy: {accuracy:.2f}")

# PrÃ©dire
predictions = pipeline.predict(["Nouveau texte Ã  classifier"])`, desc: 'Pipeline TF-IDF classique' },
                                { code: `# Alternative : Naive Bayes (souvent meilleur pour texte)
from sklearn.naive_bayes import MultinomialNB

pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(max_features=5000)),
    ("clf", MultinomialNB())
])

pipeline.fit(X_train, y_train)
print(f"Accuracy: {pipeline.score(X_test, y_test):.2f}")`, desc: 'Avec Naive Bayes' }
                            ],
                            tips: ['TrÃ¨s rapide, pas de GPU nÃ©cessaire', 'ngram_range=(1,2) capture des expressions'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Fine-tuner un modÃ¨le Transformers',
                        desc: 'EntraÃ®ner sur vos propres donnÃ©es',
                        details: {
                            explanation: 'Le fine-tuning adapte un modÃ¨le prÃ©-entraÃ®nÃ© Ã  votre tÃ¢che spÃ©cifique avec vos donnÃ©es.',
                            syntax: 'Trainer(model, args, train_dataset).train()',
                            options: [
                                { flag: 'num_train_epochs', desc: 'Nombre d\'Ã©poques (2-5)' },
                                { flag: 'learning_rate', desc: 'Taux d\'apprentissage (2e-5)' },
                                { flag: 'per_device_train_batch_size', desc: 'Batch size (8-32)' }
                            ],
                            examples: [
                                { code: `from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
import numpy as np

# DonnÃ©es
texts = ["Super", "Nul", "Excellent", "Horrible", ...]
labels = [1, 0, 1, 0, ...]

# CrÃ©er dataset
dataset = Dataset.from_dict({"text": texts, "label": labels})
dataset = dataset.train_test_split(test_size=0.2)

# Charger modÃ¨le et tokenizer
model_name = "camembert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, num_labels=2
)

# Tokeniser
def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=True,
        max_length=512
    )

dataset = dataset.map(tokenize, batched=True)

# Configuration
args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    learning_rate=2e-5,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True
)

# EntraÃ®ner
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"]
)
trainer.train()

# Sauvegarder
trainer.save_model("./my_classifier")`, desc: 'Fine-tuning CamemBERT' }
                            ],
                            tips: ['GPU fortement recommandÃ©', 'Commencez avec peu d\'Ã©poques (2-3)', 'learning_rate=2e-5 est un bon point de dÃ©part'],
                            warnings: ['NÃ©cessite plusieurs centaines/milliers d\'exemples', 'Long sans GPU']
                        }
                    },
                    {
                        cmd: 'Classification multi-label',
                        desc: 'Un texte peut avoir plusieurs catÃ©gories',
                        details: {
                            explanation: 'Quand un texte peut appartenir Ã  plusieurs catÃ©gories simultanÃ©ment (ex: un email peut Ãªtre "urgent" ET "question").',
                            syntax: 'multi_label=True ou problem_type="multi_label_classification"',
                            options: [
                                { flag: 'multi_label=True', desc: 'Pour zero-shot' },
                                { flag: 'problem_type="multi_label_classification"', desc: 'Pour fine-tuning' },
                                { flag: 'threshold', desc: 'Seuil de dÃ©cision (0.5 par dÃ©faut)' }
                            ],
                            examples: [
                                { code: `# Zero-shot multi-label
from transformers import pipeline

classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli"
)

texte = "Bug critique dans le systÃ¨me de paiement, Ã  corriger d'urgence"
labels = ["bug", "feature", "urgent", "documentation"]

result = classifier(texte, candidate_labels=labels, multi_label=True)

print("Labels dÃ©tectÃ©s:")
for label, score in zip(result['labels'], result['scores']):
    if score > 0.5:
        print(f"  âœ“ {label}: {score:.2f}")

# âœ“ bug: 0.95
# âœ“ urgent: 0.89`, desc: 'Zero-shot multi-label' },
                                { code: `# Fine-tuning multi-label
from transformers import AutoModelForSequenceClassification

# Labels encodÃ©s en one-hot
# Ex: [1, 0, 1, 0] = bug + urgent

model = AutoModelForSequenceClassification.from_pretrained(
    "camembert-base",
    num_labels=4,
    problem_type="multi_label_classification"
)

# Le reste du code est similaire au fine-tuning classique
# mais avec labels en format one-hot et BCEWithLogitsLoss`, desc: 'Fine-tuning multi-label' }
                            ],
                            tips: ['Utilisez un seuil de 0.5 pour dÃ©cider si un label s\'applique', 'Les labels ne sont pas mutuellement exclusifs'],
                            warnings: []
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 5: COMPARER ET RECHERCHER
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'similarity',
                title: 'ğŸ” Comparer et rechercher',
                icon: 'fa-code-compare',
                color: 'border-l-4 border-orange-500',
                commands: [
                    {
                        cmd: 'GÃ©nÃ©rer des embeddings de phrases',
                        desc: 'Convertir du texte en vecteurs numÃ©riques',
                        details: {
                            explanation: 'Les embeddings sont des reprÃ©sentations vectorielles denses qui capturent le sens sÃ©mantique du texte.',
                            syntax: 'model = SentenceTransformer("model_name")\nembeddings = model.encode(texts)',
                            options: [
                                { flag: 'paraphrase-multilingual-MiniLM-L12-v2', desc: 'Multilingue, rapide, 384 dims' },
                                { flag: 'dangvantuan/sentence-camembert-large', desc: 'FranÃ§ais optimisÃ©, 1024 dims' },
                                { flag: 'all-MiniLM-L6-v2', desc: 'Anglais, trÃ¨s rapide, 384 dims' },
                                { flag: 'intfloat/multilingual-e5-large', desc: 'Multilingue, haute qualitÃ©' }
                            ],
                            examples: [
                                { code: `from sentence_transformers import SentenceTransformer

# Charger un modÃ¨le multilingue
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# Encoder des phrases
phrases = [
    "Le chat dort sur le canapÃ©.",
    "Un fÃ©lin se repose sur le sofa.",
    "La voiture roule vite.",
    "Python est un langage de programmation."
]

embeddings = model.encode(phrases, show_progress_bar=True)
print(f"Shape: {embeddings.shape}")  # (4, 384)`, desc: 'GÃ©nÃ©rer des embeddings' },
                                { code: `# ModÃ¨le franÃ§ais optimisÃ©
model_fr = SentenceTransformer("dangvantuan/sentence-camembert-large")

# Encoder avec options
embeddings = model_fr.encode(
    phrases,
    convert_to_tensor=True,  # Retourne tensor PyTorch
    show_progress_bar=True,
    batch_size=32
)`, desc: 'ModÃ¨le franÃ§ais' }
                            ],
                            tips: ['Les modÃ¨les multilingues fonctionnent bien pour le franÃ§ais', 'PrÃ©-calculez et stockez les embeddings pour les gros corpus'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Calculer la similaritÃ© entre textes',
                        desc: 'Mesurer la proximitÃ© sÃ©mantique',
                        details: {
                            explanation: 'La similaritÃ© cosinus mesure l\'angle entre deux vecteurs : 1 = identique, 0 = non liÃ©s, -1 = opposÃ©s.',
                            syntax: 'from sentence_transformers import util\nscores = util.cos_sim(emb1, emb2)',
                            options: [
                                { flag: 'util.cos_sim()', desc: 'SimilaritÃ© cosinus (sentence-transformers)' },
                                { flag: 'cosine_similarity()', desc: 'Version sklearn' }
                            ],
                            examples: [
                                { code: `from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# Deux phrases Ã  comparer
phrase1 = "Le chat dort sur le canapÃ©."
phrase2 = "Un fÃ©lin se repose sur le sofa."
phrase3 = "Python est un langage de programmation."

# Encoder
emb1 = model.encode(phrase1)
emb2 = model.encode(phrase2)
emb3 = model.encode(phrase3)

# Calculer la similaritÃ©
sim_1_2 = util.cos_sim(emb1, emb2)[0][0].item()
sim_1_3 = util.cos_sim(emb1, emb3)[0][0].item()

print(f"'{phrase1[:30]}...' vs '{phrase2[:30]}...': {sim_1_2:.2f}")
print(f"'{phrase1[:30]}...' vs '{phrase3[:30]}...': {sim_1_3:.2f}")

# 0.85 (trÃ¨s similaires - mÃªme sens)
# 0.12 (pas liÃ©s)`, desc: 'SimilaritÃ© entre deux textes' },
                                { code: `# Matrice de similaritÃ© (tous contre tous)
phrases = ["Le chat dort", "Un fÃ©lin sommeille", "Python est cool", "J'aime coder"]
embeddings = model.encode(phrases)

# Matrice de similaritÃ©
sim_matrix = util.cos_sim(embeddings, embeddings)

print("Matrice de similaritÃ©:")
for i, p1 in enumerate(phrases):
    for j, p2 in enumerate(phrases):
        print(f"  {p1[:15]:15} vs {p2[:15]:15}: {sim_matrix[i][j]:.2f}")`, desc: 'Matrice de similaritÃ©' }
                            ],
                            tips: ['SimilaritÃ© > 0.7 = trÃ¨s similaires', '0.3-0.7 = liÃ©s', '< 0.3 = diffÃ©rents'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Recherche sÃ©mantique dans un corpus',
                        desc: 'Trouver les documents les plus pertinents',
                        details: {
                            explanation: 'Recherche les documents les plus similaires Ã  une requÃªte, mÃªme sans mots-clÃ©s exacts.',
                            syntax: 'util.semantic_search(query_emb, corpus_emb, top_k=5)',
                            options: [
                                { flag: 'top_k', desc: 'Nombre de rÃ©sultats' },
                                { flag: 'score_function', desc: 'cos_sim ou dot_product' }
                            ],
                            examples: [
                                { code: `from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# Corpus de documents
corpus = [
    "Python est un langage de programmation polyvalent.",
    "Le machine learning transforme l'industrie.",
    "Les chats sont des animaux de compagnie populaires.",
    "L'intelligence artificielle progresse rapidement.",
    "JavaScript est utilisÃ© pour le dÃ©veloppement web.",
    "Les rÃ©seaux de neurones imitent le cerveau humain."
]

# Encoder le corpus (Ã  faire une seule fois)
corpus_embeddings = model.encode(corpus, convert_to_tensor=True)

# RequÃªte de recherche
query = "apprentissage automatique et IA"
query_embedding = model.encode(query, convert_to_tensor=True)

# Recherche sÃ©mantique
hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)

print(f"RequÃªte: '{query}'\\n")
print("RÃ©sultats:")
for hit in hits[0]:
    print(f"  [{hit['score']:.2f}] {corpus[hit['corpus_id']]}")`, desc: 'Recherche sÃ©mantique' }
                            ],
                            tips: ['PrÃ©-calculez les embeddings du corpus une seule fois', 'Utilisez FAISS ou Chroma pour les trÃ¨s gros corpus'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'SimilaritÃ© avec Spacy',
                        desc: 'Alternative simple avec word vectors',
                        details: {
                            explanation: 'Spacy calcule la similaritÃ© basÃ©e sur les word vectors moyennÃ©s du document. Plus simple mais moins prÃ©cis.',
                            syntax: 'doc1.similarity(doc2)',
                            options: [],
                            examples: [
                                { code: `import spacy

# Charger un modÃ¨le avec vectors (_md ou _lg)
nlp = spacy.load("fr_core_news_md")

# CrÃ©er des documents
doc1 = nlp("J'aime les chats")
doc2 = nlp("J'adore les fÃ©lins")
doc3 = nlp("Python est un langage")

# Calculer la similaritÃ©
print(f"'{doc1}' vs '{doc2}': {doc1.similarity(doc2):.2f}")  # ~0.85
print(f"'{doc1}' vs '{doc3}': {doc1.similarity(doc3):.2f}")  # ~0.20

# SimilaritÃ© entre tokens
chat = nlp("chat")[0]
felin = nlp("fÃ©lin")[0]
print(f"'chat' vs 'fÃ©lin': {chat.similarity(felin):.2f}")`, desc: 'SimilaritÃ© avec Spacy' }
                            ],
                            tips: ['NÃ©cessite un modÃ¨le _md ou _lg avec word vectors', 'Plus rapide mais moins prÃ©cis que Sentence-Transformers'],
                            warnings: ['Les modÃ¨les _sm n\'ont pas de vectors (retourne toujours 1.0)']
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 6: TOPIC MODELING
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'topic-modeling',
                title: 'ğŸ“Š DÃ©couvrir les thÃ¨mes',
                icon: 'fa-sitemap',
                color: 'border-l-4 border-green-500',
                commands: [
                    {
                        cmd: 'Pipeline BERTopic complet',
                        desc: 'Topic modeling moderne avec embeddings',
                        details: {
                            explanation: 'BERTopic utilise des embeddings de phrases + UMAP + HDBSCAN pour dÃ©couvrir automatiquement les thÃ¨mes d\'un corpus.',
                            syntax: 'topic_model = BERTopic(language="french")\ntopics, probs = topic_model.fit_transform(docs)',
                            options: [
                                { flag: 'language="french"', desc: 'Optimise pour le franÃ§ais' },
                                { flag: 'min_topic_size', desc: 'Taille min d\'un topic' },
                                { flag: 'nr_topics', desc: '"auto" ou nombre fixe' },
                                { flag: 'embedding_model', desc: 'ModÃ¨le d\'embeddings personnalisÃ©' }
                            ],
                            examples: [
                                { code: `from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import pandas as pd

# Charger les documents
df = pd.read_csv("corpus.csv")
documents = df["text"].tolist()

# Option 1: Simple
topic_model = BERTopic(language="french")
topics, probs = topic_model.fit_transform(documents)

# Option 2: Avec modÃ¨le franÃ§ais personnalisÃ©
embedding_model = SentenceTransformer("dangvantuan/sentence-camembert-large")
topic_model = BERTopic(
    embedding_model=embedding_model,
    min_topic_size=15,
    verbose=True
)
topics, probs = topic_model.fit_transform(documents)

# Voir les topics
print(topic_model.get_topic_info())`, desc: 'BERTopic basique' },
                                { code: `# Configuration avancÃ©e
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer

# Composants personnalisÃ©s
embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

umap_model = UMAP(
    n_neighbors=15,
    n_components=5,
    min_dist=0.0,
    metric="cosine"
)

hdbscan_model = HDBSCAN(
    min_cluster_size=15,
    min_samples=5,
    metric="euclidean"
)

# Stop words franÃ§ais
french_stopwords = ["le", "la", "les", "de", "du", "des", "un", "une", "et", "est", "en", "que", "qui"]
vectorizer_model = CountVectorizer(stop_words=french_stopwords, ngram_range=(1, 2))

# CrÃ©er BERTopic
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    verbose=True
)

topics, probs = topic_model.fit_transform(documents)`, desc: 'BERTopic avancÃ©' }
                            ],
                            tips: ['Topic -1 = documents outliers (non classÃ©s)', 'NÃ©cessite au moins 100+ documents pour de bons rÃ©sultats'],
                            warnings: ['GPU recommandÃ© pour les grands corpus']
                        }
                    },
                    {
                        cmd: 'Visualiser les topics dÃ©couverts',
                        desc: 'Graphiques interactifs avec Plotly',
                        details: {
                            explanation: 'BERTopic gÃ©nÃ¨re des visualisations interactives pour explorer et comprendre les topics.',
                            syntax: 'topic_model.visualize_topics()',
                            options: [
                                { flag: 'visualize_topics()', desc: 'Carte 2D des topics' },
                                { flag: 'visualize_barchart()', desc: 'Mots-clÃ©s par topic' },
                                { flag: 'visualize_heatmap()', desc: 'SimilaritÃ© entre topics' },
                                { flag: 'visualize_hierarchy()', desc: 'Dendrogramme' },
                                { flag: 'visualize_documents()', desc: 'Documents sur carte' }
                            ],
                            examples: [
                                { code: `# Carte des topics (distance et taille)
fig = topic_model.visualize_topics()
fig.show()  # Affiche dans le navigateur
fig.write_html("topics_map.html")  # Sauvegarde

# Mots-clÃ©s par topic
fig = topic_model.visualize_barchart(top_n_topics=10, n_words=8)
fig.show()

# Heatmap de similaritÃ©
fig = topic_model.visualize_heatmap()
fig.show()

# HiÃ©rarchie des topics
fig = topic_model.visualize_hierarchy()
fig.show()`, desc: 'Visualisations principales' },
                                { code: `# Documents sur la carte 2D
fig = topic_model.visualize_documents(
    documents,
    embeddings=embeddings,  # PrÃ©-calculÃ©s
    hide_annotations=True
)
fig.write_html("documents_map.html")

# Ã‰volution temporelle (si vous avez des dates)
fig = topic_model.visualize_topics_over_time(
    topic_model.topics_over_time(documents, timestamps)
)
fig.show()`, desc: 'Visualisation des documents' }
                            ],
                            tips: ['write_html() pour sauvegarder les graphiques interactifs', 'Parfait dans Jupyter Notebook'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Affiner et nommer les topics',
                        desc: 'RÃ©duire, fusionner et personnaliser',
                        details: {
                            explanation: 'AprÃ¨s la dÃ©couverte, affinez les topics : rÃ©duisez leur nombre, fusionnez les similaires, ajoutez des labels personnalisÃ©s.',
                            syntax: 'topic_model.reduce_topics() / merge_topics() / set_topic_labels()',
                            options: [
                                { flag: 'reduce_topics(nr_topics=N)', desc: 'RÃ©duire Ã  N topics' },
                                { flag: 'merge_topics(topics_to_merge)', desc: 'Fusionner des topics' },
                                { flag: 'set_topic_labels(labels)', desc: 'Labels personnalisÃ©s' }
                            ],
                            examples: [
                                { code: `# RÃ©duire le nombre de topics
topic_model.reduce_topics(documents, nr_topics=10)

# Fusionner des topics similaires
topics_to_merge = [[1, 3], [5, 7, 9]]  # Fusionner 1+3 et 5+7+9
topic_model.merge_topics(documents, topics_to_merge)

# Voir les dÃ©tails d'un topic
print("Topic 0:")
for word, score in topic_model.get_topic(0):
    print(f"  {word}: {score:.4f}")

# Documents reprÃ©sentatifs
rep_docs = topic_model.get_representative_docs(0)
for doc in rep_docs[:3]:
    print(f"  - {doc[:100]}...")`, desc: 'RÃ©duire et fusionner' },
                                { code: `# Labels personnalisÃ©s
custom_labels = {
    0: "Intelligence Artificielle",
    1: "DÃ©veloppement Web",
    2: "Data Science",
    3: "CybersÃ©curitÃ©"
}
topic_model.set_topic_labels(custom_labels)

# VÃ©rifier
print(topic_model.get_topic_info()[["Topic", "CustomName", "Count"]])`, desc: 'Nommer les topics' }
                            ],
                            tips: ['Utilisez visualize_hierarchy() pour identifier les topics Ã  fusionner', 'Nommez les topics pour faciliter l\'interprÃ©tation'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Topic modeling avec LDA (classique)',
                        desc: 'Approche probabiliste traditionnelle',
                        details: {
                            explanation: 'LDA (Latent Dirichlet Allocation) est la mÃ©thode classique de topic modeling, basÃ©e sur des statistiques.',
                            syntax: 'from gensim.models import LdaModel',
                            options: [
                                { flag: 'num_topics', desc: 'Nombre de topics (Ã  dÃ©finir)' },
                                { flag: 'passes', desc: 'ItÃ©rations d\'entraÃ®nement' },
                                { flag: 'alpha', desc: 'Prior sur distribution topics' }
                            ],
                            examples: [
                                { code: `from gensim import corpora
from gensim.models import LdaModel
import spacy

# PrÃ©traitement avec Spacy
nlp = spacy.load("fr_core_news_md")

def preprocess(text):
    doc = nlp(text)
    return [token.lemma_.lower() for token in doc
            if not token.is_stop and not token.is_punct and len(token) > 2]

# Tokeniser les documents
tokenized_docs = [preprocess(doc) for doc in documents]

# CrÃ©er le dictionnaire et corpus
dictionary = corpora.Dictionary(tokenized_docs)
dictionary.filter_extremes(no_below=5, no_above=0.5)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# EntraÃ®ner LDA
lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=10,
    passes=10,
    random_state=42
)

# Afficher les topics
for idx, topic in lda_model.print_topics(num_words=8):
    print(f"Topic {idx}: {topic}")`, desc: 'LDA avec Gensim' }
                            ],
                            tips: ['PrÃ©traitez bien (lemmatisation, stopwords)', 'Choisir num_topics est difficile, testez plusieurs valeurs'],
                            warnings: ['Moins performant que BERTopic sur la plupart des tÃ¢ches']
                        }
                    },
                    {
                        cmd: 'Nommer les topics avec un LLM',
                        desc: 'GÃ©nÃ©ration de labels automatiques',
                        details: {
                            explanation: 'Utilisez un LLM local (Ollama) pour gÃ©nÃ©rer automatiquement des noms descriptifs pour chaque topic.',
                            syntax: 'BERTopic(representation_model=llm_representation)',
                            options: [
                                { flag: 'Ollama', desc: 'LLM local (Mistral, Llama)' },
                                { flag: 'OpenAI', desc: 'API OpenAI compatible' }
                            ],
                            examples: [
                                { code: `# Avec Ollama (LLM local)
from bertopic.representation import OpenAI
import openai

# Client pour Ollama
client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)

# Prompt personnalisÃ©
prompt = """
J'ai un topic contenant les documents suivants:
[DOCUMENTS]

Les mots-clÃ©s principaux sont: [KEYWORDS]

GÃ©nÃ¨re un label court (2-4 mots) qui rÃ©sume ce topic.
RÃ©ponds uniquement avec le label.
"""

llm_representation = OpenAI(
    client=client,
    model="mistral",
    prompt=prompt,
    nr_docs=5
)

# Utiliser avec BERTopic
topic_model = BERTopic(
    representation_model=llm_representation,
    verbose=True
)
topics, probs = topic_model.fit_transform(documents)

# Les topics auront des labels gÃ©nÃ©rÃ©s par le LLM
print(topic_model.get_topic_info())`, desc: 'Labels LLM avec Ollama' }
                            ],
                            tips: ['Installez Ollama et tÃ©lÃ©chargez mistral : ollama pull mistral', 'Personnalisez le prompt selon vos besoins'],
                            warnings: ['NÃ©cessite Ollama installÃ© et en cours d\'exÃ©cution']
                        }
                    },
                    {
                        cmd: 'Sauvegarder et rÃ©utiliser le modÃ¨le',
                        desc: 'Persistance pour prÃ©diction future',
                        details: {
                            explanation: 'Sauvegardez le modÃ¨le BERTopic pour prÃ©dire les topics de nouveaux documents.',
                            syntax: 'topic_model.save() / BERTopic.load()',
                            options: [
                                { flag: 'save(path)', desc: 'Sauvegarder le modÃ¨le' },
                                { flag: 'load(path)', desc: 'Charger un modÃ¨le' },
                                { flag: 'transform(docs)', desc: 'PrÃ©dire sur nouveaux docs' }
                            ],
                            examples: [
                                { code: `# Sauvegarder le modÃ¨le
topic_model.save("bertopic_model")

# Sauvegarder les embeddings sÃ©parÃ©ment (recommandÃ©)
import numpy as np
np.save("embeddings.npy", embeddings)

# --- Plus tard : charger et utiliser ---

# Charger le modÃ¨le
loaded_model = BERTopic.load("bertopic_model")

# PrÃ©dire sur de nouveaux documents
new_docs = ["Nouveau document Ã  classifier...", "Autre texte..."]
new_topics, new_probs = loaded_model.transform(new_docs)

# Afficher les rÃ©sultats
for doc, topic in zip(new_docs, new_topics):
    if topic != -1:
        words = [w for w, _ in loaded_model.get_topic(topic)[:3]]
        print(f"Topic {topic} ({', '.join(words)})")
        print(f"  â†’ {doc[:80]}...")`, desc: 'Sauvegarde et prÃ©diction' }
                            ],
                            tips: ['Sauvegardez aussi les embeddings pour accÃ©lÃ©rer transform()', 'Utilisez des noms de fichiers versionnÃ©s'],
                            warnings: []
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 7: TRADUIRE, RÃ‰SUMER, GÃ‰NÃ‰RER
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'generation',
                title: 'ğŸŒ Traduire, rÃ©sumer, gÃ©nÃ©rer',
                icon: 'fa-globe',
                color: 'border-l-4 border-cyan-500',
                commands: [
                    {
                        cmd: 'Traduire franÃ§ais â†” anglais',
                        desc: 'Traduction automatique de qualitÃ©',
                        details: {
                            explanation: 'Plusieurs options pour la traduction : modÃ¨les Transformers locaux ou services en ligne.',
                            syntax: 'pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")',
                            options: [
                                { flag: 'Helsinki-NLP/opus-mt-fr-en', desc: 'FRâ†’EN local' },
                                { flag: 'Helsinki-NLP/opus-mt-en-fr', desc: 'ENâ†’FR local' },
                                { flag: 'deep-translator', desc: 'Google/DeepL API' },
                                { flag: 'mBART', desc: 'Multilingue (50 langues)' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# FranÃ§ais â†’ Anglais
fr_en = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
result = fr_en("Bonjour, comment allez-vous ?")
print(result[0]['translation_text'])
# "Hello, how are you?"

# Anglais â†’ FranÃ§ais
en_fr = pipeline("translation", model="Helsinki-NLP/opus-mt-en-fr")
result = en_fr("Machine learning is transforming the industry.")
print(result[0]['translation_text'])
# "L'apprentissage automatique transforme l'industrie."`, desc: 'Traduction avec Transformers' },
                                { code: `# Avec deep-translator (Google Translate)
from deep_translator import GoogleTranslator

translator = GoogleTranslator(source='fr', target='en')
result = translator.translate("Bonjour le monde !")
print(result)  # "Hello world!"

# DÃ©tection automatique de la langue
translator = GoogleTranslator(source='auto', target='fr')
result = translator.translate("Hello world!")
print(result)  # "Bonjour le monde !"

# Batch
texts = ["Hello", "Goodbye", "Thank you"]
results = GoogleTranslator(source='en', target='fr').translate_batch(texts)
print(results)  # ["Bonjour", "Au revoir", "Merci"]`, desc: 'Avec Google Translate' }
                            ],
                            tips: ['pip install deep-translator', 'Helsinki-NLP a +1000 paires de langues'],
                            warnings: ['Google Translate peut avoir des limites de requÃªtes']
                        }
                    },
                    {
                        cmd: 'RÃ©sumer automatiquement un texte',
                        desc: 'GÃ©nÃ©rer un rÃ©sumÃ© condensÃ©',
                        details: {
                            explanation: 'Les modÃ¨les de summarization condensent un texte long en gardant les informations essentielles.',
                            syntax: 'pipeline("summarization", model="...")',
                            options: [
                                { flag: 'moussaKam/barthez-orangesum-abstract', desc: 'FranÃ§ais (BARThez)' },
                                { flag: 'facebook/bart-large-cnn', desc: 'Anglais (BART)' },
                                { flag: 'csebuetnlp/mT5_multilingual_XLSum', desc: 'Multilingue (mT5)' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# RÃ©sumÃ© franÃ§ais avec BARThez
summarizer = pipeline(
    "summarization",
    model="moussaKam/barthez-orangesum-abstract"
)

texte_long = """
L'intelligence artificielle connaÃ®t une croissance exponentielle.
Les modÃ¨les de langage comme GPT et BERT rÃ©volutionnent le traitement
du texte. Les entreprises investissent massivement dans ces technologies
pour automatiser leurs processus et amÃ©liorer l'expÃ©rience client.
Les applications sont nombreuses : chatbots, analyse de sentiments,
traduction automatique, gÃ©nÃ©ration de contenu...
"""

resume = summarizer(texte_long, max_length=60, min_length=20)
print(resume[0]['summary_text'])`, desc: 'RÃ©sumÃ© franÃ§ais' },
                                { code: `# Alternative multilingue
summarizer = pipeline(
    "summarization",
    model="csebuetnlp/mT5_multilingual_XLSum"
)

# Pour textes trÃ¨s longs, dÃ©coupez en chunks
def summarize_long_text(text, max_chunk=1000):
    chunks = [text[i:i+max_chunk] for i in range(0, len(text), max_chunk)]
    summaries = [summarizer(chunk, max_length=100)[0]['summary_text']
                 for chunk in chunks]
    return " ".join(summaries)`, desc: 'RÃ©sumÃ© de textes longs' }
                            ],
                            tips: ['Utilisez max_length et min_length pour contrÃ´ler la taille', 'DÃ©coupez les textes > 1024 tokens en chunks'],
                            warnings: ['Les modÃ¨les ont une limite de tokens en entrÃ©e']
                        }
                    },
                    {
                        cmd: 'RÃ©pondre Ã  une question sur un texte',
                        desc: 'Question-Answering extractif',
                        details: {
                            explanation: 'Le QA extractif trouve et extrait la rÃ©ponse directement depuis un contexte fourni.',
                            syntax: 'qa(question="...", context="...")',
                            options: [
                                { flag: 'etalab-ia/camembert-base-squadFR-fquad-piaf', desc: 'QA franÃ§ais' },
                                { flag: 'deepset/roberta-base-squad2', desc: 'QA anglais' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# QA franÃ§ais avec CamemBERT
qa = pipeline(
    "question-answering",
    model="etalab-ia/camembert-base-squadFR-fquad-piaf"
)

contexte = """
Victor Hugo est un Ã©crivain franÃ§ais nÃ© le 26 fÃ©vrier 1802 Ã  BesanÃ§on
et mort le 22 mai 1885 Ã  Paris. Il est considÃ©rÃ© comme l'un des plus
importants Ã©crivains de la langue franÃ§aise. Parmi ses Å“uvres les plus
cÃ©lÃ¨bres, on trouve Les MisÃ©rables et Notre-Dame de Paris.
"""

questions = [
    "Quand est nÃ© Victor Hugo ?",
    "OÃ¹ est-il mort ?",
    "Quelles sont ses Å“uvres cÃ©lÃ¨bres ?"
]

for question in questions:
    result = qa(question=question, context=contexte)
    print(f"Q: {question}")
    print(f"R: {result['answer']} (score: {result['score']:.2f})\\n")`, desc: 'QA franÃ§ais' }
                            ],
                            tips: ['Le score indique la confiance de la rÃ©ponse', 'Retourne answer, score, start, end'],
                            warnings: ['Ne gÃ©nÃ¨re pas de rÃ©ponse, extrait uniquement du contexte']
                        }
                    },
                    {
                        cmd: 'GÃ©nÃ©rer du texte',
                        desc: 'Continuation et crÃ©ation de texte',
                        details: {
                            explanation: 'Les modÃ¨les de gÃ©nÃ©ration crÃ©ent du texte Ã  partir d\'un prompt.',
                            syntax: 'pipeline("text-generation", model="...")',
                            options: [
                                { flag: 'max_new_tokens', desc: 'Nombre de tokens Ã  gÃ©nÃ©rer' },
                                { flag: 'temperature', desc: '0=dÃ©terministe, 1=crÃ©atif' },
                                { flag: 'do_sample', desc: 'True pour Ã©chantillonnage' },
                                { flag: 'top_p', desc: 'Nucleus sampling' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# GPT-2 (anglais)
generator = pipeline("text-generation", model="gpt2")

result = generator(
    "Once upon a time",
    max_new_tokens=50,
    do_sample=True,
    temperature=0.7
)
print(result[0]['generated_text'])`, desc: 'GÃ©nÃ©ration basique' },
                                { code: `# BLOOM multilingue
generator = pipeline(
    "text-generation",
    model="bigscience/bloom-560m"
)

# GÃ©nÃ©ration en franÃ§ais
result = generator(
    "L'intelligence artificielle va",
    max_new_tokens=100,
    do_sample=True,
    temperature=0.8,
    top_p=0.9
)
print(result[0]['generated_text'])`, desc: 'BLOOM multilingue' },
                                { code: `# FLAN-T5 (instruction following)
flan = pipeline(
    "text2text-generation",
    model="google/flan-t5-base"
)

# Suivre des instructions
print(flan("Translate to French: Hello, how are you?")[0]['generated_text'])
print(flan("Summarize: " + long_text)[0]['generated_text'])
print(flan("Explain machine learning in simple terms")[0]['generated_text'])`, desc: 'FLAN-T5 instructions' }
                            ],
                            tips: ['BLOOM et mGPT supportent le franÃ§ais', 'temperature=0 pour des rÃ©ponses dÃ©terministes'],
                            warnings: ['ModÃ¨les lourds, prÃ©fÃ©rez GPU']
                        }
                    },
                    {
                        cmd: 'PrÃ©dire un mot masquÃ©',
                        desc: 'Fill-mask avec BERT/CamemBERT',
                        details: {
                            explanation: 'Le fill-mask prÃ©dit le mot manquant dans une phrase, utile pour comprendre le modÃ¨le ou gÃ©nÃ©rer des variantes.',
                            syntax: 'pipeline("fill-mask", model="camembert-base")',
                            options: [
                                { flag: '<mask>', desc: 'Token masquÃ© pour CamemBERT/RoBERTa' },
                                { flag: '[MASK]', desc: 'Token masquÃ© pour BERT' },
                                { flag: 'top_k', desc: 'Nombre de prÃ©dictions' }
                            ],
                            examples: [
                                { code: `from transformers import pipeline

# CamemBERT franÃ§ais
unmasker = pipeline("fill-mask", model="camembert-base")

result = unmasker("Paris est la <mask> de la France.")

print("PrÃ©dictions:")
for r in result[:5]:
    print(f"  {r['token_str']:15} ({r['score']:.2f})")

# capitale        (0.92)
# ville           (0.03)
# capitale        (0.01)
# ...`, desc: 'Fill-mask avec CamemBERT' },
                                { code: `# BERT anglais (attention: [MASK] pas <mask>)
unmasker_en = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker_en("The capital of France is [MASK].")
print(result[0]['token_str'])  # "paris"`, desc: 'Fill-mask anglais' }
                            ],
                            tips: ['<mask> pour CamemBERT/RoBERTa, [MASK] pour BERT', 'Utile pour le data augmentation'],
                            warnings: []
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 8: RAG
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'rag',
                title: 'ğŸ“š RAG : Interroger vos documents',
                icon: 'fa-brain',
                color: 'border-l-4 border-amber-500',
                commands: [
                    {
                        cmd: 'DÃ©couper des documents pour le RAG',
                        desc: 'Chunking intelligent des textes longs',
                        details: {
                            explanation: 'Le RAG nÃ©cessite de dÃ©couper les documents en chunks de taille appropriÃ©e pour l\'embedding et la recherche.',
                            syntax: 'RecursiveCharacterTextSplitter(chunk_size=1000)',
                            options: [
                                { flag: 'chunk_size', desc: 'Taille cible des chunks (caractÃ¨res)' },
                                { flag: 'chunk_overlap', desc: 'Chevauchement entre chunks' },
                                { flag: 'separators', desc: 'SÃ©parateurs prioritaires' }
                            ],
                            examples: [
                                { code: `from langchain.text_splitter import RecursiveCharacterTextSplitter

# Splitter avec chevauchement
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\\n\\n", "\\n", ". ", " ", ""]
)

# DÃ©couper un texte
long_text = "..."  # Votre document
chunks = splitter.split_text(long_text)
print(f"{len(chunks)} chunks crÃ©Ã©s")

# Afficher les premiers chunks
for i, chunk in enumerate(chunks[:3]):
    print(f"Chunk {i}: {len(chunk)} chars")
    print(f"  {chunk[:100]}...\\n")`, desc: 'DÃ©coupage basique' },
                                { code: `# Charger et dÃ©couper un PDF
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("document.pdf")
pages = loader.load()

# DÃ©couper
chunks = splitter.split_documents(pages)
print(f"{len(chunks)} chunks depuis le PDF")

# Chaque chunk a .page_content et .metadata
print(chunks[0].page_content[:200])
print(chunks[0].metadata)  # {'source': 'document.pdf', 'page': 0}`, desc: 'Depuis un PDF' }
                            ],
                            tips: ['chunk_overlap Ã©vite de couper des idÃ©es', 'Ajustez chunk_size selon votre embedding model (max 512 tokens souvent)'],
                            warnings: ['pip install langchain pypdf']
                        }
                    },
                    {
                        cmd: 'CrÃ©er un index vectoriel',
                        desc: 'Stocker les embeddings pour la recherche',
                        details: {
                            explanation: 'Un vector store stocke les embeddings des chunks pour une recherche rapide par similaritÃ©.',
                            syntax: 'vectorstore = FAISS.from_documents(chunks, embeddings)',
                            options: [
                                { flag: 'FAISS', desc: 'Facebook AI, rapide, local' },
                                { flag: 'Chroma', desc: 'Open source, persistant' },
                                { flag: 'Pinecone', desc: 'Cloud, scalable' }
                            ],
                            examples: [
                                { code: `from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# ModÃ¨le d'embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# CrÃ©er le vector store
vectorstore = FAISS.from_documents(chunks, embeddings)

# Sauvegarder localement
vectorstore.save_local("faiss_index")

# Charger plus tard
vectorstore = FAISS.load_local("faiss_index", embeddings)`, desc: 'Index FAISS' },
                                { code: `# Avec Chroma (persistant automatiquement)
from langchain.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# Recherche
results = vectorstore.similarity_search("votre question", k=4)
for doc in results:
    print(f"[{doc.metadata.get('source', 'N/A')}]")
    print(f"  {doc.page_content[:150]}...\\n")`, desc: 'Index Chroma' }
                            ],
                            tips: ['FAISS est le plus rapide pour des datasets moyens', 'Chroma persiste automatiquement sur disque'],
                            warnings: ['pip install faiss-cpu ou chromadb']
                        }
                    },
                    {
                        cmd: 'Pipeline RAG complet',
                        desc: 'Retrieval + LLM pour rÃ©pondre aux questions',
                        details: {
                            explanation: 'Le RAG combine recherche de documents pertinents et gÃ©nÃ©ration de rÃ©ponse par un LLM.',
                            syntax: 'RetrievalQA.from_chain_type(llm, retriever)',
                            options: [
                                { flag: 'chain_type="stuff"', desc: 'Tout dans le prompt' },
                                { flag: 'chain_type="map_reduce"', desc: 'Pour longs docs' },
                                { flag: 'return_source_documents', desc: 'Retourner les sources' }
                            ],
                            examples: [
                                { code: `from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# CrÃ©er le retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

# LLM (OpenAI ou local)
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# ChaÃ®ne RAG
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# Poser une question
result = qa_chain.invoke({"query": "Comment fonctionne le RAG ?"})

print("RÃ©ponse:")
print(result["result"])

print("\\nSources:")
for doc in result["source_documents"]:
    print(f"  - {doc.metadata.get('source', 'N/A')}: {doc.page_content[:80]}...")`, desc: 'RAG avec OpenAI' }
                            ],
                            tips: ['stuff = tout dans le prompt (simple), map_reduce = pour longs docs', 'Ajustez k selon la taille de contexte du LLM'],
                            warnings: ['NÃ©cessite une clÃ© API OpenAI ou un LLM local']
                        }
                    },
                    {
                        cmd: 'RAG conversationnel avec mÃ©moire',
                        desc: 'Chat multi-tours avec contexte',
                        details: {
                            explanation: 'Le RAG conversationnel garde l\'historique de la conversation pour des Ã©changes naturels.',
                            syntax: 'ConversationalRetrievalChain.from_llm(llm, retriever, memory)',
                            options: [
                                { flag: 'memory', desc: 'Buffer de conversation' },
                                { flag: 'return_source_documents', desc: 'Inclure les sources' }
                            ],
                            examples: [
                                { code: `from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# MÃ©moire de conversation
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# ChaÃ®ne conversationnelle
qa = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

# Conversation
response1 = qa.invoke({"question": "Qu'est-ce que Python ?"})
print(f"Q: Qu'est-ce que Python ?")
print(f"R: {response1['answer']}\\n")

# Question de suivi (utilise le contexte)
response2 = qa.invoke({"question": "Quels sont ses avantages ?"})
print(f"Q: Quels sont ses avantages ?")
print(f"R: {response2['answer']}")

# La mÃ©moire contient l'historique
print("\\nHistorique:", memory.chat_memory.messages)`, desc: 'Chat RAG avec mÃ©moire' }
                            ],
                            tips: ['La mÃ©moire permet les rÃ©fÃ©rences pronominales (il, elle, Ã§a...)', 'Limitez la taille de la mÃ©moire pour les longues conversations'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'RAG avec LLM local (Ollama)',
                        desc: 'Sans API externe, tout en local',
                        details: {
                            explanation: 'Utilisez Ollama pour faire du RAG entiÃ¨rement en local, sans dÃ©pendre d\'APIs externes.',
                            syntax: 'HuggingFacePipeline ou ChatOllama',
                            options: [
                                { flag: 'mistral', desc: 'Rapide et efficace' },
                                { flag: 'llama3', desc: 'Meilleure qualitÃ©' },
                                { flag: 'mixtral', desc: 'Mixture of Experts' }
                            ],
                            examples: [
                                { code: `# Installer Ollama: https://ollama.ai
# Terminal: ollama pull mistral

from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# LLM local avec Ollama
llm = Ollama(model="mistral", temperature=0)

# RAG local
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# Utilisation
result = qa_chain.invoke({"query": "Votre question ici"})
print(result["result"])`, desc: 'RAG avec Ollama' },
                                { code: `# Alternative: HuggingFace local
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

# Charger un modÃ¨le local
pipe = pipeline(
    "text-generation",
    model="mistralai/Mistral-7B-Instruct-v0.1",
    max_new_tokens=512,
    device=0  # GPU
)

llm = HuggingFacePipeline(pipeline=pipe)

# Utiliser dans RAG
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)`, desc: 'RAG avec HuggingFace local' }
                            ],
                            tips: ['Ollama est le plus simple pour du local', 'Mistral 7B est un bon compromis qualitÃ©/vitesse'],
                            warnings: ['ModÃ¨les 7B+ nÃ©cessitent GPU avec >8GB VRAM']
                        }
                    }
                ]
            },
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CATÃ‰GORIE 9: COMMANDES DE RÃ‰FÃ‰RENCE
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                id: 'reference',
                title: 'ğŸ“‹ Commandes de RÃ©fÃ©rence',
                icon: 'fa-terminal',
                color: 'border-l-4 border-slate-500',
                commands: [
                    {
                        cmd: 'nlp = spacy.load("model")',
                        desc: 'Charger un modÃ¨le Spacy',
                        details: {
                            explanation: 'Charge un modÃ¨le de langue Spacy.',
                            syntax: 'import spacy\nnlp = spacy.load("fr_core_news_md")',
                            options: [
                                { flag: 'fr_core_news_sm/md/lg', desc: 'ModÃ¨les franÃ§ais' },
                                { flag: 'en_core_web_sm/md/lg', desc: 'ModÃ¨les anglais' }
                            ],
                            examples: [
                                { code: 'nlp = spacy.load("fr_core_news_md")\ndoc = nlp("Bonjour")', desc: 'Usage basique' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'doc = nlp(texte)',
                        desc: 'Traiter un texte avec Spacy',
                        details: {
                            explanation: 'CrÃ©e un document Spacy avec tokenization, POS, NER.',
                            syntax: 'doc = nlp("Mon texte")',
                            options: [],
                            examples: [
                                { code: 'doc = nlp("Paris est belle.")\nfor token in doc:\n    print(token.text, token.pos_)', desc: 'Traitement' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'token.lemma_, token.pos_, token.dep_',
                        desc: 'Attributs d\'un token Spacy',
                        details: {
                            explanation: 'AccÃ¨de aux propriÃ©tÃ©s linguistiques d\'un token.',
                            syntax: 'token.attribut',
                            options: [
                                { flag: '.text', desc: 'Texte original' },
                                { flag: '.lemma_', desc: 'Lemme' },
                                { flag: '.pos_', desc: 'Part of speech' },
                                { flag: '.dep_', desc: 'DÃ©pendance' },
                                { flag: '.is_stop', desc: 'Stop word?' }
                            ],
                            examples: [
                                { code: '[t.lemma_ for t in doc if not t.is_stop]', desc: 'Lemmes sans stopwords' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'doc.ents',
                        desc: 'EntitÃ©s nommÃ©es Spacy',
                        details: {
                            explanation: 'Liste des entitÃ©s nommÃ©es dÃ©tectÃ©es.',
                            syntax: 'for ent in doc.ents: print(ent.text, ent.label_)',
                            options: [],
                            examples: [
                                { code: 'entities = [(ent.text, ent.label_) for ent in doc.ents]', desc: 'Extraction' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'pipeline("task", model="...")',
                        desc: 'Pipeline Transformers',
                        details: {
                            explanation: 'CrÃ©e un pipeline Hugging Face pour une tÃ¢che spÃ©cifique.',
                            syntax: 'from transformers import pipeline\npipe = pipeline("task", model="model_name")',
                            options: [
                                { flag: 'sentiment-analysis', desc: 'Analyse de sentiment' },
                                { flag: 'ner', desc: 'Named Entity Recognition' },
                                { flag: 'summarization', desc: 'RÃ©sumÃ©' },
                                { flag: 'translation', desc: 'Traduction' },
                                { flag: 'text-generation', desc: 'GÃ©nÃ©ration' },
                                { flag: 'zero-shot-classification', desc: 'Classification zero-shot' },
                                { flag: 'question-answering', desc: 'QA extractif' },
                                { flag: 'fill-mask', desc: 'PrÃ©diction mot masquÃ©' }
                            ],
                            examples: [
                                { code: 'classifier = pipeline("sentiment-analysis")\nclassifier("I love this!")', desc: 'Sentiment' }
                            ],
                            tips: ['device=0 pour GPU'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'SentenceTransformer(model)',
                        desc: 'Charger modÃ¨le d\'embeddings',
                        details: {
                            explanation: 'Charge un modÃ¨le Sentence-Transformers.',
                            syntax: 'from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer("model_name")',
                            options: [
                                { flag: 'paraphrase-multilingual-MiniLM-L12-v2', desc: 'Multilingue' },
                                { flag: 'dangvantuan/sentence-camembert-large', desc: 'FranÃ§ais' }
                            ],
                            examples: [
                                { code: 'model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")', desc: 'Charger' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'model.encode(texts)',
                        desc: 'GÃ©nÃ©rer embeddings',
                        details: {
                            explanation: 'Encode une liste de textes en vecteurs.',
                            syntax: 'embeddings = model.encode(texts)',
                            options: [
                                { flag: 'convert_to_tensor=True', desc: 'Retourne tensor PyTorch' },
                                { flag: 'show_progress_bar=True', desc: 'Affiche progression' }
                            ],
                            examples: [
                                { code: 'emb = model.encode(["Hello", "Bonjour"])\nprint(emb.shape)', desc: 'Encoder' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'util.cos_sim(emb1, emb2)',
                        desc: 'SimilaritÃ© cosinus',
                        details: {
                            explanation: 'Calcule la similaritÃ© entre embeddings.',
                            syntax: 'from sentence_transformers import util\nsim = util.cos_sim(emb1, emb2)',
                            options: [],
                            examples: [
                                { code: 'similarity = util.cos_sim(emb1, emb2)[0][0].item()', desc: 'SimilaritÃ©' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'BERTopic()',
                        desc: 'CrÃ©er modÃ¨le BERTopic',
                        details: {
                            explanation: 'CrÃ©e un modÃ¨le de topic modeling.',
                            syntax: 'from bertopic import BERTopic\ntopic_model = BERTopic(language="french")',
                            options: [
                                { flag: 'language', desc: 'Langue du corpus' },
                                { flag: 'embedding_model', desc: 'ModÃ¨le d\'embeddings' },
                                { flag: 'min_topic_size', desc: 'Taille min topic' }
                            ],
                            examples: [
                                { code: 'topics, probs = topic_model.fit_transform(docs)', desc: 'EntraÃ®ner' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'topic_model.get_topic_info()',
                        desc: 'Infos sur les topics',
                        details: {
                            explanation: 'Retourne un DataFrame avec les infos des topics.',
                            syntax: 'topic_model.get_topic_info()',
                            options: [],
                            examples: [
                                { code: 'print(topic_model.get_topic_info())\nprint(topic_model.get_topic(0))', desc: 'Explorer' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 're.sub(pattern, repl, text)',
                        desc: 'Substitution regex',
                        details: {
                            explanation: 'Remplace les occurrences d\'un pattern.',
                            syntax: 'import re\ntext = re.sub(r"pattern", "remplacement", text)',
                            options: [],
                            examples: [
                                { code: 'text = re.sub(r"http\\S+", "", text)  # Supprime URLs', desc: 'Nettoyer' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'detect(text)',
                        desc: 'DÃ©tecter la langue',
                        details: {
                            explanation: 'Identifie la langue d\'un texte.',
                            syntax: 'from langdetect import detect\nlang = detect("Bonjour")',
                            options: [],
                            examples: [
                                { code: 'detect("Hello")  # "en"\ndetect("Bonjour")  # "fr"', desc: 'DÃ©tection' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'TfidfVectorizer()',
                        desc: 'Vectorisation TF-IDF',
                        details: {
                            explanation: 'Convertit des textes en vecteurs TF-IDF.',
                            syntax: 'from sklearn.feature_extraction.text import TfidfVectorizer',
                            options: [
                                { flag: 'max_features', desc: 'Nombre max features' },
                                { flag: 'ngram_range', desc: 'N-grammes' }
                            ],
                            examples: [
                                { code: 'vectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(texts)', desc: 'Vectoriser' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'RecursiveCharacterTextSplitter()',
                        desc: 'DÃ©couper des documents',
                        details: {
                            explanation: 'DÃ©coupe des textes longs en chunks pour le RAG.',
                            syntax: 'from langchain.text_splitter import RecursiveCharacterTextSplitter',
                            options: [
                                { flag: 'chunk_size', desc: 'Taille des chunks' },
                                { flag: 'chunk_overlap', desc: 'Chevauchement' }
                            ],
                            examples: [
                                { code: 'splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\nchunks = splitter.split_text(text)', desc: 'DÃ©couper' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'FAISS.from_documents()',
                        desc: 'CrÃ©er index vectoriel',
                        details: {
                            explanation: 'CrÃ©e un vector store FAISS.',
                            syntax: 'from langchain.vectorstores import FAISS',
                            options: [],
                            examples: [
                                { code: 'vectorstore = FAISS.from_documents(chunks, embeddings)\nvectorstore.save_local("index")', desc: 'CrÃ©er et sauver' }
                            ],
                            tips: [],
                            warnings: []
                        }
                    }
                ]
            }
        ];
    </script>

    <!-- Logique commune -->
    <script src="../js/cheatsheet.js"></script>
</body>
</html>
