<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Aide-m√©moire Rust : workflows pratiques pour Web, Data Engineering, ML, NLP et IA Multimodale.">
    <title>Rust - IT Cheatsheets</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body class="dark-theme text-slate-200">

    <header class="bg-slate-900/50 border-b border-white/5 py-8 px-4 relative overflow-hidden header-glow">
        <div class="max-w-4xl mx-auto relative z-10">
            <div class="flex items-center justify-between mb-4">
                <a href="../index.html" class="nav-back inline-flex items-center text-slate-400 hover:text-sky-400 transition">
                    <i class="fas fa-arrow-left mr-2"></i>Retour
                </a>
                <a href="../index.html" class="inline-flex items-center text-slate-400 hover:text-sky-400 transition">
                    <i class="fas fa-home mr-2"></i>Accueil
                </a>
            </div>
            <div class="text-center">
                <div class="inline-flex items-center justify-center w-16 h-16 rounded-xl bg-orange-500/20 mb-4 icon-glow">
                    <i class="fab fa-rust text-3xl text-orange-400"></i>
                </div>
                <h1 class="text-3xl font-bold mb-2 gradient-text">Rust</h1>
                <p class="text-slate-400">Workflows pratiques pour Web, Data, ML et IA</p>
            </div>
        </div>
    </header>

    <main class="max-w-4xl mx-auto p-4 relative z-10">
        <div class="mb-8 relative">
            <input type="text" id="searchInput" placeholder="Rechercher un workflow..."
                   class="search-dark w-full p-4 pl-12 rounded-lg outline-none transition">
            <i class="fas fa-search absolute left-4 top-1/2 transform -translate-y-1/2 text-slate-500"></i>
        </div>
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6" id="categoriesGrid"></div>
    </main>

    <div id="detailModal" class="fixed inset-0 bg-black/70 hidden items-center justify-center z-50 p-4 modal-overlay" onclick="closeModal(event)">
        <div class="modal-content-dark rounded-xl max-w-2xl w-full max-h-[90vh] overflow-y-auto shadow-2xl modal-content" onclick="event.stopPropagation()">
            <div id="modalContent"></div>
        </div>
    </div>

    <footer class="border-t border-white/5 text-center text-slate-500 py-8 text-sm relative z-10">
        <p>¬© 2026 - Dr FENOHASINA Toto Jean Felicien</p>
    </footer>

    <script>
        const cheatsheetData = [
            {
                id: 'web-api',
                title: 'üåê Infrastructure Web & API',
                icon: 'fa-server',
                color: 'border-l-4 border-orange-500',
                commands: [
                    {
                        cmd: 'Cr√©er une API REST avec Axum',
                        desc: 'Setup complet d\'une API production-ready',
                        details: {
                            explanation: 'Axum est le framework web moderne recommand√© pour Rust. Il utilise Tower pour les middlewares et s\'int√®gre parfaitement avec Tokio.',
                            syntax: 'cargo add axum tokio --features full serde --features derive',
                            options: [
                                { flag: 'axum', desc: 'Framework web async' },
                                { flag: 'tower-http', desc: 'Middlewares HTTP (CORS, compression)' },
                                { flag: 'serde', desc: 'S√©rialisation JSON' }
                            ],
                            examples: [
                                { code: 'use axum::{routing::{get, post}, Json, Router};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Serialize)]\nstruct User { id: u32, name: String }\n\n#[derive(Deserialize)]\nstruct CreateUser { name: String }\n\nasync fn get_users() -> Json<Vec<User>> {\n    Json(vec![User { id: 1, name: "Alice".into() }])\n}\n\nasync fn create_user(Json(payload): Json<CreateUser>) -> Json<User> {\n    Json(User { id: 2, name: payload.name })\n}\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route("/users", get(get_users).post(create_user));\n    \n    let listener = tokio::net::TcpListener::bind("0.0.0.0:3000").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}', desc: 'API CRUD basique' }
                            ],
                            tips: ['Utilisez extractors (Json, Path, Query) pour parser les requ√™tes'],
                            warnings: ['Axum 0.7+ requiert Tokio 1.x']
                        }
                    },
                    {
                        cmd: 'G√©rer l\'authentification JWT',
                        desc: 'jsonwebtoken et middleware auth',
                        details: {
                            explanation: 'Impl√©mente l\'authentification JWT avec validation de tokens et middleware de protection des routes.',
                            syntax: 'cargo add jsonwebtoken chrono',
                            options: [
                                { flag: 'Algorithm::HS256', desc: 'Algorithme HMAC SHA-256' },
                                { flag: 'Validation', desc: 'Configuration de validation' }
                            ],
                            examples: [
                                { code: 'use jsonwebtoken::{encode, decode, Header, Validation, EncodingKey, DecodingKey};\nuse serde::{Deserialize, Serialize};\nuse chrono::{Utc, Duration};\n\n#[derive(Serialize, Deserialize)]\nstruct Claims {\n    sub: String,\n    exp: usize,\n}\n\nfn create_token(user_id: &str, secret: &[u8]) -> String {\n    let expiration = Utc::now() + Duration::hours(24);\n    let claims = Claims {\n        sub: user_id.to_owned(),\n        exp: expiration.timestamp() as usize,\n    };\n    encode(&Header::default(), &claims, &EncodingKey::from_secret(secret)).unwrap()\n}\n\nfn verify_token(token: &str, secret: &[u8]) -> Result<Claims, _> {\n    decode::<Claims>(token, &DecodingKey::from_secret(secret), &Validation::default())\n        .map(|data| data.claims)\n}', desc: 'Cr√©ation et validation JWT' }
                            ],
                            tips: ['Stockez le secret dans une variable d\'environnement'],
                            warnings: ['Ne jamais exposer le secret JWT']
                        }
                    },
                    {
                        cmd: 'Connecter PostgreSQL',
                        desc: 'sqlx avec connection pool et migrations',
                        details: {
                            explanation: 'SQLx fournit des requ√™tes SQL v√©rifi√©es √† la compilation et un pool de connexions async.',
                            syntax: 'cargo add sqlx --features runtime-tokio,postgres,macros',
                            options: [
                                { flag: 'PgPool', desc: 'Pool de connexions PostgreSQL' },
                                { flag: 'query!', desc: 'Macro pour requ√™tes v√©rifi√©es' },
                                { flag: 'migrate!', desc: 'Migrations embarqu√©es' }
                            ],
                            examples: [
                                { code: 'use sqlx::postgres::PgPoolOptions;\n\n#[tokio::main]\nasync fn main() -> Result<(), sqlx::Error> {\n    let pool = PgPoolOptions::new()\n        .max_connections(5)\n        .connect("postgres://user:pass@localhost/db").await?;\n    \n    // Requ√™te v√©rifi√©e √† la compilation\n    let users = sqlx::query!("SELECT id, name FROM users WHERE active = $1", true)\n        .fetch_all(&pool)\n        .await?;\n    \n    for user in users {\n        println!("{}: {}", user.id, user.name);\n    }\n    Ok(())\n}', desc: 'Pool et requ√™tes typ√©es' },
                                { code: '# Migrations\nsqlx migrate add create_users\nsqlx migrate run\n\n# Dans migrations/XXX_create_users.sql\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);', desc: 'Migrations SQLx' }
                            ],
                            tips: ['Utilisez DATABASE_URL pour la v√©rification √† la compilation'],
                            warnings: ['sqlx-cli requis: cargo install sqlx-cli']
                        }
                    },
                    {
                        cmd: 'Valider les donn√©es entrantes',
                        desc: 'validator crate pour validation',
                        details: {
                            explanation: 'Le crate validator permet de valider les donn√©es avec des attributs d√©claratifs.',
                            syntax: 'cargo add validator --features derive',
                            options: [
                                { flag: '#[validate(email)]', desc: 'Valide un email' },
                                { flag: '#[validate(length)]', desc: 'Valide la longueur' },
                                { flag: '#[validate(range)]', desc: 'Valide une plage' }
                            ],
                            examples: [
                                { code: 'use validator::Validate;\nuse serde::Deserialize;\n\n#[derive(Deserialize, Validate)]\nstruct CreateUser {\n    #[validate(length(min = 2, max = 50))]\n    name: String,\n    \n    #[validate(email)]\n    email: String,\n    \n    #[validate(range(min = 18, max = 120))]\n    age: u8,\n}\n\nasync fn create_user(Json(payload): Json<CreateUser>) -> Result<Json<User>, StatusCode> {\n    payload.validate().map_err(|_| StatusCode::BAD_REQUEST)?;\n    // Cr√©ation user...\n}', desc: 'Validation avec attributs' }
                            ],
                            tips: ['Combinez avec serde pour d√©s√©rialisation + validation'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Documenter l\'API avec OpenAPI',
                        desc: 'utoipa et swagger-ui',
                        details: {
                            explanation: 'Utoipa g√©n√®re automatiquement la documentation OpenAPI √† partir de vos types et routes.',
                            syntax: 'cargo add utoipa --features axum_extras\ncargo add utoipa-swagger-ui --features axum',
                            options: [
                                { flag: '#[utoipa::path]', desc: 'Documenter un endpoint' },
                                { flag: '#[derive(ToSchema)]', desc: 'Sch√©ma pour types' }
                            ],
                            examples: [
                                { code: 'use utoipa::{OpenApi, ToSchema};\nuse utoipa_swagger_ui::SwaggerUi;\n\n#[derive(ToSchema, Serialize)]\nstruct User {\n    #[schema(example = 1)]\n    id: u32,\n    #[schema(example = "Alice")]\n    name: String,\n}\n\n#[utoipa::path(\n    get,\n    path = "/users",\n    responses((status = 200, body = Vec<User>))\n)]\nasync fn get_users() -> Json<Vec<User>> { /* ... */ }\n\n#[derive(OpenApi)]\n#[openapi(paths(get_users), components(schemas(User)))]\nstruct ApiDoc;\n\nlet app = Router::new()\n    .route("/users", get(get_users))\n    .merge(SwaggerUi::new("/swagger-ui").url("/api-docs/openapi.json", ApiDoc::openapi()));', desc: 'Documentation Swagger' }
                            ],
                            tips: ['Swagger UI disponible √† /swagger-ui'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Ajouter du rate limiting',
                        desc: 'tower middleware et governor',
                        details: {
                            explanation: 'Governor impl√©mente le rate limiting avec l\'algorithme GCRA (Generic Cell Rate Algorithm).',
                            syntax: 'cargo add tower-governor',
                            options: [
                                { flag: 'per_second', desc: 'Limite par seconde' },
                                { flag: 'per_minute', desc: 'Limite par minute' },
                                { flag: 'burst_size', desc: 'Rafale autoris√©e' }
                            ],
                            examples: [
                                { code: 'use tower_governor::{GovernorLayer, GovernorConfigBuilder};\nuse std::time::Duration;\n\nlet governor_conf = GovernorConfigBuilder::default()\n    .per_second(2)  // 2 req/sec\n    .burst_size(5)  // Burst de 5\n    .finish()\n    .unwrap();\n\nlet app = Router::new()\n    .route("/api/resource", get(handler))\n    .layer(GovernorLayer {\n        config: governor_conf,\n    });', desc: 'Rate limiting global' }
                            ],
                            tips: ['Utilisez des limites diff√©rentes par route si n√©cessaire'],
                            warnings: ['Configurez selon votre infrastructure']
                        }
                    },
                    {
                        cmd: 'D√©ployer avec Docker',
                        desc: 'Multi-stage builds optimis√©s',
                        details: {
                            explanation: 'Un Dockerfile multi-stage permet de cr√©er des images minimales avec seulement le binaire.',
                            syntax: 'docker build -t myapp .',
                            options: [
                                { flag: 'rust:slim', desc: 'Image de build l√©g√®re' },
                                { flag: 'debian:bookworm-slim', desc: 'Image runtime minimale' },
                                { flag: 'scratch', desc: 'Image vide (binaire statique)' }
                            ],
                            examples: [
                                { code: '# Dockerfile\nFROM rust:1.75-slim AS builder\nWORKDIR /app\nCOPY . .\nRUN cargo build --release\n\nFROM debian:bookworm-slim\nRUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*\nCOPY --from=builder /app/target/release/myapp /usr/local/bin/\nEXPOSE 3000\nCMD ["myapp"]', desc: 'Dockerfile multi-stage' }
                            ],
                            tips: ['Utilisez .dockerignore pour exclure target/'],
                            warnings: ['Image finale ~50MB vs ~1GB avec rust:latest']
                        }
                    }
                ]
            },
            {
                id: 'data-engineering',
                title: 'üìä Data Engineering & ETL',
                icon: 'fa-database',
                color: 'border-l-4 border-blue-500',
                commands: [
                    {
                        cmd: 'Traiter des CSV volumineux avec Polars',
                        desc: 'LazyFrame pour fichiers multi-GB',
                        details: {
                            explanation: 'Polars est une alternative √† Pandas 10-100x plus rapide. LazyFrame permet l\'optimisation automatique des requ√™tes.',
                            syntax: 'cargo add polars --features lazy,csv',
                            options: [
                                { flag: 'lazy', desc: '√âvaluation paresseuse' },
                                { flag: 'csv', desc: 'Support CSV' },
                                { flag: 'parquet', desc: 'Support Parquet' }
                            ],
                            examples: [
                                { code: 'use polars::prelude::*;\n\nfn main() -> Result<(), PolarsError> {\n    // Lecture lazy (scan, pas load)\n    let lf = LazyCsvReader::new("data.csv")\n        .has_header(true)\n        .finish()?;\n    \n    // Pipeline ETL optimis√©\n    let result = lf\n        .filter(col("amount").gt(lit(100)))\n        .group_by([col("category")])\n        .agg([\n            col("amount").sum().alias("total"),\n            col("amount").mean().alias("avg"),\n            col("id").count().alias("count"),\n        ])\n        .sort("total", SortOptions::default().with_order_descending(true))\n        .limit(10)\n        .collect()?;  // Ex√©cute ici\n    \n    println!("{}", result);\n    Ok(())\n}', desc: 'Pipeline ETL complet' }
                            ],
                            tips: ['scan_csv > read_csv pour fichiers volumineux'],
                            warnings: ['collect() ex√©cute le plan optimis√©']
                        }
                    },
                    {
                        cmd: 'Construire un pipeline ETL async',
                        desc: 'tokio streams avec backpressure',
                        details: {
                            explanation: 'Utilisez les streams async pour traiter des donn√©es en flux avec contr√¥le de backpressure.',
                            syntax: 'cargo add tokio-stream futures',
                            options: [
                                { flag: 'StreamExt', desc: 'M√©thodes d\'extension stream' },
                                { flag: 'buffer_unordered', desc: 'Parall√©lisation avec limite' }
                            ],
                            examples: [
                                { code: 'use tokio_stream::{StreamExt, wrappers::ReceiverStream};\nuse futures::stream;\n\nasync fn process_batch(items: Vec<Record>) -> Vec<ProcessedRecord> {\n    // Traitement async\n}\n\n#[tokio::main]\nasync fn main() {\n    let records: Vec<Record> = load_records();\n    \n    // Pipeline avec backpressure (max 10 en parall√®le)\n    let results: Vec<_> = stream::iter(records)\n        .chunks(100)  // Batch de 100\n        .map(|batch| process_batch(batch))\n        .buffer_unordered(10)  // 10 batches en parall√®le\n        .collect()\n        .await;\n}', desc: 'Pipeline avec backpressure' }
                            ],
                            tips: ['buffer_unordered pour parall√©lisme contr√¥l√©'],
                            warnings: ['Attention √† la consommation m√©moire']
                        }
                    },
                    {
                        cmd: 'Lire/√©crire du Parquet',
                        desc: 'Format columnar haute performance',
                        details: {
                            explanation: 'Parquet est le format standard pour le Big Data. Polars le supporte nativement avec compression.',
                            syntax: 'cargo add polars --features parquet',
                            options: [
                                { flag: 'CompressionMethod::Snappy', desc: 'Compression rapide' },
                                { flag: 'CompressionMethod::Zstd', desc: 'Meilleure compression' }
                            ],
                            examples: [
                                { code: 'use polars::prelude::*;\n\nfn main() -> Result<(), PolarsError> {\n    // Lecture Parquet\n    let df = LazyFrame::scan_parquet("data.parquet", Default::default())?\n        .filter(col("date").gt(lit("2024-01-01")))\n        .collect()?;\n    \n    // √âcriture Parquet avec compression\n    let mut file = std::fs::File::create("output.parquet")?;\n    ParquetWriter::new(&mut file)\n        .with_compression(ParquetCompression::Zstd(None))\n        .finish(&mut df.clone())?;\n    \n    Ok(())\n}', desc: 'Lecture/√©criture Parquet' }
                            ],
                            tips: ['Parquet = 10x moins d\'espace que CSV'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Connecter Kafka pour streaming',
                        desc: 'rdkafka consumer/producer',
                        details: {
                            explanation: 'rdkafka est le binding Rust officiel pour librdkafka, utilis√© pour Kafka en production.',
                            syntax: 'cargo add rdkafka --features cmake-build',
                            options: [
                                { flag: 'StreamConsumer', desc: 'Consumer async' },
                                { flag: 'FutureProducer', desc: 'Producer async' }
                            ],
                            examples: [
                                { code: 'use rdkafka::config::ClientConfig;\nuse rdkafka::consumer::{StreamConsumer, Consumer};\nuse rdkafka::Message;\nuse tokio_stream::StreamExt;\n\n#[tokio::main]\nasync fn main() {\n    let consumer: StreamConsumer = ClientConfig::new()\n        .set("bootstrap.servers", "localhost:9092")\n        .set("group.id", "my-group")\n        .create()\n        .expect("Consumer creation failed");\n    \n    consumer.subscribe(&["my-topic"]).unwrap();\n    \n    let mut stream = consumer.stream();\n    while let Some(message) = stream.next().await {\n        match message {\n            Ok(m) => {\n                let payload = m.payload_view::<str>().unwrap().unwrap();\n                println!("Received: {}", payload);\n            }\n            Err(e) => eprintln!("Error: {}", e),\n        }\n    }\n}', desc: 'Consumer Kafka async' }
                            ],
                            tips: ['Utilisez tokio::spawn pour consumer parall√®le'],
                            warnings: ['Requiert librdkafka install√©']
                        }
                    },
                    {
                        cmd: 'Parall√©liser avec Rayon',
                        desc: 'par_iter pour CPU-bound tasks',
                        details: {
                            explanation: 'Rayon transforme les it√©rateurs s√©quentiels en parall√®les avec une API simple.',
                            syntax: 'cargo add rayon',
                            options: [
                                { flag: 'par_iter()', desc: 'It√©rateur parall√®le' },
                                { flag: 'par_chunks()', desc: 'Chunks parall√®les' }
                            ],
                            examples: [
                                { code: 'use rayon::prelude::*;\n\nfn main() {\n    let data: Vec<i32> = (0..1_000_000).collect();\n    \n    // Transformation parall√®le\n    let results: Vec<i32> = data\n        .par_iter()\n        .map(|x| expensive_computation(*x))\n        .filter(|x| *x > 100)\n        .collect();\n    \n    // R√©duction parall√®le\n    let sum: i64 = data\n        .par_iter()\n        .map(|x| (*x as i64).pow(2))\n        .sum();\n}', desc: 'Map/reduce parall√®le' }
                            ],
                            tips: ['Rayon choisit automatiquement le nombre de threads'],
                            warnings: ['√âvitez pour I/O-bound (utilisez tokio)']
                        }
                    },
                    {
                        cmd: 'G√©rer des fichiers JSON volumineux',
                        desc: 'serde_json streaming',
                        details: {
                            explanation: 'Pour les fichiers JSON trop gros pour la m√©moire, utilisez le streaming avec serde_json.',
                            syntax: 'cargo add serde_json',
                            options: [
                                { flag: 'Deserializer::from_reader', desc: 'Lecture streaming' },
                                { flag: 'StreamDeserializer', desc: 'Flux d\'objets' }
                            ],
                            examples: [
                                { code: 'use serde::{Deserialize, Serialize};\nuse serde_json::Deserializer;\nuse std::fs::File;\nuse std::io::BufReader;\n\n#[derive(Deserialize)]\nstruct Record { id: u32, value: String }\n\nfn main() -> std::io::Result<()> {\n    let file = File::open("large.json")?;\n    let reader = BufReader::new(file);\n    \n    // Stream d\'objets JSON (un par ligne - JSONL)\n    let stream = Deserializer::from_reader(reader)\n        .into_iter::<Record>();\n    \n    for result in stream {\n        match result {\n            Ok(record) => println!("{}: {}", record.id, record.value),\n            Err(e) => eprintln!("Error: {}", e),\n        }\n    }\n    Ok(())\n}', desc: 'Streaming JSONL' }
                            ],
                            tips: ['Utilisez JSONL (JSON Lines) pour streaming'],
                            warnings: []
                        }
                    }
                ]
            },
            {
                id: 'ml-embedded',
                title: 'üß† ML Embarqu√© & Inference',
                icon: 'fa-microchip',
                color: 'border-l-4 border-green-500',
                commands: [
                    {
                        cmd: 'Charger un mod√®le ONNX',
                        desc: 'onnxruntime ou tract pour inference',
                        details: {
                            explanation: 'ONNX est le format standard pour l\'√©change de mod√®les ML. Tract et onnxruntime permettent l\'inf√©rence en Rust.',
                            syntax: 'cargo add tract-onnx',
                            options: [
                                { flag: 'tract-onnx', desc: 'Pure Rust, portable' },
                                { flag: 'ort', desc: 'Bindings onnxruntime (plus rapide)' }
                            ],
                            examples: [
                                { code: 'use tract_onnx::prelude::*;\n\nfn main() -> TractResult<()> {\n    // Charger le mod√®le\n    let model = tract_onnx::onnx()\n        .model_for_path("model.onnx")?\n        .with_input_fact(0, f32::fact([1, 3, 224, 224]).into())?\n        .into_optimized()?\n        .into_runnable()?;\n    \n    // Pr√©parer l\'entr√©e\n    let input: Tensor = tract_ndarray::Array4::<f32>::zeros((1, 3, 224, 224)).into();\n    \n    // Inf√©rence\n    let result = model.run(tvec!(input.into()))?;\n    let output = result[0].to_array_view::<f32>()?;\n    \n    println!("Output shape: {:?}", output.shape());\n    Ok(())\n}', desc: 'Inference avec Tract' }
                            ],
                            tips: ['Tract est pure Rust = pas de d√©pendances C'],
                            warnings: ['Convertissez vos mod√®les PyTorch/TF en ONNX']
                        }
                    },
                    {
                        cmd: 'Ex√©cuter une inf√©rence sur edge',
                        desc: 'Optimisation m√©moire et performance',
                        details: {
                            explanation: 'Pour l\'edge computing, optimisez la m√©moire et utilisez des mod√®les quantifi√©s.',
                            syntax: 'cargo add tract-onnx --features pulse',
                            options: [
                                { flag: 'into_optimized()', desc: 'Optimisation du graphe' },
                                { flag: 'concretize_symbols()', desc: 'Forcer dimensions statiques' }
                            ],
                            examples: [
                                { code: 'use tract_onnx::prelude::*;\n\nfn main() -> TractResult<()> {\n    // Optimisations pour edge\n    let model = tract_onnx::onnx()\n        .model_for_path("model.onnx")?\n        .with_input_fact(0, f32::fact([1, 3, 224, 224]).into())?\n        .into_optimized()?  // Fusionne op√©rations\n        .into_runnable()?;\n    \n    // Pr√©-allouer le buffer de sortie\n    let mut output_buffer = vec![0f32; 1000];\n    \n    // Boucle d\'inf√©rence optimis√©e\n    for frame in camera_frames() {\n        let input = preprocess(frame);\n        let result = model.run(tvec!(input.into()))?;\n        // Copie directe sans allocation\n        result[0].as_slice::<f32>()?.iter()\n            .zip(output_buffer.iter_mut())\n            .for_each(|(src, dst)| *dst = *src);\n    }\n    Ok(())\n}', desc: 'Boucle d\'inf√©rence optimis√©e' }
                            ],
                            tips: ['R√©utilisez les buffers pour √©viter allocations'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'D√©ployer sur Raspberry Pi',
                        desc: 'Cross-compilation ARM',
                        details: {
                            explanation: 'Cross-compilez pour ARM Linux (Raspberry Pi) depuis votre machine de d√©veloppement.',
                            syntax: 'rustup target add aarch64-unknown-linux-gnu',
                            options: [
                                { flag: 'aarch64-unknown-linux-gnu', desc: 'RPi 64-bit' },
                                { flag: 'armv7-unknown-linux-gnueabihf', desc: 'RPi 32-bit' }
                            ],
                            examples: [
                                { code: '# Installation cross-compiler\nsudo apt install gcc-aarch64-linux-gnu\n\n# .cargo/config.toml\n[target.aarch64-unknown-linux-gnu]\nlinker = "aarch64-linux-gnu-gcc"\n\n# Compilation\ncargo build --release --target aarch64-unknown-linux-gnu\n\n# Copier sur RPi\nscp target/aarch64-unknown-linux-gnu/release/myapp pi@raspberrypi:~/', desc: 'Cross-compilation ARM' },
                                { code: '# Avec cross (plus simple)\ncargo install cross\ncross build --release --target aarch64-unknown-linux-gnu', desc: 'Avec l\'outil cross' }
                            ],
                            tips: ['cross utilise Docker pour simplifier'],
                            warnings: ['V√©rifiez la compatibilit√© des crates natifs']
                        }
                    },
                    {
                        cmd: 'Optimiser pour contraintes m√©moire',
                        desc: 'heapless et buffers statiques',
                        details: {
                            explanation: 'Pour l\'embarqu√© sans allocateur, utilisez heapless pour des structures √† taille fixe.',
                            syntax: 'cargo add heapless',
                            options: [
                                { flag: 'Vec', desc: 'Vecteur √† capacit√© fixe' },
                                { flag: 'String', desc: 'String √† capacit√© fixe' },
                                { flag: 'FnvIndexMap', desc: 'HashMap sans alloc' }
                            ],
                            examples: [
                                { code: 'use heapless::{Vec, String};\n\n// Vec avec capacit√© max de 32 √©l√©ments\nlet mut vec: Vec<u8, 32> = Vec::new();\nvec.push(1).unwrap();\nvec.push(2).unwrap();\n\n// String avec max 64 caract√®res\nlet mut s: String<64> = String::new();\ns.push_str("Hello").unwrap();\n\n// Buffer statique pour donn√©es sensor\nstatic mut BUFFER: [f32; 256] = [0.0; 256];\n\nfn process_sensor_data(data: &[f32]) {\n    // Utilise le buffer statique\n    unsafe {\n        BUFFER[..data.len()].copy_from_slice(data);\n    }\n}', desc: 'Structures sans allocation' }
                            ],
                            tips: ['Pr√©f√©rez les types g√©n√©riques const N'],
                            warnings: ['push() retourne Err si capacit√© atteinte']
                        }
                    },
                    {
                        cmd: 'Int√©grer TensorFlow Lite',
                        desc: 'tflite-rs pour mod√®les quantifi√©s',
                        details: {
                            explanation: 'TensorFlow Lite est optimis√© pour les mod√®les quantifi√©s INT8 sur appareils mobiles/embarqu√©s.',
                            syntax: 'cargo add tflitec',
                            options: [
                                { flag: 'Interpreter', desc: 'Ex√©cuteur TFLite' },
                                { flag: 'with_threads', desc: 'Multi-threading' }
                            ],
                            examples: [
                                { code: 'use tflitec::interpreter::{Interpreter, Options};\nuse tflitec::tensor::Tensor;\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    let options = Options::default();\n    let interpreter = Interpreter::with_model_path("model.tflite", Some(options))?;\n    \n    interpreter.allocate_tensors()?;\n    \n    // Remplir l\'entr√©e\n    let input = interpreter.input(0)?;\n    // input.data_mut() pour modifier les donn√©es\n    \n    // Inf√©rence\n    interpreter.invoke()?;\n    \n    // R√©cup√©rer la sortie\n    let output = interpreter.output(0)?;\n    let data = output.data::<f32>();\n    \n    Ok(())\n}', desc: 'Inference TFLite' }
                            ],
                            tips: ['Mod√®les INT8 = 4x plus petits, souvent plus rapides'],
                            warnings: ['N√©cessite le runtime TFLite C']
                        }
                    }
                ]
            },
            {
                id: 'data-science',
                title: 'üìà Data Science avec Polars',
                icon: 'fa-chart-line',
                color: 'border-l-4 border-cyan-500',
                commands: [
                    {
                        cmd: 'Cr√©er et manipuler des DataFrames',
                        desc: 'DataFrame, Series, select',
                        details: {
                            explanation: 'Polars offre une API similaire √† Pandas mais optimis√©e pour la performance.',
                            syntax: 'cargo add polars --features lazy',
                            options: [
                                { flag: 'df!', desc: 'Macro pour cr√©er DataFrame' },
                                { flag: 'Series', desc: 'Colonne typ√©e' }
                            ],
                            examples: [
                                { code: 'use polars::prelude::*;\n\nfn main() -> Result<(), PolarsError> {\n    // Cr√©er un DataFrame\n    let df = df! [\n        "name" => ["Alice", "Bob", "Charlie"],\n        "age" => [25, 30, 35],\n        "salary" => [50000.0, 60000.0, 70000.0]\n    ]?;\n    \n    // S√©lectionner colonnes\n    let selected = df.select(["name", "age"])?;\n    \n    // Acc√©der √† une colonne\n    let ages = df.column("age")?;\n    println!("Mean age: {}", ages.mean().unwrap());\n    \n    // Ajouter une colonne calcul√©e\n    let df = df.lazy()\n        .with_column((col("salary") * lit(1.1)).alias("new_salary"))\n        .collect()?;\n    \n    println!("{}", df);\n    Ok(())\n}', desc: 'Op√©rations DataFrame basiques' }
                            ],
                            tips: ['Utilisez lazy() pour cha√Æner les op√©rations'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Filtrer et transformer les donn√©es',
                        desc: 'filter, with_columns, apply',
                        details: {
                            explanation: 'Polars permet des filtres et transformations expressifs avec une syntaxe d√©clarative.',
                            syntax: 'col("column").filter(...).with_columns(...)',
                            options: [
                                { flag: 'filter()', desc: 'Filtrer les lignes' },
                                { flag: 'with_columns()', desc: 'Ajouter/modifier colonnes' },
                                { flag: 'apply()', desc: 'Fonction personnalis√©e' }
                            ],
                            examples: [
                                { code: 'use polars::prelude::*;\n\nfn main() -> Result<(), PolarsError> {\n    let df = df! [\n        "product" => ["A", "B", "A", "C", "B"],\n        "quantity" => [10, 20, 15, 5, 25],\n        "price" => [100.0, 200.0, 100.0, 300.0, 200.0]\n    ]?;\n    \n    let result = df.lazy()\n        // Filtrer\n        .filter(col("quantity").gt(lit(10)))\n        // Ajouter colonnes\n        .with_columns([\n            (col("quantity") * col("price")).alias("total"),\n            col("product").str().to_uppercase().alias("product_upper"),\n        ])\n        // Trier\n        .sort("total", SortOptions::default().with_order_descending(true))\n        .collect()?;\n    \n    println!("{}", result);\n    Ok(())\n}', desc: 'Filtrage et transformation' }
                            ],
                            tips: ['Cha√Ænez lazy operations avant collect()'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Agr√©ger et grouper',
                        desc: 'group_by, agg, pivot',
                        details: {
                            explanation: 'Les agr√©gations Polars sont hautement optimis√©es avec parall√©lisation automatique.',
                            syntax: 'df.group_by([cols]).agg([aggregations])',
                            options: [
                                { flag: 'sum()', desc: 'Somme' },
                                { flag: 'mean()', desc: 'Moyenne' },
                                { flag: 'count()', desc: 'Comptage' },
                                { flag: 'first()/last()', desc: 'Premier/dernier' }
                            ],
                            examples: [
                                { code: 'use polars::prelude::*;\n\nfn main() -> Result<(), PolarsError> {\n    let df = df! [\n        "category" => ["A", "B", "A", "B", "A"],\n        "value" => [10, 20, 30, 40, 50]\n    ]?;\n    \n    // Group by avec agr√©gations multiples\n    let result = df.lazy()\n        .group_by([col("category")])\n        .agg([\n            col("value").sum().alias("total"),\n            col("value").mean().alias("average"),\n            col("value").count().alias("count"),\n            col("value").std(1).alias("std_dev"),\n        ])\n        .collect()?;\n    \n    println!("{}", result);\n    Ok(())\n}', desc: 'Agr√©gations multiples' }
                            ],
                            tips: ['Polars parall√©lise automatiquement les agr√©gations'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Joindre des datasets',
                        desc: 'join, concat, merge',
                        details: {
                            explanation: 'Polars supporte tous les types de jointures avec optimisation automatique.',
                            syntax: 'df1.join(&df2, [on], [on], JoinType)',
                            options: [
                                { flag: 'JoinType::Inner', desc: 'Inner join' },
                                { flag: 'JoinType::Left', desc: 'Left join' },
                                { flag: 'JoinType::Outer', desc: 'Full outer join' }
                            ],
                            examples: [
                                { code: 'use polars::prelude::*;\n\nfn main() -> Result<(), PolarsError> {\n    let users = df! [\n        "user_id" => [1, 2, 3],\n        "name" => ["Alice", "Bob", "Charlie"]\n    ]?;\n    \n    let orders = df! [\n        "order_id" => [101, 102, 103],\n        "user_id" => [1, 1, 2],\n        "amount" => [100.0, 150.0, 200.0]\n    ]?;\n    \n    // Left join\n    let result = users.lazy()\n        .join(\n            orders.lazy(),\n            [col("user_id")],\n            [col("user_id")],\n            JoinArgs::new(JoinType::Left)\n        )\n        .collect()?;\n    \n    // Concat vertical\n    let df1 = df!["a" => [1, 2]]?;\n    let df2 = df!["a" => [3, 4]]?;\n    let combined = concat([df1.lazy(), df2.lazy()], UnionArgs::default())?.collect()?;\n    \n    Ok(())\n}', desc: 'Jointures et concat√©nation' }
                            ],
                            tips: ['Utilisez lazy joins pour optimisation'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Visualiser avec Plotters',
                        desc: 'Charts et export PNG/SVG',
                        details: {
                            explanation: 'Plotters est la biblioth√®que de visualisation principale en Rust, pure Rust sans d√©pendances.',
                            syntax: 'cargo add plotters',
                            options: [
                                { flag: 'BitMapBackend', desc: 'Export PNG' },
                                { flag: 'SVGBackend', desc: 'Export SVG' },
                                { flag: 'LineSeries', desc: 'Graphique ligne' }
                            ],
                            examples: [
                                { code: 'use plotters::prelude::*;\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new("chart.png", (800, 600)).into_drawing_area();\n    root.fill(&WHITE)?;\n    \n    let mut chart = ChartBuilder::on(&root)\n        .caption("Sales Over Time", ("sans-serif", 40))\n        .margin(10)\n        .x_label_area_size(40)\n        .y_label_area_size(50)\n        .build_cartesian_2d(0..12, 0..100)?;\n    \n    chart.configure_mesh().draw()?;\n    \n    let data = vec![(1, 20), (2, 35), (3, 45), (4, 60), (5, 55)];\n    \n    chart.draw_series(LineSeries::new(data.clone(), &RED))?\n        .label("Product A")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n    \n    chart.configure_series_labels()\n        .border_style(&BLACK)\n        .draw()?;\n    \n    root.present()?;\n    Ok(())\n}', desc: 'Line chart avec l√©gende' }
                            ],
                            tips: ['SVGBackend pour graphiques web'],
                            warnings: []
                        }
                    }
                ]
            },
            {
                id: 'nlp-pipeline',
                title: 'üìù Pipeline NLP',
                icon: 'fa-language',
                color: 'border-l-4 border-purple-500',
                commands: [
                    {
                        cmd: 'Tokenizer avec HuggingFace',
                        desc: 'tokenizers crate pour BPE, WordPiece',
                        details: {
                            explanation: 'Le crate tokenizers de HuggingFace offre des tokenizers ultra-rapides en Rust pur.',
                            syntax: 'cargo add tokenizers',
                            options: [
                                { flag: 'BPE', desc: 'Byte-Pair Encoding' },
                                { flag: 'WordPiece', desc: 'WordPiece (BERT)' },
                                { flag: 'Unigram', desc: 'Unigram (SentencePiece)' }
                            ],
                            examples: [
                                { code: 'use tokenizers::tokenizer::Tokenizer;\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Charger tokenizer pr√©-entra√Æn√©\n    let tokenizer = Tokenizer::from_file("tokenizer.json")?;\n    \n    let text = "Hello, how are you?";\n    let encoding = tokenizer.encode(text, false)?;\n    \n    println!("Tokens: {:?}", encoding.get_tokens());\n    println!("IDs: {:?}", encoding.get_ids());\n    println!("Attention mask: {:?}", encoding.get_attention_mask());\n    \n    // Batch encoding\n    let texts = vec!["Hello world", "Rust is fast"];\n    let encodings = tokenizer.encode_batch(texts, false)?;\n    \n    Ok(())\n}', desc: 'Tokenization basique' }
                            ],
                            tips: ['T√©l√©chargez tokenizer.json depuis HuggingFace Hub'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'G√©n√©rer des embeddings',
                        desc: 'rust-bert pour sentence embeddings',
                        details: {
                            explanation: 'rust-bert permet d\'ex√©cuter des mod√®les BERT et d√©riv√©s pour g√©n√©rer des embeddings.',
                            syntax: 'cargo add rust-bert',
                            options: [
                                { flag: 'SentenceEmbeddingsModel', desc: 'Mod√®le sentence embeddings' },
                                { flag: 'all-MiniLM-L6-v2', desc: 'Mod√®le l√©ger recommand√©' }
                            ],
                            examples: [
                                { code: 'use rust_bert::pipelines::sentence_embeddings::{\n    SentenceEmbeddingsBuilder, SentenceEmbeddingsModelType\n};\n\nfn main() -> anyhow::Result<()> {\n    let model = SentenceEmbeddingsBuilder::remote(\n        SentenceEmbeddingsModelType::AllMiniLmL6V2\n    ).create_model()?;\n    \n    let sentences = [\n        "This is an example sentence",\n        "Each sentence is converted to a vector",\n    ];\n    \n    let embeddings = model.encode(&sentences)?;\n    \n    // embeddings: Vec<Vec<f32>>\n    println!("Dimension: {}", embeddings[0].len());\n    \n    Ok(())\n}', desc: 'Sentence embeddings' }
                            ],
                            tips: ['all-MiniLM-L6-v2 est rapide et bon pour la similarit√©'],
                            warnings: ['Premier appel t√©l√©charge le mod√®le (~90MB)']
                        }
                    },
                    {
                        cmd: 'Classification de texte',
                        desc: 'Sentiment analysis et classification',
                        details: {
                            explanation: 'Utilisez des mod√®les pr√©-entra√Æn√©s pour classifier des textes (sentiment, cat√©gorie, etc.).',
                            syntax: 'cargo add rust-bert',
                            options: [
                                { flag: 'SentimentModel', desc: 'Analyse de sentiment' },
                                { flag: 'ZeroShotClassificationModel', desc: 'Classification zero-shot' }
                            ],
                            examples: [
                                { code: 'use rust_bert::pipelines::sentiment::SentimentModel;\n\nfn main() -> anyhow::Result<()> {\n    let model = SentimentModel::new(Default::default())?;\n    \n    let texts = [\n        "I love this product!",\n        "This is terrible.",\n        "It is okay, nothing special."\n    ];\n    \n    let sentiments = model.predict(&texts);\n    \n    for (text, sentiment) in texts.iter().zip(sentiments.iter()) {\n        println!("{}: {:?} (score: {:.2})",\n            text, sentiment.polarity, sentiment.score);\n    }\n    \n    Ok(())\n}', desc: 'Analyse de sentiment' }
                            ],
                            tips: ['Zero-shot permet de classifier sans entra√Ænement'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Extraction d\'entit√©s nomm√©es',
                        desc: 'NER avec mod√®les pr√©-entra√Æn√©s',
                        details: {
                            explanation: 'NER (Named Entity Recognition) identifie les personnes, organisations, lieux, etc. dans un texte.',
                            syntax: 'cargo add rust-bert',
                            options: [
                                { flag: 'NERModel', desc: 'Mod√®le NER' },
                                { flag: 'Entity', desc: 'Entit√© d√©tect√©e' }
                            ],
                            examples: [
                                { code: 'use rust_bert::pipelines::ner::NERModel;\n\nfn main() -> anyhow::Result<()> {\n    let model = NERModel::new(Default::default())?;\n    \n    let texts = [\n        "Apple Inc. was founded by Steve Jobs in California.",\n        "The Eiffel Tower is located in Paris, France."\n    ];\n    \n    let entities = model.predict(&texts);\n    \n    for (text, text_entities) in texts.iter().zip(entities.iter()) {\n        println!("Text: {}", text);\n        for entity in text_entities {\n            println!("  {} [{}]: {} (score: {:.2})",\n                entity.word, entity.label, entity.score);\n        }\n    }\n    \n    Ok(())\n}', desc: 'Extraction NER' }
                            ],
                            tips: ['Labels: PER (personne), ORG (organisation), LOC (lieu)'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Recherche s√©mantique',
                        desc: 'Embeddings + cosine similarity',
                        details: {
                            explanation: 'Trouvez des documents similaires en comparant leurs embeddings avec la similarit√© cosinus.',
                            syntax: 'Embeddings + calcul similarit√©',
                            options: [
                                { flag: 'cosine_similarity', desc: 'Similarit√© cosinus' },
                                { flag: 'dot_product', desc: 'Produit scalaire' }
                            ],
                            examples: [
                                { code: 'fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {\n    let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();\n    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();\n    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();\n    dot / (norm_a * norm_b)\n}\n\nfn semantic_search(\n    query_embedding: &[f32],\n    doc_embeddings: &[Vec<f32>],\n    top_k: usize\n) -> Vec<(usize, f32)> {\n    let mut scores: Vec<_> = doc_embeddings.iter()\n        .enumerate()\n        .map(|(i, emb)| (i, cosine_similarity(query_embedding, emb)))\n        .collect();\n    \n    scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());\n    scores.truncate(top_k);\n    scores\n}\n\n// Usage avec rust-bert embeddings\nlet results = semantic_search(&query_emb, &doc_embeddings, 5);', desc: 'Recherche s√©mantique' }
                            ],
                            tips: ['Pr√©-calculez les embeddings des documents'],
                            warnings: ['Pour de gros corpus, utilisez une DB vectorielle']
                        }
                    }
                ]
            },
            {
                id: 'multimodal-ai',
                title: 'üñºÔ∏è IA Multimodale',
                icon: 'fa-images',
                color: 'border-l-4 border-pink-500',
                commands: [
                    {
                        cmd: 'Charger et pr√©traiter des images',
                        desc: 'image crate pour manipulation',
                        details: {
                            explanation: 'Le crate image permet de charger, redimensionner et transformer des images pour l\'inf√©rence.',
                            syntax: 'cargo add image',
                            options: [
                                { flag: 'DynamicImage', desc: 'Image dynamique' },
                                { flag: 'ImageBuffer', desc: 'Buffer typ√©' },
                                { flag: 'resize', desc: 'Redimensionnement' }
                            ],
                            examples: [
                                { code: 'use image::{DynamicImage, GenericImageView, imageops::FilterType};\n\nfn preprocess_for_model(path: &str) -> Vec<f32> {\n    let img = image::open(path).unwrap();\n    \n    // Redimensionner √† 224x224\n    let resized = img.resize_exact(224, 224, FilterType::Triangle);\n    \n    // Convertir en RGB\n    let rgb = resized.to_rgb8();\n    \n    // Normaliser et convertir en tensor [C, H, W]\n    let mean = [0.485, 0.456, 0.406];\n    let std = [0.229, 0.224, 0.225];\n    \n    let mut tensor = vec![0f32; 3 * 224 * 224];\n    for (x, y, pixel) in rgb.enumerate_pixels() {\n        let idx = (y * 224 + x) as usize;\n        for c in 0..3 {\n            let val = pixel[c] as f32 / 255.0;\n            tensor[c * 224 * 224 + idx] = (val - mean[c]) / std[c];\n        }\n    }\n    tensor\n}', desc: 'Pr√©traitement pour CNN' }
                            ],
                            tips: ['Utilisez ImageNet mean/std pour mod√®les pr√©-entra√Æn√©s'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Inf√©rence vision avec Candle',
                        desc: 'Framework ML de HuggingFace en Rust',
                        details: {
                            explanation: 'Candle est le framework ML de HuggingFace en Rust, optimis√© pour l\'inf√©rence.',
                            syntax: 'cargo add candle-core candle-nn candle-transformers',
                            options: [
                                { flag: 'candle-core', desc: 'Tenseurs et op√©rations' },
                                { flag: 'candle-nn', desc: 'Couches neuronales' },
                                { flag: 'candle-transformers', desc: 'Mod√®les pr√©-entra√Æn√©s' }
                            ],
                            examples: [
                                { code: 'use candle_core::{Device, Tensor};\nuse candle_nn::VarBuilder;\n\nfn main() -> candle_core::Result<()> {\n    let device = Device::Cpu; // ou Device::new_cuda(0)?\n    \n    // Charger les poids\n    let vb = VarBuilder::from_pth("model.pth", candle_core::DType::F32, &device)?;\n    \n    // Cr√©er un tensor d\'entr√©e [batch, channels, height, width]\n    let input = Tensor::zeros((1, 3, 224, 224), candle_core::DType::F32, &device)?;\n    \n    // Inf√©rence (exemple simplifi√©)\n    let output = model.forward(&input)?;\n    \n    let logits = output.to_vec2::<f32>()?;\n    println!("Output shape: {:?}", output.shape());\n    \n    Ok(())\n}', desc: 'Inf√©rence avec Candle' }
                            ],
                            tips: ['Candle supporte CUDA pour acc√©l√©ration GPU'],
                            warnings: ['API encore en √©volution']
                        }
                    },
                    {
                        cmd: 'Pipeline image ‚Üí texte',
                        desc: 'CLIP pour vision-langage',
                        details: {
                            explanation: 'CLIP (Contrastive Language-Image Pre-training) lie images et texte dans un espace commun.',
                            syntax: 'cargo add candle-transformers',
                            options: [
                                { flag: 'ClipModel', desc: 'Mod√®le CLIP' },
                                { flag: 'encode_image', desc: 'Encoder une image' },
                                { flag: 'encode_text', desc: 'Encoder du texte' }
                            ],
                            examples: [
                                { code: '// Pseudo-code CLIP avec Candle\nuse candle_transformers::models::clip;\n\nfn image_text_similarity(\n    image_embeddings: &Tensor,\n    text_embeddings: &Tensor,\n) -> candle_core::Result<Tensor> {\n    // Normaliser les embeddings\n    let image_norm = image_embeddings.normalize(2, 1)?;\n    let text_norm = text_embeddings.normalize(2, 1)?;\n    \n    // Similarit√© cosinus via matmul\n    let similarity = image_norm.matmul(&text_norm.t()?)?;\n    \n    // Softmax pour probabilit√©s\n    let logits = (similarity * 100.0)?; // temperature scaling\n    candle_nn::ops::softmax(&logits, 1)\n}\n\n// Usage: trouver quelle description correspond √† l\'image\nlet labels = ["a photo of a cat", "a photo of a dog", "a photo of a car"];\nlet probs = image_text_similarity(&img_emb, &text_embs)?;', desc: 'Similarit√© image-texte CLIP' }
                            ],
                            tips: ['CLIP excellent pour zero-shot classification'],
                            warnings: ['Mod√®les CLIP volumineux (~400MB+)']
                        }
                    },
                    {
                        cmd: 'Traitement audio',
                        desc: 'symphonia pour d√©codage, mel spectrograms',
                        details: {
                            explanation: 'Symphonia d√©code l\'audio, puis convertissez en spectrogrammes mel pour les mod√®les audio.',
                            syntax: 'cargo add symphonia --features mp3,wav',
                            options: [
                                { flag: 'Decoder', desc: 'D√©codeur audio' },
                                { flag: 'SampleBuffer', desc: 'Buffer d\'√©chantillons' }
                            ],
                            examples: [
                                { code: 'use symphonia::core::codecs::DecoderOptions;\nuse symphonia::core::formats::FormatOptions;\nuse symphonia::core::io::MediaSourceStream;\nuse symphonia::core::probe::Hint;\n\nfn load_audio(path: &str) -> Vec<f32> {\n    let file = std::fs::File::open(path).unwrap();\n    let mss = MediaSourceStream::new(Box::new(file), Default::default());\n    \n    let probed = symphonia::default::get_probe()\n        .format(&Hint::new(), mss, &FormatOptions::default(), &Default::default())\n        .unwrap();\n    \n    let mut format = probed.format;\n    let track = format.default_track().unwrap();\n    let mut decoder = symphonia::default::get_codecs()\n        .make(&track.codec_params, &DecoderOptions::default())\n        .unwrap();\n    \n    let mut samples = Vec::new();\n    while let Ok(packet) = format.next_packet() {\n        let decoded = decoder.decode(&packet).unwrap();\n        // Extraire les samples...\n    }\n    samples\n}\n\n// Mel spectrogram (simplifi√©)\nfn to_mel_spectrogram(samples: &[f32], sr: u32) -> Vec<Vec<f32>> {\n    // FFT + filterbank mel\n    todo!()\n}', desc: 'Chargement et traitement audio' }
                            ],
                            tips: ['Pour Whisper, utilisez 16kHz mono'],
                            warnings: ['Mel spectrograms n√©cessitent rustfft']
                        }
                    },
                    {
                        cmd: 'Combiner vision et texte',
                        desc: 'Embeddings multimodaux',
                        details: {
                            explanation: 'Fusionnez les embeddings d\'image et de texte pour des t√¢ches multimodales.',
                            syntax: 'Concat√©nation ou projection commune',
                            options: [
                                { flag: 'Early fusion', desc: 'Fusion des entr√©es' },
                                { flag: 'Late fusion', desc: 'Fusion des embeddings' },
                                { flag: 'Cross-attention', desc: 'Attention crois√©e' }
                            ],
                            examples: [
                                { code: '// Late fusion: concat√©ner les embeddings\nfn fuse_embeddings(\n    image_emb: &[f32],  // dim: 512\n    text_emb: &[f32],   // dim: 768\n) -> Vec<f32> {\n    // Simple concat√©nation\n    let mut fused = Vec::with_capacity(image_emb.len() + text_emb.len());\n    fused.extend_from_slice(image_emb);\n    fused.extend_from_slice(text_emb);\n    fused  // dim: 1280\n}\n\n// Projection dans espace commun\nfn project_to_common_space(\n    emb: &[f32],\n    projection_matrix: &[Vec<f32>],  // [original_dim, target_dim]\n    target_dim: usize,\n) -> Vec<f32> {\n    let mut projected = vec![0f32; target_dim];\n    for (i, row) in projection_matrix.iter().enumerate() {\n        for (j, &w) in row.iter().enumerate() {\n            projected[j] += emb[i] * w;\n        }\n    }\n    projected\n}', desc: 'Fusion d\'embeddings' }
                            ],
                            tips: ['CLIP projette d√©j√† dans un espace commun'],
                            warnings: []
                        }
                    }
                ]
            },
            {
                id: 'async-concurrency',
                title: '‚ö° Async & Concurrence',
                icon: 'fa-bolt',
                color: 'border-l-4 border-violet-500',
                commands: [
                    {
                        cmd: 'Configurer Tokio correctement',
                        desc: 'Runtime flavors et features',
                        details: {
                            explanation: 'Tokio offre diff√©rentes configurations de runtime selon vos besoins de performance.',
                            syntax: 'cargo add tokio --features full',
                            options: [
                                { flag: 'rt-multi-thread', desc: 'Runtime multi-thread√©' },
                                { flag: 'rt', desc: 'Runtime single-thread' },
                                { flag: 'macros', desc: '#[tokio::main] et #[tokio::test]' }
                            ],
                            examples: [
                                { code: '// Multi-thread (d√©faut avec #[tokio::main])\n#[tokio::main]\nasync fn main() {\n    // Utilise tous les cores\n}\n\n// Single-thread (moins d\'overhead)\n#[tokio::main(flavor = "current_thread")]\nasync fn main() {\n    // Un seul thread\n}\n\n// Configuration manuelle\nuse tokio::runtime::Builder;\n\nfn main() {\n    let runtime = Builder::new_multi_thread()\n        .worker_threads(4)\n        .enable_all()\n        .build()\n        .unwrap();\n    \n    runtime.block_on(async {\n        // Code async\n    });\n}', desc: 'Configurations Tokio' }
                            ],
                            tips: ['current_thread pour CLI simples, multi_thread pour serveurs'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Ex√©cuter des t√¢ches en parall√®le',
                        desc: 'spawn, JoinSet, select!',
                        details: {
                            explanation: 'Lancez plusieurs t√¢ches async en parall√®le et r√©cup√©rez leurs r√©sultats.',
                            syntax: 'tokio::spawn / JoinSet / select!',
                            options: [
                                { flag: 'spawn', desc: 'Lance une t√¢che' },
                                { flag: 'JoinSet', desc: 'Groupe de t√¢ches' },
                                { flag: 'join!', desc: 'Attendre plusieurs futures' }
                            ],
                            examples: [
                                { code: 'use tokio::task::JoinSet;\n\n#[tokio::main]\nasync fn main() {\n    // M√©thode 1: join! pour quelques t√¢ches\n    let (a, b, c) = tokio::join!(\n        fetch_data("url1"),\n        fetch_data("url2"),\n        fetch_data("url3"),\n    );\n    \n    // M√©thode 2: JoinSet pour n t√¢ches\n    let mut set = JoinSet::new();\n    \n    for url in urls {\n        set.spawn(fetch_data(url));\n    }\n    \n    // R√©cup√©rer les r√©sultats au fur et √† mesure\n    while let Some(result) = set.join_next().await {\n        match result {\n            Ok(data) => println!("Got: {:?}", data),\n            Err(e) => eprintln!("Task failed: {}", e),\n        }\n    }\n}', desc: 'Parall√©lisme avec JoinSet' }
                            ],
                            tips: ['JoinSet permet d\'annuler toutes les t√¢ches'],
                            warnings: ['spawn retourne JoinHandle, n\'oubliez pas await']
                        }
                    },
                    {
                        cmd: 'Communiquer entre t√¢ches',
                        desc: 'mpsc, broadcast, watch channels',
                        details: {
                            explanation: 'Les channels permettent la communication s√ªre entre t√¢ches async.',
                            syntax: 'tokio::sync::{mpsc, broadcast, watch}',
                            options: [
                                { flag: 'mpsc', desc: 'Multi-producer, single-consumer' },
                                { flag: 'broadcast', desc: 'Multi-producer, multi-consumer' },
                                { flag: 'watch', desc: 'Single value, multi-consumer' }
                            ],
                            examples: [
                                { code: 'use tokio::sync::{mpsc, broadcast, watch};\n\n#[tokio::main]\nasync fn main() {\n    // MPSC: plusieurs producteurs, un consommateur\n    let (tx, mut rx) = mpsc::channel::<String>(100);\n    \n    tokio::spawn(async move {\n        tx.send("Hello".into()).await.unwrap();\n    });\n    \n    while let Some(msg) = rx.recv().await {\n        println!("Received: {}", msg);\n    }\n    \n    // Broadcast: tous les consumers re√ßoivent\n    let (tx, _) = broadcast::channel::<String>(100);\n    let mut rx1 = tx.subscribe();\n    let mut rx2 = tx.subscribe();\n    \n    tx.send("Broadcast!".into()).unwrap();\n    \n    // Watch: valeur observable\n    let (tx, mut rx) = watch::channel("initial");\n    tx.send("updated").unwrap();\n    println!("Current: {}", *rx.borrow());\n}', desc: 'Types de channels' }
                            ],
                            tips: ['watch parfait pour config dynamique'],
                            warnings: ['broadcast drop les messages si buffer plein']
                        }
                    },
                    {
                        cmd: 'G√©rer timeouts et annulations',
                        desc: 'tokio::select!, cancellation tokens',
                        details: {
                            explanation: 'G√©rez les timeouts et annulez proprement les t√¢ches en cours.',
                            syntax: 'tokio::select! / tokio::time::timeout',
                            options: [
                                { flag: 'timeout', desc: 'Timeout simple' },
                                { flag: 'select!', desc: 'Attendre le premier' },
                                { flag: 'CancellationToken', desc: 'Annulation coop√©rative' }
                            ],
                            examples: [
                                { code: 'use tokio::time::{timeout, Duration};\nuse tokio_util::sync::CancellationToken;\n\n#[tokio::main]\nasync fn main() {\n    // Timeout simple\n    match timeout(Duration::from_secs(5), slow_operation()).await {\n        Ok(result) => println!("Success: {:?}", result),\n        Err(_) => println!("Timeout!"),\n    }\n    \n    // select! pour le premier qui termine\n    tokio::select! {\n        result = operation_a() => println!("A finished: {:?}", result),\n        result = operation_b() => println!("B finished: {:?}", result),\n        _ = tokio::time::sleep(Duration::from_secs(10)) => {\n            println!("Global timeout");\n        }\n    }\n    \n    // Cancellation token\n    let token = CancellationToken::new();\n    let token_clone = token.clone();\n    \n    tokio::spawn(async move {\n        tokio::select! {\n            _ = long_running_task() => {},\n            _ = token_clone.cancelled() => {\n                println!("Task cancelled");\n            }\n        }\n    });\n    \n    // Plus tard: annuler\n    token.cancel();\n}', desc: 'Timeouts et annulation' }
                            ],
                            tips: ['CancellationToken est coop√©ratif, la t√¢che doit v√©rifier'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Limiter la concurrence',
                        desc: 'Semaphore, buffer_unordered',
                        details: {
                            explanation: 'Contr√¥lez le nombre de t√¢ches simultan√©es pour √©viter la surcharge.',
                            syntax: 'tokio::sync::Semaphore',
                            options: [
                                { flag: 'Semaphore', desc: 'Limite de concurrence' },
                                { flag: 'acquire', desc: 'Obtenir un permit' },
                                { flag: 'buffer_unordered', desc: 'Pour streams' }
                            ],
                            examples: [
                                { code: 'use tokio::sync::Semaphore;\nuse std::sync::Arc;\n\n#[tokio::main]\nasync fn main() {\n    let semaphore = Arc::new(Semaphore::new(10)); // Max 10 concurrent\n    let mut handles = vec![];\n    \n    for i in 0..100 {\n        let permit = semaphore.clone().acquire_owned().await.unwrap();\n        handles.push(tokio::spawn(async move {\n            // Le permit est d√©tenu pendant l\'op√©ration\n            let result = expensive_operation(i).await;\n            drop(permit); // Lib√®re explicitement (ou automatique √† la fin)\n            result\n        }));\n    }\n    \n    for handle in handles {\n        let _ = handle.await;\n    }\n}\n\n// Avec streams\nuse futures::stream::{self, StreamExt};\n\nlet results: Vec<_> = stream::iter(items)\n    .map(|item| process(item))\n    .buffer_unordered(10)  // Max 10 en parall√®le\n    .collect()\n    .await;', desc: 'Limitation de concurrence' }
                            ],
                            tips: ['buffer_unordered pour streams, Semaphore pour t√¢ches spawn'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'D√©boguer du code async',
                        desc: 'tokio-console et tracing',
                        details: {
                            explanation: 'tokio-console permet de visualiser les t√¢ches async en temps r√©el.',
                            syntax: 'cargo add console-subscriber',
                            options: [
                                { flag: 'console-subscriber', desc: 'Instrumenter pour tokio-console' },
                                { flag: 'tracing', desc: 'Logging structur√©' }
                            ],
                            examples: [
                                { code: '// Cargo.toml\n// tokio = { version = "1", features = ["full", "tracing"] }\n// console-subscriber = "0.2"\n\nuse console_subscriber;\n\n#[tokio::main]\nasync fn main() {\n    // Activer tokio-console\n    console_subscriber::init();\n    \n    // Votre code...\n    run_server().await;\n}\n\n// Lancer tokio-console dans un autre terminal:\n// $ tokio-console\n\n// Avec tracing pour logs structur√©s\nuse tracing::{info, instrument};\n\n#[instrument]\nasync fn process_request(id: u32) {\n    info!("Processing request");\n    // ...\n}', desc: 'Setup tokio-console' }
                            ],
                            tips: ['tokio-console montre les t√¢ches bloqu√©es'],
                            warnings: ['Overhead en production, utilisez feature flags']
                        }
                    }
                ]
            },
            {
                id: 'cli-tools',
                title: 'üîß CLI & Outils Syst√®me',
                icon: 'fa-terminal',
                color: 'border-l-4 border-amber-500',
                commands: [
                    {
                        cmd: 'Cr√©er un CLI avec Clap',
                        desc: 'derive macro et subcommands',
                        details: {
                            explanation: 'Clap est la biblioth√®que standard pour cr√©er des CLI en Rust avec parsing automatique.',
                            syntax: 'cargo add clap --features derive',
                            options: [
                                { flag: '#[command]', desc: 'Configure la commande' },
                                { flag: '#[arg]', desc: 'Configure un argument' },
                                { flag: 'Subcommand', desc: 'Sous-commandes' }
                            ],
                            examples: [
                                { code: 'use clap::{Parser, Subcommand};\n\n#[derive(Parser)]\n#[command(name = "myapp", version, about = "My awesome CLI")]\nstruct Cli {\n    /// Verbose output\n    #[arg(short, long)]\n    verbose: bool,\n    \n    /// Config file path\n    #[arg(short, long, default_value = "config.toml")]\n    config: String,\n    \n    #[command(subcommand)]\n    command: Commands,\n}\n\n#[derive(Subcommand)]\nenum Commands {\n    /// Initialize a new project\n    Init {\n        #[arg(short, long)]\n        name: String,\n    },\n    /// Run the server\n    Run {\n        #[arg(short, long, default_value = "8080")]\n        port: u16,\n    },\n}\n\nfn main() {\n    let cli = Cli::parse();\n    \n    match cli.command {\n        Commands::Init { name } => init_project(&name),\n        Commands::Run { port } => run_server(port),\n    }\n}', desc: 'CLI avec subcommands' }
                            ],
                            tips: ['/// comments deviennent l\'aide automatique'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Afficher une sortie color√©e',
                        desc: 'colored et indicatif progress bars',
                        details: {
                            explanation: 'Am√©liorez l\'UX de votre CLI avec des couleurs et barres de progression.',
                            syntax: 'cargo add colored indicatif',
                            options: [
                                { flag: 'colored', desc: 'Texte color√©' },
                                { flag: 'ProgressBar', desc: 'Barre de progression' },
                                { flag: 'MultiProgress', desc: 'Plusieurs barres' }
                            ],
                            examples: [
                                { code: 'use colored::*;\nuse indicatif::{ProgressBar, ProgressStyle};\n\nfn main() {\n    // Couleurs\n    println!("{}", "Success!".green().bold());\n    println!("{}", "Warning!".yellow());\n    println!("{}", "Error!".red().bold());\n    \n    // Progress bar\n    let pb = ProgressBar::new(100);\n    pb.set_style(ProgressStyle::default_bar()\n        .template("{spinner:.green} [{bar:40.cyan/blue}] {pos}/{len} {msg}")\n        .unwrap());\n    \n    for i in 0..100 {\n        pb.set_message(format!("Processing item {}", i));\n        pb.inc(1);\n        std::thread::sleep(std::time::Duration::from_millis(50));\n    }\n    pb.finish_with_message("Done!");\n}', desc: 'Couleurs et progress' }
                            ],
                            tips: ['indicatif g√®re automatiquement le terminal'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Lire/√©crire des fichiers de config',
                        desc: 'serde + toml/yaml',
                        details: {
                            explanation: 'G√©rez la configuration de votre application avec des fichiers TOML ou YAML.',
                            syntax: 'cargo add serde toml --features serde/derive',
                            options: [
                                { flag: 'toml', desc: 'Format TOML' },
                                { flag: 'serde_yaml', desc: 'Format YAML' },
                                { flag: 'directories', desc: 'Chemins standard OS' }
                            ],
                            examples: [
                                { code: 'use serde::{Deserialize, Serialize};\n\n#[derive(Serialize, Deserialize, Default)]\nstruct Config {\n    server: ServerConfig,\n    database: DatabaseConfig,\n}\n\n#[derive(Serialize, Deserialize, Default)]\nstruct ServerConfig {\n    host: String,\n    port: u16,\n}\n\n#[derive(Serialize, Deserialize, Default)]\nstruct DatabaseConfig {\n    url: String,\n}\n\nfn load_config(path: &str) -> Config {\n    let content = std::fs::read_to_string(path)\n        .unwrap_or_default();\n    toml::from_str(&content).unwrap_or_default()\n}\n\nfn save_config(config: &Config, path: &str) {\n    let content = toml::to_string_pretty(config).unwrap();\n    std::fs::write(path, content).unwrap();\n}\n\n// config.toml:\n// [server]\n// host = "localhost"\n// port = 8080\n// [database]\n// url = "postgres://..."', desc: 'Config TOML' }
                            ],
                            tips: ['Utilisez directories crate pour ~/.config/app/'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Ex√©cuter des commandes externes',
                        desc: 'std::process::Command',
                        details: {
                            explanation: 'Lancez des processus externes et capturez leur sortie.',
                            syntax: 'std::process::Command',
                            options: [
                                { flag: 'output()', desc: 'Capture stdout/stderr' },
                                { flag: 'status()', desc: 'Juste le code retour' },
                                { flag: 'spawn()', desc: 'Processus en arri√®re-plan' }
                            ],
                            examples: [
                                { code: 'use std::process::{Command, Stdio};\n\nfn main() -> std::io::Result<()> {\n    // Capturer la sortie\n    let output = Command::new("git")\n        .args(["status", "--short"])\n        .output()?;\n    \n    if output.status.success() {\n        let stdout = String::from_utf8_lossy(&output.stdout);\n        println!("Git status:\\n{}", stdout);\n    } else {\n        let stderr = String::from_utf8_lossy(&output.stderr);\n        eprintln!("Error: {}", stderr);\n    }\n    \n    // Pipe entre commandes\n    let ls = Command::new("ls")\n        .stdout(Stdio::piped())\n        .spawn()?;\n    \n    let grep = Command::new("grep")\n        .arg(".rs")\n        .stdin(ls.stdout.unwrap())\n        .output()?;\n    \n    println!("Rust files: {}", String::from_utf8_lossy(&grep.stdout));\n    \n    Ok(())\n}', desc: 'Ex√©cution et pipes' }
                            ],
                            tips: ['Utilisez which crate pour trouver les binaires'],
                            warnings: ['Attention aux injections de commandes!']
                        }
                    },
                    {
                        cmd: 'Surveiller des fichiers (watch)',
                        desc: 'notify-rs avec debouncing',
                        details: {
                            explanation: 'Surveillez les modifications de fichiers en temps r√©el.',
                            syntax: 'cargo add notify',
                            options: [
                                { flag: 'RecommendedWatcher', desc: 'Watcher optimal par OS' },
                                { flag: 'RecursiveMode', desc: 'Surveiller r√©cursivement' }
                            ],
                            examples: [
                                { code: 'use notify::{Watcher, RecommendedWatcher, RecursiveMode, Event};\nuse std::sync::mpsc::channel;\nuse std::time::Duration;\n\nfn main() -> notify::Result<()> {\n    let (tx, rx) = channel();\n    \n    let mut watcher = RecommendedWatcher::new(\n        move |res: Result<Event, _>| {\n            if let Ok(event) = res {\n                tx.send(event).unwrap();\n            }\n        },\n        notify::Config::default()\n            .with_poll_interval(Duration::from_secs(1)),\n    )?;\n    \n    watcher.watch("./src".as_ref(), RecursiveMode::Recursive)?;\n    \n    println!("Watching ./src for changes...");\n    \n    loop {\n        match rx.recv() {\n            Ok(event) => {\n                println!("Change detected: {:?}", event.paths);\n                // Rebuild, restart, etc.\n            }\n            Err(e) => println!("Watch error: {:?}", e),\n        }\n    }\n}', desc: 'File watcher' }
                            ],
                            tips: ['Ajoutez debouncing pour √©viter les triggers multiples'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Distribuer multi-plateforme',
                        desc: 'cross-compilation et cargo-dist',
                        details: {
                            explanation: 'Cr√©ez des binaires pour Windows, macOS et Linux depuis une seule machine.',
                            syntax: 'cargo install cargo-dist',
                            options: [
                                { flag: 'cargo-dist', desc: 'Distribution automatis√©e' },
                                { flag: 'cross', desc: 'Cross-compilation Docker' }
                            ],
                            examples: [
                                { code: '# Setup cargo-dist\ncargo dist init\n\n# G√©n√©rer les builds pour toutes les plateformes\ncargo dist build\n\n# Dans GitHub Actions (.github/workflows/release.yml)\nname: Release\non:\n  push:\n    tags: ["v*"]\njobs:\n  dist:\n    uses: axodotdev/cargo-dist/.github/workflows/cargo-dist.yml@v0.5.0\n    with:\n      plan: ""\n\n# Targets support√©s:\n# - x86_64-unknown-linux-gnu\n# - x86_64-apple-darwin\n# - aarch64-apple-darwin\n# - x86_64-pc-windows-msvc', desc: 'CI/CD avec cargo-dist' },
                                { code: '# Cross-compilation manuelle\ncargo install cross\n\n# Build pour Linux depuis macOS/Windows\ncross build --release --target x86_64-unknown-linux-gnu\n\n# Build pour Windows depuis Linux/macOS\ncross build --release --target x86_64-pc-windows-gnu', desc: 'Cross-compilation avec cross' }
                            ],
                            tips: ['cargo-dist g√©n√®re aussi les installers'],
                            warnings: ['Certaines crates natives ne cross-compilent pas']
                        }
                    }
                ]
            },
            {
                id: 'performance',
                title: 'üöÄ Performance & Optimisation',
                icon: 'fa-tachometer-alt',
                color: 'border-l-4 border-red-500',
                commands: [
                    {
                        cmd: 'Profiler avec Flamegraph',
                        desc: 'cargo-flamegraph pour visualiser',
                        details: {
                            explanation: 'Les flamegraphs montrent o√π votre programme passe son temps.',
                            syntax: 'cargo install flamegraph',
                            options: [
                                { flag: 'flamegraph', desc: 'G√©n√®re un SVG interactif' },
                                { flag: '--release', desc: 'Profiler le build optimis√©' }
                            ],
                            examples: [
                                { code: '# Installation\ncargo install flamegraph\n\n# Sur Linux, autoriser perf\necho -1 | sudo tee /proc/sys/kernel/perf_event_paranoid\n\n# G√©n√©rer le flamegraph\ncargo flamegraph --bin myapp -- arg1 arg2\n\n# R√©sultat: flamegraph.svg\n# Ouvrir dans un navigateur pour explorer interactivement\n\n# Pour un binaire existant\nflamegraph -o output.svg ./target/release/myapp', desc: 'G√©n√©rer un flamegraph' }
                            ],
                            tips: ['Les barres larges = temps pass√©, cherchez-les'],
                            warnings: ['Compilez avec debug symbols: profile.release.debug = true']
                        }
                    },
                    {
                        cmd: 'Benchmarker avec Criterion',
                        desc: 'Benchmarks statistiques fiables',
                        details: {
                            explanation: 'Criterion fournit des benchmarks avec analyse statistique et d√©tection de r√©gression.',
                            syntax: 'cargo add criterion --dev',
                            options: [
                                { flag: 'black_box', desc: '√âvite l\'optimisation du compilateur' },
                                { flag: 'criterion_group!', desc: 'Groupe de benchmarks' }
                            ],
                            examples: [
                                { code: '// benches/my_benchmark.rs\nuse criterion::{black_box, criterion_group, criterion_main, Criterion};\n\nfn fibonacci(n: u64) -> u64 {\n    match n {\n        0 => 1,\n        1 => 1,\n        n => fibonacci(n-1) + fibonacci(n-2),\n    }\n}\n\nfn criterion_benchmark(c: &mut Criterion) {\n    c.bench_function("fib 20", |b| {\n        b.iter(|| fibonacci(black_box(20)))\n    });\n    \n    // Comparaison de plusieurs impl√©mentations\n    let mut group = c.benchmark_group("fibonacci");\n    for size in [10, 15, 20].iter() {\n        group.bench_with_input(\n            format!("recursive_{}", size),\n            size,\n            |b, &s| b.iter(|| fibonacci(s))\n        );\n    }\n    group.finish();\n}\n\ncriterion_group!(benches, criterion_benchmark);\ncriterion_main!(benches);\n\n// Cargo.toml:\n// [[bench]]\n// name = "my_benchmark"\n// harness = false', desc: 'Benchmark avec Criterion' }
                            ],
                            tips: ['cargo bench g√©n√®re des rapports HTML'],
                            warnings: ['Utilisez black_box pour √©viter l\'√©limination de code']
                        }
                    },
                    {
                        cmd: 'R√©duire les allocations',
                        desc: 'Cow, SmallVec, arena allocators',
                        details: {
                            explanation: 'Minimisez les allocations heap pour am√©liorer les performances.',
                            syntax: 'cargo add smallvec bumpalo',
                            options: [
                                { flag: 'Cow', desc: 'Clone-on-write' },
                                { flag: 'SmallVec', desc: 'Vec sur la stack si petit' },
                                { flag: 'Bump', desc: 'Arena allocator' }
                            ],
                            examples: [
                                { code: 'use std::borrow::Cow;\nuse smallvec::{SmallVec, smallvec};\nuse bumpalo::Bump;\n\n// Cow: √©vite le clone si pas de modification\nfn process(input: Cow<str>) -> Cow<str> {\n    if input.contains("bad") {\n        // Clone seulement si n√©cessaire\n        Cow::Owned(input.replace("bad", "good"))\n    } else {\n        input // Pas de clone\n    }\n}\n\n// SmallVec: sur la stack si <= 8 √©l√©ments\nlet mut vec: SmallVec<[i32; 8]> = smallvec![1, 2, 3];\nvec.push(4); // Toujours sur la stack\n\n// Arena: allocations group√©es, lib√©ration en bloc\nlet bump = Bump::new();\nlet x = bump.alloc(42);\nlet s = bump.alloc_str("hello");\nlet v = bump.alloc_slice_copy(&[1, 2, 3]);\n// Tout lib√©r√© quand bump est dropped', desc: 'Techniques d\'optimisation m√©moire' }
                            ],
                            tips: ['SmallVec excellent pour petites collections temporaires'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Optimiser les it√©rations',
                        desc: 'iter vs into_iter, collect tips',
                        details: {
                            explanation: 'Choisissez le bon type d\'it√©rateur et √©vitez les allocations inutiles.',
                            syntax: 'iter() / into_iter() / iter_mut()',
                            options: [
                                { flag: 'iter()', desc: 'Emprunte les √©l√©ments' },
                                { flag: 'into_iter()', desc: 'Consomme le conteneur' },
                                { flag: 'iter_mut()', desc: 'Emprunte mutablement' }
                            ],
                            examples: [
                                { code: '// Pr√©f√©rez into_iter si vous n\'avez plus besoin du Vec\nlet sum: i32 = vec.into_iter().sum();\n\n// Cha√Ænez sans collect interm√©diaire\n// ‚ùå Mauvais:\nlet temp: Vec<_> = data.iter().filter(|x| x > &10).collect();\nlet result: Vec<_> = temp.iter().map(|x| x * 2).collect();\n\n// ‚úÖ Bon:\nlet result: Vec<_> = data.iter()\n    .filter(|x| **x > 10)\n    .map(|x| x * 2)\n    .collect();\n\n// Utilisez fold au lieu de collect + reduce\nlet sum = data.iter().fold(0, |acc, x| acc + x);\n\n// extend au lieu de multiple push\nlet mut result = Vec::with_capacity(1000);\nresult.extend(data.iter().map(|x| x * 2));\n\n// R√©utilisez les buffers\nlet mut buffer = String::with_capacity(1024);\nfor item in items {\n    buffer.clear(); // Garde la capacit√©\n    write!(&mut buffer, "{}", item).unwrap();\n    process(&buffer);\n}', desc: 'Patterns d\'it√©ration optimaux' }
                            ],
                            tips: ['with_capacity √©vite les r√©allocations'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Compiler pour la performance',
                        desc: 'LTO, codegen-units, target-cpu',
                        details: {
                            explanation: 'Configurez Cargo pour des builds optimis√©s en production.',
                            syntax: 'Cargo.toml [profile.release]',
                            options: [
                                { flag: 'lto', desc: 'Link-Time Optimization' },
                                { flag: 'codegen-units', desc: 'Parall√©lisme compilation' },
                                { flag: 'target-cpu', desc: 'Optimiser pour CPU sp√©cifique' }
                            ],
                            examples: [
                                { code: '# Cargo.toml\n[profile.release]\nopt-level = 3          # Optimisation maximale\nlto = "fat"            # LTO cross-crate complet\ncodegen-units = 1      # Meilleure optimisation (plus lent √† compiler)\npanic = "abort"        # Pas de stack unwinding\nstrip = true           # Supprime les symboles\n\n# Profile personnalis√© pour bench\n[profile.bench]\ninherits = "release"\ndebug = true           # Symboles pour profiling\n\n# Optimiser pour le CPU local\n# RUSTFLAGS="-C target-cpu=native" cargo build --release\n\n# Ou dans .cargo/config.toml\n[build]\nrustflags = ["-C", "target-cpu=native"]\n\n# V√©rifier les optimisations appliqu√©es\n# cargo rustc --release -- --print cfg', desc: 'Configuration release optimale' }
                            ],
                            tips: ['lto="fat" + codegen-units=1 = binaire le plus rapide'],
                            warnings: ['Compilation beaucoup plus lente']
                        }
                    },
                    {
                        cmd: 'Utiliser SIMD (vectorisation)',
                        desc: 'std::simd pour calculs parall√®les',
                        details: {
                            explanation: 'SIMD (Single Instruction Multiple Data) traite plusieurs donn√©es en parall√®le.',
                            syntax: '#![feature(portable_simd)]',
                            options: [
                                { flag: 'std::simd', desc: 'SIMD portable (nightly)' },
                                { flag: 'packed_simd', desc: 'Crate stable' }
                            ],
                            examples: [
                                { code: '// Avec std::simd (nightly)\n#![feature(portable_simd)]\nuse std::simd::*;\n\nfn dot_product_simd(a: &[f32], b: &[f32]) -> f32 {\n    assert_eq!(a.len(), b.len());\n    let (a_chunks, a_remainder) = a.as_simd::<8>();\n    let (b_chunks, b_remainder) = b.as_simd::<8>();\n    \n    let mut sum = f32x8::splat(0.0);\n    for (a_chunk, b_chunk) in a_chunks.iter().zip(b_chunks.iter()) {\n        sum += *a_chunk * *b_chunk;\n    }\n    \n    let mut result = sum.reduce_sum();\n    for (a, b) in a_remainder.iter().zip(b_remainder.iter()) {\n        result += a * b;\n    }\n    result\n}\n\n// Stable alternative: laisser le compilateur vectoriser\n#[inline]\nfn dot_product(a: &[f32], b: &[f32]) -> f32 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n// Compilez avec: RUSTFLAGS="-C target-cpu=native"', desc: 'SIMD pour calculs vectoriels' }
                            ],
                            tips: ['Souvent le compilateur auto-vectorise avec -C target-cpu=native'],
                            warnings: ['std::simd est unstable, utilisez packed_simd pour stable']
                        }
                    }
                ]
            },
            {
                id: 'error-handling',
                title: 'üõ°Ô∏è Erreurs & Robustesse',
                icon: 'fa-shield-alt',
                color: 'border-l-4 border-yellow-500',
                commands: [
                    {
                        cmd: 'D√©finir des erreurs m√©tier',
                        desc: 'thiserror pour enum errors',
                        details: {
                            explanation: 'thiserror g√©n√®re automatiquement les impl√©mentations Error pour vos types d\'erreur.',
                            syntax: 'cargo add thiserror',
                            options: [
                                { flag: '#[error]', desc: 'Message d\'erreur' },
                                { flag: '#[from]', desc: 'Conversion automatique' },
                                { flag: '#[source]', desc: 'Erreur source' }
                            ],
                            examples: [
                                { code: 'use thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error("User not found: {0}")]\n    UserNotFound(String),\n    \n    #[error("Invalid input: {field} - {message}")]\n    ValidationError {\n        field: String,\n        message: String,\n    },\n    \n    #[error("Database error")]\n    DatabaseError(#[from] sqlx::Error),\n    \n    #[error("IO error")]\n    IoError(#[from] std::io::Error),\n    \n    #[error("Internal error: {0}")]\n    Internal(#[source] anyhow::Error),\n}\n\n// Usage\nfn find_user(id: &str) -> Result<User, AppError> {\n    let user = db.find(id)\n        .ok_or_else(|| AppError::UserNotFound(id.to_string()))?;\n    Ok(user)\n}', desc: 'Erreurs avec thiserror' }
                            ],
                            tips: ['#[from] permet l\'utilisation de ?'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Propager avec contexte',
                        desc: 'anyhow pour context()',
                        details: {
                            explanation: 'anyhow ajoute du contexte aux erreurs pour un meilleur debugging.',
                            syntax: 'cargo add anyhow',
                            options: [
                                { flag: 'context()', desc: 'Ajouter du contexte' },
                                { flag: 'with_context()', desc: 'Contexte lazy' },
                                { flag: 'bail!', desc: 'Retourner une erreur' }
                            ],
                            examples: [
                                { code: 'use anyhow::{Context, Result, bail};\n\nfn load_config(path: &str) -> Result<Config> {\n    let content = std::fs::read_to_string(path)\n        .with_context(|| format!("Failed to read config from {}", path))?;\n    \n    let config: Config = toml::from_str(&content)\n        .context("Failed to parse config file")?;\n    \n    if config.port == 0 {\n        bail!("Port cannot be 0");\n    }\n    \n    Ok(config)\n}\n\nfn main() -> Result<()> {\n    let config = load_config("config.toml")?;\n    \n    // L\'erreur affiche toute la cha√Æne:\n    // Error: Failed to read config from config.toml\n    //\n    // Caused by:\n    //     No such file or directory (os error 2)\n    \n    Ok(())\n}', desc: 'Contexte avec anyhow' }
                            ],
                            tips: ['anyhow pour apps, thiserror pour biblioth√®ques'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Logger les erreurs structur√©es',
                        desc: 'tracing avec span context',
                        details: {
                            explanation: 'tracing permet du logging structur√© avec contexte de span.',
                            syntax: 'cargo add tracing tracing-subscriber',
                            options: [
                                { flag: '#[instrument]', desc: 'Auto-instrument une fonction' },
                                { flag: 'span', desc: 'Contexte nomm√©' },
                                { flag: 'event', desc: 'Log un √©v√©nement' }
                            ],
                            examples: [
                                { code: 'use tracing::{info, error, instrument, span, Level};\nuse tracing_subscriber;\n\n#[instrument(skip(password))]\nasync fn login(username: &str, password: &str) -> Result<User, AuthError> {\n    info!("Attempting login");\n    \n    let user = find_user(username).await?;\n    \n    if !verify_password(&user, password) {\n        error!(username, "Invalid password");\n        return Err(AuthError::InvalidCredentials);\n    }\n    \n    info!(user_id = %user.id, "Login successful");\n    Ok(user)\n}\n\nfn main() {\n    // Setup subscriber\n    tracing_subscriber::fmt()\n        .with_max_level(Level::DEBUG)\n        .json() // Format JSON pour production\n        .init();\n    \n    // Span manuel\n    let span = span!(Level::INFO, "request", id = 42);\n    let _guard = span.enter();\n    \n    // Tous les logs dans ce scope ont request.id = 42\n}', desc: 'Logging structur√©' }
                            ],
                            tips: ['#[instrument] capture automatiquement les arguments'],
                            warnings: ['skip les donn√©es sensibles comme passwords']
                        }
                    },
                    {
                        cmd: 'Impl√©menter des retries intelligents',
                        desc: 'backoff avec exponential retry',
                        details: {
                            explanation: 'R√©essayez les op√©rations avec backoff exponentiel pour g√©rer les erreurs transitoires.',
                            syntax: 'cargo add backoff --features tokio',
                            options: [
                                { flag: 'ExponentialBackoff', desc: 'Backoff exponentiel' },
                                { flag: 'retry', desc: 'Fonction retry' }
                            ],
                            examples: [
                                { code: 'use backoff::{ExponentialBackoff, retry, Error};\nuse std::time::Duration;\n\n// Sync\nfn fetch_with_retry() -> Result<Data, MyError> {\n    let backoff = ExponentialBackoff {\n        initial_interval: Duration::from_millis(100),\n        max_interval: Duration::from_secs(10),\n        max_elapsed_time: Some(Duration::from_secs(60)),\n        ..Default::default()\n    };\n    \n    retry(backoff, || {\n        match fetch_data() {\n            Ok(data) => Ok(data),\n            Err(e) if e.is_transient() => Err(Error::transient(e)),\n            Err(e) => Err(Error::permanent(e)),\n        }\n    })\n}\n\n// Async\nuse backoff::future::retry;\n\nasync fn fetch_with_retry_async() -> Result<Data, MyError> {\n    retry(ExponentialBackoff::default(), || async {\n        fetch_data_async().await.map_err(Error::transient)\n    }).await\n}', desc: 'Retry avec backoff' }
                            ],
                            tips: ['Distinguez erreurs transient vs permanent'],
                            warnings: ['Attention aux op√©rations non-idempotentes']
                        }
                    },
                    {
                        cmd: 'Valider les entr√©es utilisateur',
                        desc: 'validator crate pour r√®gles',
                        details: {
                            explanation: 'Validez les entr√©es utilisateur avec des r√®gles d√©claratives.',
                            syntax: 'cargo add validator --features derive',
                            options: [
                                { flag: '#[validate]', desc: 'Active la validation' },
                                { flag: 'custom', desc: 'Validation personnalis√©e' }
                            ],
                            examples: [
                                { code: 'use validator::{Validate, ValidationError};\n\nfn validate_username(username: &str) -> Result<(), ValidationError> {\n    if username.contains("admin") {\n        return Err(ValidationError::new("reserved_word"));\n    }\n    Ok(())\n}\n\n#[derive(Validate)]\nstruct CreateUser {\n    #[validate(length(min = 3, max = 20), custom = "validate_username")]\n    username: String,\n    \n    #[validate(email)]\n    email: String,\n    \n    #[validate(range(min = 0, max = 150))]\n    age: u8,\n    \n    #[validate(url)]\n    website: Option<String>,\n    \n    #[validate(nested)]\n    address: Address,\n}\n\n#[derive(Validate)]\nstruct Address {\n    #[validate(length(min = 1))]\n    street: String,\n    #[validate(length(equal = 5))]\n    zip: String,\n}\n\nfn process_user(input: CreateUser) -> Result<(), ValidationErrors> {\n    input.validate()?;\n    // Input is valid\n    Ok(())\n}', desc: 'Validation compl√®te' }
                            ],
                            tips: ['nested pour valider les structs imbriqu√©es'],
                            warnings: ['Validez TOUJOURS les entr√©es utilisateur']
                        }
                    }
                ]
            },
            {
                id: 'project-ml-api',
                title: 'üéØ Projet: API ML Pipeline',
                icon: 'fa-rocket',
                color: 'border-l-4 border-gradient-to-r from-orange-500 to-pink-500',
                commands: [
                    {
                        cmd: '√âtape 1: Setup projet workspace',
                        desc: 'Structure Cargo workspace',
                        details: {
                            explanation: 'Organisez votre projet avec un Cargo workspace pour s√©parer API, workers et libs.',
                            syntax: 'cargo new ml-api --lib',
                            options: [],
                            examples: [
                                { code: '# Structure du projet\nml-pipeline/\n‚îú‚îÄ‚îÄ Cargo.toml           # Workspace root\n‚îú‚îÄ‚îÄ api/                  # Service API REST\n‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml\n‚îÇ   ‚îî‚îÄ‚îÄ src/main.rs\n‚îú‚îÄ‚îÄ worker/               # Worker ML async\n‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml\n‚îÇ   ‚îî‚îÄ‚îÄ src/main.rs\n‚îú‚îÄ‚îÄ core/                 # Logique m√©tier partag√©e\n‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml\n‚îÇ   ‚îî‚îÄ‚îÄ src/lib.rs\n‚îî‚îÄ‚îÄ models/               # Fichiers de mod√®les ML\n    ‚îî‚îÄ‚îÄ model.onnx\n\n# Cargo.toml (root)\n[workspace]\nmembers = ["api", "worker", "core"]\nresolver = "2"\n\n[workspace.dependencies]\ntokio = { version = "1", features = ["full"] }\nserde = { version = "1", features = ["derive"] }\naxum = "0.7"\ntracing = "0.1"', desc: 'Structure workspace' }
                            ],
                            tips: ['workspace.dependencies pour versions unifi√©es'],
                            warnings: []
                        }
                    },
                    {
                        cmd: '√âtape 2: API REST avec Axum',
                        desc: 'Endpoints et middleware',
                        details: {
                            explanation: 'Cr√©ez l\'API REST pour soumettre des jobs ML et r√©cup√©rer les r√©sultats.',
                            syntax: 'api/src/main.rs',
                            options: [],
                            examples: [
                                { code: '// api/src/main.rs\nuse axum::{routing::{get, post}, Json, Router, extract::State};\nuse serde::{Deserialize, Serialize};\nuse std::sync::Arc;\nuse tokio::sync::mpsc;\n\n#[derive(Clone)]\nstruct AppState {\n    job_sender: mpsc::Sender<Job>,\n}\n\n#[derive(Deserialize)]\nstruct PredictRequest {\n    data: Vec<f32>,\n}\n\n#[derive(Serialize)]\nstruct PredictResponse {\n    job_id: String,\n    status: String,\n}\n\nasync fn submit_job(\n    State(state): State<Arc<AppState>>,\n    Json(req): Json<PredictRequest>,\n) -> Json<PredictResponse> {\n    let job_id = uuid::Uuid::new_v4().to_string();\n    let job = Job { id: job_id.clone(), data: req.data };\n    \n    state.job_sender.send(job).await.unwrap();\n    \n    Json(PredictResponse {\n        job_id,\n        status: "queued".into(),\n    })\n}\n\nasync fn get_result(/* ... */) -> Json<ResultResponse> {\n    // R√©cup√©rer depuis Redis/DB\n}\n\n#[tokio::main]\nasync fn main() {\n    let (tx, rx) = mpsc::channel(100);\n    let state = Arc::new(AppState { job_sender: tx });\n    \n    let app = Router::new()\n        .route("/predict", post(submit_job))\n        .route("/result/:job_id", get(get_result))\n        .with_state(state);\n    \n    let listener = tokio::net::TcpListener::bind("0.0.0.0:3000").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}', desc: 'API avec job queue' }
                            ],
                            tips: ['Utilisez Redis pour les r√©sultats en production'],
                            warnings: []
                        }
                    },
                    {
                        cmd: '√âtape 3: Int√©gration Polars',
                        desc: 'Pr√©traitement des donn√©es',
                        details: {
                            explanation: 'Utilisez Polars pour le pr√©traitement des donn√©es avant l\'inf√©rence.',
                            syntax: 'core/src/preprocessing.rs',
                            options: [],
                            examples: [
                                { code: '// core/src/preprocessing.rs\nuse polars::prelude::*;\n\npub fn preprocess_input(data: Vec<f32>) -> Result<Vec<f32>, PolarsError> {\n    // Cr√©er un DataFrame\n    let df = df! [\n        "features" => &data\n    ]?;\n    \n    // Normalisation\n    let normalized = df.lazy()\n        .with_column(\n            ((col("features") - col("features").mean()) / col("features").std(1))\n                .alias("normalized")\n        )\n        .collect()?;\n    \n    // Extraire les valeurs normalis√©es\n    let result: Vec<f32> = normalized\n        .column("normalized")?\n        .f32()?\n        .into_no_null_iter()\n        .collect();\n    \n    Ok(result)\n}\n\npub fn batch_preprocess(inputs: Vec<Vec<f32>>) -> Result<Vec<Vec<f32>>, PolarsError> {\n    inputs.into_iter()\n        .map(preprocess_input)\n        .collect()\n}', desc: 'Pr√©traitement Polars' }
                            ],
                            tips: ['Pr√©-calculez mean/std sur les donn√©es d\'entra√Ænement'],
                            warnings: []
                        }
                    },
                    {
                        cmd: '√âtape 4: Worker ML async',
                        desc: 'Inference queue avec Tract',
                        details: {
                            explanation: 'Le worker consomme les jobs de la queue et ex√©cute l\'inf√©rence.',
                            syntax: 'worker/src/main.rs',
                            options: [],
                            examples: [
                                { code: '// worker/src/main.rs\nuse tract_onnx::prelude::*;\nuse tokio::sync::mpsc;\n\nstruct MLWorker {\n    model: SimplePlan<TypedFact, Box<dyn TypedOp>, Graph<TypedFact, Box<dyn TypedOp>>>,\n}\n\nimpl MLWorker {\n    fn new(model_path: &str) -> TractResult<Self> {\n        let model = tract_onnx::onnx()\n            .model_for_path(model_path)?\n            .with_input_fact(0, f32::fact([1, 10]).into())?\n            .into_optimized()?\n            .into_runnable()?;\n        \n        Ok(Self { model })\n    }\n    \n    fn predict(&self, input: Vec<f32>) -> TractResult<Vec<f32>> {\n        let input_tensor: Tensor = tract_ndarray::Array2::from_shape_vec(\n            (1, input.len()),\n            input\n        )?.into();\n        \n        let result = self.model.run(tvec!(input_tensor.into()))?;\n        let output = result[0].to_array_view::<f32>()?;\n        \n        Ok(output.iter().cloned().collect())\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    let worker = MLWorker::new("models/model.onnx").unwrap();\n    \n    // Recevoir les jobs (depuis Redis/channel)\n    while let Some(job) = receive_job().await {\n        let preprocessed = core::preprocess_input(job.data).unwrap();\n        let prediction = worker.predict(preprocessed).unwrap();\n        \n        // Stocker le r√©sultat\n        store_result(job.id, prediction).await;\n    }\n}', desc: 'Worker ML' }
                            ],
                            tips: ['R√©utilisez le mod√®le charg√© pour toutes les inf√©rences'],
                            warnings: []
                        }
                    },
                    {
                        cmd: '√âtape 5: Observabilit√©',
                        desc: 'Tracing, metrics, health checks',
                        details: {
                            explanation: 'Ajoutez l\'observabilit√© pour monitorer votre service en production.',
                            syntax: 'Tracing + Prometheus metrics',
                            options: [],
                            examples: [
                                { code: '// api/src/main.rs\nuse axum::{routing::get, Router};\nuse metrics_exporter_prometheus::PrometheusBuilder;\nuse tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n\nasync fn health() -> &\'static str {\n    "OK"\n}\n\nasync fn ready() -> &\'static str {\n    // V√©rifier connexions DB, mod√®le charg√©, etc.\n    "OK"\n}\n\n#[tokio::main]\nasync fn main() {\n    // Setup tracing\n    tracing_subscriber::registry()\n        .with(tracing_subscriber::fmt::layer().json())\n        .init();\n    \n    // Setup Prometheus metrics\n    let prometheus_handle = PrometheusBuilder::new()\n        .install_recorder()\n        .unwrap();\n    \n    let app = Router::new()\n        .route("/health", get(health))\n        .route("/ready", get(ready))\n        .route("/metrics", get(move || {\n            std::future::ready(prometheus_handle.render())\n        }))\n        // ... autres routes\n        ;\n    \n    // M√©triques personnalis√©es\n    metrics::counter!("predictions_total").increment(1);\n    metrics::histogram!("prediction_duration_seconds").record(duration);\n}', desc: 'Observabilit√© compl√®te' }
                            ],
                            tips: ['/health pour liveness, /ready pour readiness'],
                            warnings: []
                        }
                    },
                    {
                        cmd: '√âtape 6: Containerisation Docker',
                        desc: 'Multi-stage build optimis√©',
                        details: {
                            explanation: 'Cr√©ez des images Docker optimis√©es pour le d√©ploiement.',
                            syntax: 'Dockerfile',
                            options: [],
                            examples: [
                                { code: '# Dockerfile\n# Stage 1: Build\nFROM rust:1.75-slim AS builder\n\nWORKDIR /app\n\n# Cache des d√©pendances\nCOPY Cargo.toml Cargo.lock ./\nCOPY api/Cargo.toml api/\nCOPY worker/Cargo.toml worker/\nCOPY core/Cargo.toml core/\n\nRUN mkdir -p api/src worker/src core/src && \\\n    echo "fn main() {}" > api/src/main.rs && \\\n    echo "fn main() {}" > worker/src/main.rs && \\\n    echo "" > core/src/lib.rs && \\\n    cargo build --release && \\\n    rm -rf api/src worker/src core/src\n\n# Build r√©el\nCOPY . .\nRUN cargo build --release\n\n# Stage 2: Runtime\nFROM debian:bookworm-slim\n\nRUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*\n\nCOPY --from=builder /app/target/release/api /usr/local/bin/\nCOPY --from=builder /app/target/release/worker /usr/local/bin/\nCOPY models/ /app/models/\n\nEXPOSE 3000\nCMD ["api"]', desc: 'Dockerfile multi-stage' }
                            ],
                            tips: ['Copiez Cargo.toml d\'abord pour cacher les deps'],
                            warnings: []
                        }
                    },
                    {
                        cmd: '√âtape 7: D√©ploiement production',
                        desc: 'Kubernetes et scaling',
                        details: {
                            explanation: 'D√©ployez sur Kubernetes avec scaling automatique.',
                            syntax: 'kubectl apply -f k8s/',
                            options: [],
                            examples: [
                                { code: '# k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ml-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ml-api\n  template:\n    metadata:\n      labels:\n        app: ml-api\n    spec:\n      containers:\n      - name: api\n        image: myregistry/ml-api:latest\n        ports:\n        - containerPort: 3000\n        resources:\n          requests:\n            memory: "256Mi"\n            cpu: "250m"\n          limits:\n            memory: "512Mi"\n            cpu: "500m"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3000\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ml-api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ml-api\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70', desc: 'Kubernetes deployment' }
                            ],
                            tips: ['Utilisez HPA pour auto-scaling'],
                            warnings: ['Configurez les ressources selon vos mod√®les ML']
                        }
                    }
                ]
            }
        ];
    </script>
    <script src="../js/cheatsheet.js"></script>
</body>
</html>
