<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Aide-m√©moire R Statistiques & Machine Learning : GLM, GLMnet, Random Forest, XGBoost, LCA et √©valuation de mod√®les.">
    <title>R Stats & ML - IT Cheatsheets</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body class="dark-theme text-slate-200">

    <header class="bg-slate-900/50 border-b border-white/5 py-8 px-4 relative overflow-hidden header-glow">
        <div class="max-w-4xl mx-auto relative z-10">
            <div class="flex items-center justify-between mb-4">
                <a href="../index.html" class="nav-back inline-flex items-center text-slate-400 hover:text-sky-400 transition">
                    <i class="fas fa-arrow-left mr-2"></i>Retour
                </a>
                <a href="../index.html" class="inline-flex items-center text-slate-400 hover:text-sky-400 transition">
                    <i class="fas fa-home mr-2"></i>Accueil
                </a>
            </div>
            <div class="text-center">
                <div class="inline-flex items-center justify-center w-16 h-16 rounded-xl bg-blue-500/20 mb-4 icon-glow">
                    <i class="fab fa-r-project text-3xl text-blue-400"></i>
                </div>
                <h1 class="text-3xl font-bold mb-2 gradient-text">R Stats & Machine Learning</h1>
                <p class="text-slate-400">Statistiques, GLM, GLMnet, Random Forest, XGBoost, LCA - Mod√®les qui g√©n√©ralisent</p>
            </div>
        </div>
    </header>

    <main class="max-w-4xl mx-auto p-4 relative z-10">
        <div class="mb-8 relative">
            <input type="text" id="searchInput" placeholder="Rechercher une commande..."
                   class="search-dark w-full p-4 pl-12 rounded-lg outline-none transition">
            <i class="fas fa-search absolute left-4 top-1/2 transform -translate-y-1/2 text-slate-500"></i>
        </div>
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6" id="categoriesGrid"></div>
    </main>

    <div id="detailModal" class="fixed inset-0 bg-black/70 hidden items-center justify-center z-50 p-4 modal-overlay" onclick="closeModal(event)">
        <div class="modal-content-dark rounded-xl max-w-2xl w-full max-h-[90vh] overflow-y-auto shadow-2xl modal-content" onclick="event.stopPropagation()">
            <div id="modalContent"></div>
        </div>
    </div>

    <footer class="border-t border-white/5 text-center text-slate-500 py-8 text-sm relative z-10">
        <p>¬© 2026 - Dr FENOHASINA Toto Jean Felicien</p>
    </footer>

    <script>
        const cheatsheetData = [
            {
                id: 'setup',
                title: 'üì¶ Installation & Configuration',
                icon: 'fa-download',
                color: 'border-l-4 border-blue-500',
                commands: [
                    {
                        cmd: 'Installer les packages statistiques essentiels',
                        desc: 'tidyverse, caret, et packages de base',
                        details: {
                            explanation: 'Installation des packages fondamentaux pour la statistique et le ML en R.',
                            syntax: 'install.packages(c("tidyverse", "caret", "broom"))',
                            options: [
                                { flag: 'tidyverse', desc: 'Manipulation de donn√©es (dplyr, ggplot2, tidyr)' },
                                { flag: 'caret', desc: 'Framework ML unifi√© (train, cross-validation)' },
                                { flag: 'broom', desc: 'Nettoyer les sorties de mod√®les en tibbles' },
                                { flag: 'skimr', desc: 'Statistiques descriptives rapides' }
                            ],
                            examples: [
                                { code: `# Installation des packages essentiels
install.packages(c(
  "tidyverse",    # Manipulation et visualisation
  "caret",        # Machine Learning unifi√©
  "broom",        # Tidy model outputs
  "skimr",        # Stats descriptives
  "corrplot",     # Matrices de corr√©lation
  "performance"   # √âvaluation des mod√®les
))

# Charger les packages
library(tidyverse)
library(caret)
library(broom)`, desc: 'Installation de base' }
                            ],
                            tips: ['caret unifie l\'interface de nombreux mod√®les ML', 'tidymodels est l\'alternative moderne √† caret'],
                            warnings: ['Certains packages n√©cessitent Rtools sur Windows']
                        }
                    },
                    {
                        cmd: 'Installer les packages ML avanc√©s',
                        desc: 'glmnet, randomForest, xgboost',
                        details: {
                            explanation: 'Installation des algorithmes de machine learning pour la mod√©lisation pr√©dictive.',
                            syntax: 'install.packages(c("glmnet", "randomForest", "xgboost"))',
                            options: [
                                { flag: 'glmnet', desc: 'R√©gression r√©gularis√©e (LASSO, Ridge, Elastic Net)' },
                                { flag: 'randomForest', desc: 'For√™ts al√©atoires' },
                                { flag: 'ranger', desc: 'Random Forest plus rapide' },
                                { flag: 'xgboost', desc: 'Gradient Boosting optimis√©' }
                            ],
                            examples: [
                                { code: `# Packages ML
install.packages(c(
  "glmnet",       # R√©gularisation LASSO/Ridge/Elastic Net
  "randomForest", # For√™ts al√©atoires classique
  "ranger",       # Random Forest rapide
  "xgboost",      # Gradient Boosting
  "e1071"         # SVM, Naive Bayes
))

# Pour la multinomiale
install.packages("nnet")  # R√©gression multinomiale

# Charger
library(glmnet)
library(randomForest)
library(xgboost)`, desc: 'Packages ML avanc√©s' }
                            ],
                            tips: ['ranger est 10x plus rapide que randomForest', 'xgboost supporte le GPU avec xgboost::xgb.train(..., gpu_id = 0)'],
                            warnings: ['xgboost peut n√©cessiter une compilation sp√©ciale pour GPU']
                        }
                    },
                    {
                        cmd: 'Installer les packages pour LCA',
                        desc: 'poLCA et mclust pour l\'analyse en classes latentes',
                        details: {
                            explanation: 'Latent Class Analysis (LCA) identifie des sous-groupes cach√©s dans les donn√©es cat√©gorielles.',
                            syntax: 'install.packages(c("poLCA", "mclust"))',
                            options: [
                                { flag: 'poLCA', desc: 'LCA pour variables cat√©gorielles' },
                                { flag: 'mclust', desc: 'Clustering bas√© sur mod√®les (Gaussian Mixture)' },
                                { flag: 'tidyLPA', desc: 'Interface tidy pour Latent Profile Analysis' },
                                { flag: 'lcmm', desc: 'Mod√®les √† classes latentes longitudinaux' }
                            ],
                            examples: [
                                { code: `# Packages LCA
install.packages(c(
  "poLCA",    # Latent Class Analysis (cat√©goriel)
  "mclust",   # Gaussian Mixture Models
  "tidyLPA", # Interface tidy pour LPA
  "lcmm"      # Mod√®les longitudinaux
))

library(poLCA)
library(mclust)`, desc: 'Installation LCA' }
                            ],
                            tips: ['poLCA pour variables cat√©gorielles, mclust pour continues'],
                            warnings: ['LCA n√©cessite suffisamment d\'observations par classe']
                        }
                    },
                    {
                        cmd: 'Configurer la reproductibilit√©',
                        desc: 'Seed et options pour r√©sultats reproductibles',
                        details: {
                            explanation: 'Fixer le seed garantit des r√©sultats identiques √† chaque ex√©cution.',
                            syntax: 'set.seed(123)',
                            options: [
                                { flag: 'set.seed(n)', desc: 'Fixer le g√©n√©rateur al√©atoire' },
                                { flag: 'RNGkind()', desc: 'Type de g√©n√©rateur al√©atoire' }
                            ],
                            examples: [
                                { code: `# Reproductibilit√©
set.seed(42)  # Fixer le seed

# Configuration globale
options(
  scipen = 999,           # √âviter notation scientifique
  digits = 4,             # 4 d√©cimales
  warn = 1                # Afficher warnings imm√©diatement
)

# Pour caret - reproductibilit√© parall√®le
library(doParallel)
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# Configuration trainControl reproductible
ctrl <- trainControl(
  method = "cv",
  number = 10,
  seeds = lapply(1:11, function(x) sample.int(1000, 10))
)`, desc: 'Configuration reproductible' }
                            ],
                            tips: ['Toujours documenter le seed utilis√©', 'seeds dans trainControl pour parall√©lisation reproductible'],
                            warnings: ['Le seed doit √™tre fix√© AVANT chaque op√©ration al√©atoire']
                        }
                    }
                ]
            },
            {
                id: 'descriptive',
                title: 'üìä Statistique Descriptive',
                icon: 'fa-chart-pie',
                color: 'border-l-4 border-emerald-500',
                commands: [
                    {
                        cmd: 'R√©sumer un dataset rapidement',
                        desc: 'Vue d\'ensemble avec summary, skim, glimpse',
                        details: {
                            explanation: 'Obtenir rapidement les statistiques descriptives d\'un jeu de donn√©es.',
                            syntax: 'summary(df) / skimr::skim(df) / glimpse(df)',
                            options: [
                                { flag: 'summary()', desc: 'R√©sum√© R base (min, max, mean, quartiles)' },
                                { flag: 'skim()', desc: 'R√©sum√© complet avec histogrammes' },
                                { flag: 'glimpse()', desc: 'Structure et premiers valeurs' },
                                { flag: 'describe()', desc: 'psych::describe pour stats d√©taill√©es' }
                            ],
                            examples: [
                                { code: `library(tidyverse)
library(skimr)

# Donn√©es exemple
data(mtcars)
df <- mtcars

# Vue rapide de la structure
glimpse(df)

# R√©sum√© statistique basique
summary(df)

# R√©sum√© complet avec skimr
skim(df)

# Stats d√©taill√©es avec psych
# install.packages("psych")
psych::describe(df)`, desc: 'R√©sum√©s statistiques' }
                            ],
                            tips: ['skim() affiche des mini-histogrammes en console', 'glimpse() montre les types de colonnes'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Calculer des statistiques par groupe',
                        desc: 'Moyenne, √©cart-type, m√©diane par cat√©gorie',
                        details: {
                            explanation: 'Statistiques descriptives ventil√©es par groupe avec dplyr.',
                            syntax: 'df %>% group_by(var) %>% summarize(...)',
                            options: [
                                { flag: 'mean()', desc: 'Moyenne' },
                                { flag: 'sd()', desc: '√âcart-type' },
                                { flag: 'median()', desc: 'M√©diane' },
                                { flag: 'IQR()', desc: '√âcart interquartile' },
                                { flag: 'n()', desc: 'Effectif' }
                            ],
                            examples: [
                                { code: `library(tidyverse)

df <- mtcars %>%
  mutate(cyl = factor(cyl))

# Stats par groupe
df %>%
  group_by(cyl) %>%
  summarize(
    n = n(),
    mean_mpg = mean(mpg),
    sd_mpg = sd(mpg),
    median_mpg = median(mpg),
    q1 = quantile(mpg, 0.25),
    q3 = quantile(mpg, 0.75),
    min_mpg = min(mpg),
    max_mpg = max(mpg)
  )

# Plusieurs variables √† la fois
df %>%
  group_by(cyl) %>%
  summarize(across(
    c(mpg, hp, wt),
    list(mean = mean, sd = sd),
    .names = "{.col}_{.fn}"
  ))`, desc: 'Stats par groupe' }
                            ],
                            tips: ['across() pour appliquer √† plusieurs colonnes', 'na.rm = TRUE pour ignorer les NA'],
                            warnings: []
                        }
                    },
                    {
                        cmd: 'Cr√©er une matrice de corr√©lation',
                        desc: 'Corr√©lations entre variables num√©riques',
                        details: {
                            explanation: 'Calculer et visualiser les corr√©lations entre variables.',
                            syntax: 'cor(df) / corrplot(cor(df))',
                            options: [
                                { flag: 'pearson', desc: 'Corr√©lation lin√©aire (d√©faut)' },
                                { flag: 'spearman', desc: 'Corr√©lation de rang (robuste)' },
                                { flag: 'kendall', desc: 'Tau de Kendall' }
                            ],
                            examples: [
                                { code: `library(tidyverse)
library(corrplot)

# S√©lectionner variables num√©riques
df_num <- mtcars %>% select(where(is.numeric))

# Matrice de corr√©lation
cor_matrix <- cor(df_num, use = "complete.obs")

# Visualisation basique
corrplot(cor_matrix, method = "circle")

# Visualisation am√©lior√©e
corrplot(cor_matrix,
         method = "color",
         type = "upper",
         order = "hclust",
         addCoef.col = "black",
         tl.col = "black",
         tl.srt = 45,
         diag = FALSE)

# Test de significativit√©
library(Hmisc)
cor_test <- rcorr(as.matrix(df_num))
cor_test$r  # Corr√©lations
cor_test$P  # P-values`, desc: 'Matrice de corr√©lation' }
                            ],
                            tips: ['use = "complete.obs" pour ignorer les NA', 'Spearman est robuste aux outliers'],
                            warnings: ['Corr√©lation ‚â† causalit√©']
                        }
                    },
                    {
                        cmd: 'D√©tecter les valeurs aberrantes',
                        desc: 'Identifier les outliers avec IQR ou z-score',
                        details: {
                            explanation: 'Les outliers peuvent biaiser les mod√®les. Plusieurs m√©thodes de d√©tection existent.',
                            syntax: 'IQR method: Q1 - 1.5*IQR, Q3 + 1.5*IQR',
                            options: [
                                { flag: 'IQR', desc: 'M√©thode des quartiles (robuste)' },
                                { flag: 'z-score', desc: '|z| > 3 (sensible aux extr√™mes)' },
                                { flag: 'MAD', desc: 'Median Absolute Deviation (tr√®s robuste)' }
                            ],
                            examples: [
                                { code: `library(tidyverse)

# M√©thode IQR
detect_outliers_iqr <- function(x) {
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr)
}

# Appliquer
df <- mtcars %>%
  mutate(outlier_mpg = detect_outliers_iqr(mpg))

# M√©thode Z-score
detect_outliers_z <- function(x, threshold = 3) {
  z <- abs(scale(x))
  z > threshold
}

# M√©thode MAD (robuste)
detect_outliers_mad <- function(x, threshold = 3) {
  med <- median(x, na.rm = TRUE)
  mad <- mad(x, na.rm = TRUE)
  abs(x - med) / mad > threshold
}

# Visualiser les outliers
ggplot(mtcars, aes(x = "", y = mpg)) +
  geom_boxplot() +
  geom_jitter(width = 0.2, alpha = 0.5)`, desc: 'D√©tection d\'outliers' }
                            ],
                            tips: ['MAD est plus robuste que le z-score', 'Toujours visualiser avant de supprimer'],
                            warnings: ['Ne pas supprimer automatiquement les outliers sans analyse']
                        }
                    },
                    {
                        cmd: 'Visualiser les distributions',
                        desc: 'Histogrammes, densit√©s, boxplots avec ggplot2',
                        details: {
                            explanation: 'Visualiser la forme des distributions est essentiel avant la mod√©lisation.',
                            syntax: 'ggplot(df, aes(x)) + geom_histogram()',
                            options: [
                                { flag: 'geom_histogram()', desc: 'Histogramme' },
                                { flag: 'geom_density()', desc: 'Courbe de densit√©' },
                                { flag: 'geom_boxplot()', desc: 'Bo√Æte √† moustaches' },
                                { flag: 'geom_qq()', desc: 'Q-Q plot (normalit√©)' }
                            ],
                            examples: [
                                { code: `library(tidyverse)

# Histogramme avec densit√©
ggplot(mtcars, aes(x = mpg)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 15, fill = "steelblue", alpha = 0.7) +
  geom_density(color = "red", linewidth = 1) +
  labs(title = "Distribution de MPG")

# Boxplot par groupe
ggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +
  geom_boxplot() +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(x = "Cylindres", y = "MPG")

# Q-Q plot pour v√©rifier normalit√©
ggplot(mtcars, aes(sample = mpg)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot - Normalit√© de MPG")

# Toutes les variables num√©riques
mtcars %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 20) +
  facet_wrap(~name, scales = "free")`, desc: 'Visualisation des distributions' }
                            ],
                            tips: ['Le Q-Q plot montre si la distribution est normale', 'facet_wrap pour voir toutes les variables'],
                            warnings: []
                        }
                    }
                ]
            },
            {
                id: 'inferential',
                title: 'üìà Statistique Inf√©rentielle',
                icon: 'fa-scale-balanced',
                color: 'border-l-4 border-purple-500',
                commands: [
                    {
                        cmd: 'Test t de Student',
                        desc: 'Comparer deux moyennes',
                        details: {
                            explanation: 'Le test t compare les moyennes de deux groupes. Hypoth√®se: distributions normales.',
                            syntax: 't.test(x, y) ou t.test(x ~ group, data)',
                            options: [
                                { flag: 'paired = TRUE', desc: 'Test appari√© (m√™mes sujets)' },
                                { flag: 'var.equal = TRUE', desc: 'Variances √©gales (Student)' },
                                { flag: 'alternative', desc: '"two.sided", "less", "greater"' }
                            ],
                            examples: [
                                { code: `# Test t ind√©pendant
# H0: les moyennes sont √©gales
t_result <- t.test(mpg ~ am, data = mtcars)
print(t_result)

# Extraire les r√©sultats proprement
library(broom)
tidy(t_result)

# Test appari√© (avant/apr√®s)
avant <- c(5.2, 4.8, 6.1, 5.5, 4.9)
apres <- c(5.8, 5.2, 6.5, 5.9, 5.3)
t.test(avant, apres, paired = TRUE)

# Test unilat√©ral
t.test(mpg ~ am, data = mtcars, alternative = "greater")

# V√©rifier les hypoth√®ses
# 1. Normalit√© (Shapiro-Wilk)
shapiro.test(mtcars$mpg[mtcars$am == 0])
shapiro.test(mtcars$mpg[mtcars$am == 1])

# 2. Homog√©n√©it√© des variances (Levene)
car::leveneTest(mpg ~ factor(am), data = mtcars)`, desc: 'Test t de Student' }
                            ],
                            tips: ['Welch t-test (d√©faut) ne suppose pas variances √©gales', 'Taille d\'effet: cohen.d() du package effsize'],
                            warnings: ['V√©rifier la normalit√© avec Shapiro-Wilk si n < 30']
                        }
                    },
                    {
                        cmd: 'ANOVA',
                        desc: 'Comparer plus de deux groupes',
                        details: {
                            explanation: 'L\'ANOVA teste si au moins un groupe diff√®re des autres. Hypoth√®se: normalit√© et homosc√©dasticit√©.',
                            syntax: 'aov(y ~ group, data) puis summary()',
                            options: [
                                { flag: 'aov()', desc: 'ANOVA classique' },
                                { flag: 'oneway.test()', desc: 'Welch ANOVA (variances in√©gales)' },
                                { flag: 'kruskal.test()', desc: 'Alternative non-param√©trique' }
                            ],
                            examples: [
                                { code: `# ANOVA √† un facteur
anova_result <- aov(mpg ~ factor(cyl), data = mtcars)
summary(anova_result)

# R√©sultats tidy
library(broom)
tidy(anova_result)

# Tests post-hoc (comparaisons multiples)
TukeyHSD(anova_result)

# Visualiser les comparaisons
plot(TukeyHSD(anova_result))

# Welch ANOVA (variances in√©gales)
oneway.test(mpg ~ factor(cyl), data = mtcars)

# Alternative non-param√©trique: Kruskal-Wallis
kruskal.test(mpg ~ factor(cyl), data = mtcars)

# Post-hoc non-param√©trique
pairwise.wilcox.test(mtcars$mpg, mtcars$cyl,
                     p.adjust.method = "bonferroni")

# V√©rifier homog√©n√©it√© des variances
bartlett.test(mpg ~ factor(cyl), data = mtcars)`, desc: 'ANOVA et post-hoc' }
                            ],
                            tips: ['Tukey HSD corrige pour comparaisons multiples', 'Kruskal-Wallis si non-normalit√©'],
                            warnings: ['ANOVA dit SI diff√©rence, pas O√ô - utiliser post-hoc']
                        }
                    },
                    {
                        cmd: 'Test du Chi-deux',
                        desc: 'Association entre variables cat√©gorielles',
                        details: {
                            explanation: 'Le test du Chi-deux √©value l\'ind√©pendance entre deux variables cat√©gorielles.',
                            syntax: 'chisq.test(table(x, y))',
                            options: [
                                { flag: 'chisq.test()', desc: 'Test du Chi-deux de Pearson' },
                                { flag: 'fisher.test()', desc: 'Test exact de Fisher (petits effectifs)' },
                                { flag: 'correct = FALSE', desc: 'Sans correction de Yates' }
                            ],
                            examples: [
                                { code: `# Tableau de contingence
tab <- table(mtcars$cyl, mtcars$am)
print(tab)

# Test du Chi-deux
chi_result <- chisq.test(tab)
print(chi_result)

# R√©sidus standardis√©s (contribution √† chi¬≤)
chi_result$residuals

# Test exact de Fisher (petits effectifs)
fisher.test(tab)

# Taille d'effet: V de Cram√©r
library(vcd)
assocstats(tab)

# Visualisation
library(ggplot2)
mtcars %>%
  count(cyl, am) %>%
  ggplot(aes(x = factor(cyl), y = n, fill = factor(am))) +
  geom_col(position = "dodge") +
  labs(x = "Cylindres", fill = "Transmission")

# Mosaic plot
mosaicplot(tab, color = TRUE, main = "Cyl vs AM")`, desc: 'Test du Chi-deux' }
                            ],
                            tips: ['Fisher exact si effectifs attendus < 5', 'V de Cram√©r mesure la force d\'association'],
                            warnings: ['Chi-deux invalide si effectifs attendus < 5']
                        }
                    },
                    {
                        cmd: 'Corr√©lation et test',
                        desc: 'Tester la significativit√© d\'une corr√©lation',
                        details: {
                            explanation: 'Tester si une corr√©lation est significativement diff√©rente de z√©ro.',
                            syntax: 'cor.test(x, y, method = "pearson")',
                            options: [
                                { flag: 'pearson', desc: 'Corr√©lation lin√©aire (d√©faut)' },
                                { flag: 'spearman', desc: 'Corr√©lation de rang' },
                                { flag: 'kendall', desc: 'Tau de Kendall' }
                            ],
                            examples: [
                                { code: `# Test de corr√©lation
cor_result <- cor.test(mtcars$mpg, mtcars$wt)
print(cor_result)

# R√©sultat tidy
library(broom)
tidy(cor_result)

# Corr√©lation de Spearman (robuste)
cor.test(mtcars$mpg, mtcars$wt, method = "spearman")

# Matrice de corr√©lation avec p-values
library(Hmisc)
cor_matrix <- rcorr(as.matrix(mtcars[, c("mpg", "hp", "wt", "qsec")]))
cor_matrix$r  # Corr√©lations
cor_matrix$P  # P-values

# Visualiser avec significativit√©
library(corrplot)
corrplot(cor_matrix$r,
         p.mat = cor_matrix$P,
         sig.level = 0.05,
         insig = "blank",
         method = "color",
         type = "upper")`, desc: 'Test de corr√©lation' }
                            ],
                            tips: ['Spearman pour relations monotones non-lin√©aires', 'Intervalle de confiance inclus dans cor.test()'],
                            warnings: ['Corr√©lation significative ‚â† corr√©lation importante']
                        }
                    },
                    {
                        cmd: 'Tests non-param√©triques',
                        desc: 'Alternatives quand les hypoth√®ses ne sont pas respect√©es',
                        details: {
                            explanation: 'Tests qui ne supposent pas de distribution particuli√®re des donn√©es.',
                            syntax: 'wilcox.test(x, y) / kruskal.test()',
                            options: [
                                { flag: 'wilcox.test()', desc: 'Mann-Whitney U (2 groupes)' },
                                { flag: 'kruskal.test()', desc: 'Kruskal-Wallis (>2 groupes)' },
                                { flag: 'friedman.test()', desc: 'Mesures r√©p√©t√©es' }
                            ],
                            examples: [
                                { code: `# Mann-Whitney U (√©quivalent t-test)
wilcox.test(mpg ~ am, data = mtcars)

# Test appari√©: Wilcoxon signed-rank
avant <- c(5.2, 4.8, 6.1, 5.5, 4.9)
apres <- c(5.8, 5.2, 6.5, 5.9, 5.3)
wilcox.test(avant, apres, paired = TRUE)

# Kruskal-Wallis (√©quivalent ANOVA)
kruskal.test(mpg ~ factor(cyl), data = mtcars)

# Post-hoc: Dunn test
library(dunn.test)
dunn.test(mtcars$mpg, mtcars$cyl, method = "bonferroni")

# Friedman test (mesures r√©p√©t√©es)
# Donn√©es en format matrice
data_matrix <- matrix(c(
  5, 4, 7,  # Sujet 1
  6, 5, 8,  # Sujet 2
  4, 3, 6   # Sujet 3
), nrow = 3, byrow = TRUE)
friedman.test(data_matrix)`, desc: 'Tests non-param√©triques' }
                            ],
                            tips: ['Utilisez non-param√©trique si n < 30 et non-normalit√©', 'Plus robustes aux outliers'],
                            warnings: ['Moins de puissance statistique que les tests param√©triques']
                        }
                    }
                ]
            },
            {
                id: 'glm',
                title: 'üìâ GLM - R√©gression',
                icon: 'fa-chart-line',
                color: 'border-l-4 border-cyan-500',
                commands: [
                    {
                        cmd: 'R√©gression lin√©aire simple',
                        desc: 'Pr√©dire une variable continue',
                        details: {
                            explanation: 'La r√©gression lin√©aire mod√©lise la relation entre une variable d√©pendante continue et des pr√©dicteurs.',
                            syntax: 'lm(y ~ x1 + x2, data)',
                            options: [
                                { flag: '+ x', desc: 'Ajouter un pr√©dicteur' },
                                { flag: '* x', desc: 'Interaction (+ effets principaux)' },
                                { flag: ': x', desc: 'Interaction seulement' },
                                { flag: 'I(x^2)', desc: 'Terme polynomial' }
                            ],
                            examples: [
                                { code: `# R√©gression lin√©aire simple
model <- lm(mpg ~ wt, data = mtcars)
summary(model)

# R√©sultats tidy
library(broom)
tidy(model)      # Coefficients
glance(model)    # R¬≤, AIC, etc.
augment(model)   # Pr√©dictions, r√©sidus

# R√©gression multiple
model_mult <- lm(mpg ~ wt + hp + qsec, data = mtcars)
summary(model_mult)

# Avec interaction
model_inter <- lm(mpg ~ wt * am, data = mtcars)
summary(model_inter)

# Intervalles de confiance
confint(model, level = 0.95)

# Pr√©diction
new_data <- data.frame(wt = c(2.5, 3.0, 3.5))
predict(model, new_data, interval = "confidence")
predict(model, new_data, interval = "prediction")`, desc: 'R√©gression lin√©aire' }
                            ],
                            tips: ['R¬≤ ajust√© p√©nalise les variables inutiles', 'AIC/BIC pour comparer des mod√®les'],
                            warnings: ['V√©rifier les hypoth√®ses avec les diagnostics']
                        }
                    },
                    {
                        cmd: 'Diagnostics de r√©gression',
                        desc: 'V√©rifier les hypoth√®ses du mod√®le',
                        details: {
                            explanation: 'V√©rifier lin√©arit√©, normalit√© des r√©sidus, homosc√©dasticit√© et ind√©pendance.',
                            syntax: 'plot(model) / performance::check_model()',
                            options: [
                                { flag: 'Residuals vs Fitted', desc: 'Lin√©arit√© et homosc√©dasticit√©' },
                                { flag: 'Q-Q', desc: 'Normalit√© des r√©sidus' },
                                { flag: 'Scale-Location', desc: 'Homosc√©dasticit√©' },
                                { flag: 'Cook\'s distance', desc: 'Points influents' }
                            ],
                            examples: [
                                { code: `model <- lm(mpg ~ wt + hp, data = mtcars)

# Diagnostics R base (4 plots)
par(mfrow = c(2, 2))
plot(model)
par(mfrow = c(1, 1))

# Package performance (plus complet)
library(performance)
check_model(model)

# Tests formels
# 1. Normalit√© des r√©sidus
shapiro.test(residuals(model))

# 2. Homosc√©dasticit√© (Breusch-Pagan)
library(lmtest)
bptest(model)

# 3. Multicolin√©arit√© (VIF)
library(car)
vif(model)  # VIF > 5-10 = probl√®me

# 4. Autocorr√©lation (Durbin-Watson)
library(lmtest)
dwtest(model)

# Points influents
plot(cooks.distance(model), type = "h")
abline(h = 4/nrow(mtcars), col = "red")`, desc: 'Diagnostics du mod√®le' }
                            ],
                            tips: ['VIF > 10 indique une forte multicolin√©arit√©', 'Cook\'s distance > 4/n = point influent'],
                            warnings: ['Ne pas ignorer les violations d\'hypoth√®ses']
                        }
                    },
                    {
                        cmd: 'R√©gression logistique binaire',
                        desc: 'Pr√©dire une variable binaire (0/1)',
                        details: {
                            explanation: 'La r√©gression logistique mod√©lise la probabilit√© d\'un √©v√©nement binaire.',
                            syntax: 'glm(y ~ x, family = binomial, data)',
                            options: [
                                { flag: 'family = binomial', desc: 'Logistique (d√©faut link = logit)' },
                                { flag: 'link = "probit"', desc: 'Alternative au logit' }
                            ],
                            examples: [
                                { code: `# R√©gression logistique
model <- glm(am ~ wt + hp,
             family = binomial,
             data = mtcars)
summary(model)

# Odds ratios (exponentielle des coefficients)
exp(coef(model))
exp(confint(model))

# R√©sultats tidy
library(broom)
tidy(model, exponentiate = TRUE)

# Pr√©diction de probabilit√©s
mtcars$pred_prob <- predict(model, type = "response")

# Classification avec seuil 0.5
mtcars$pred_class <- ifelse(mtcars$pred_prob > 0.5, 1, 0)

# Matrice de confusion
table(Predicted = mtcars$pred_class,
      Actual = mtcars$am)

# Courbe ROC et AUC
library(pROC)
roc_obj <- roc(mtcars$am, mtcars$pred_prob)
plot(roc_obj, main = paste("AUC =", round(auc(roc_obj), 3)))

# Pseudo R¬≤
library(pscl)
pR2(model)`, desc: 'R√©gression logistique' }
                            ],
                            tips: ['Odds ratio > 1 = effet positif', 'AUC > 0.7 = discrimination acceptable'],
                            warnings: ['Interpr√©ter les coefficients comme log-odds, pas probabilit√©s']
                        }
                    },
                    {
                        cmd: 'R√©gression multinomiale',
                        desc: 'Pr√©dire une variable cat√©gorielle √† plus de 2 classes',
                        details: {
                            explanation: 'Extension de la logistique pour des outcomes avec plus de 2 cat√©gories non ordonn√©es.',
                            syntax: 'nnet::multinom(y ~ x, data)',
                            options: [
                                { flag: 'nnet::multinom()', desc: 'R√©gression multinomiale' },
                                { flag: 'MASS::polr()', desc: 'R√©gression ordinale' }
                            ],
                            examples: [
                                { code: `library(nnet)

# Transformer en facteur
mtcars$cyl_f <- factor(mtcars$cyl)

# R√©gression multinomiale
model <- multinom(cyl_f ~ wt + hp, data = mtcars)
summary(model)

# Coefficients et odds ratios
coef(model)
exp(coef(model))  # Relative Risk Ratios

# Z-scores et p-values
z <- summary(model)$coefficients / summary(model)$standard.errors
p <- (1 - pnorm(abs(z))) * 2
print(p)

# Pr√©diction
pred_class <- predict(model, type = "class")
pred_prob <- predict(model, type = "probs")

# Matrice de confusion
table(Predicted = pred_class, Actual = mtcars$cyl_f)

# R√©gression ordinale (si cat√©gories ordonn√©es)
library(MASS)
model_ord <- polr(factor(gear) ~ wt + hp, data = mtcars)
summary(model_ord)`, desc: 'R√©gression multinomiale' }
                            ],
                            tips: ['La premi√®re cat√©gorie est la r√©f√©rence', 'polr() pour variables ordinales'],
                            warnings: ['N√©cessite suffisamment d\'observations par cat√©gorie']
                        }
                    },
                    {
                        cmd: 'GLM avec autres distributions',
                        desc: 'Poisson, Gamma, binomiale n√©gative',
                        details: {
                            explanation: 'GLM permet de mod√©liser diff√©rents types de donn√©es avec des distributions appropri√©es.',
                            syntax: 'glm(y ~ x, family = poisson/Gamma/...)',
                            options: [
                                { flag: 'poisson', desc: 'Donn√©es de comptage' },
                                { flag: 'Gamma', desc: 'Donn√©es positives continues' },
                                { flag: 'MASS::glm.nb()', desc: 'Binomiale n√©gative (surdispersion)' }
                            ],
                            examples: [
                                { code: `# R√©gression de Poisson (comptage)
# Simuler des donn√©es de comptage
set.seed(42)
df <- data.frame(
  x = rnorm(100),
  count = rpois(100, lambda = exp(1 + 0.5 * rnorm(100)))
)

model_pois <- glm(count ~ x, family = poisson, data = df)
summary(model_pois)

# V√©rifier surdispersion
# Deviance/df >> 1 = surdispersion
model_pois$deviance / model_pois$df.residual

# Binomiale n√©gative si surdispersion
library(MASS)
model_nb <- glm.nb(count ~ x, data = df)
summary(model_nb)

# R√©gression Gamma (temps, co√ªts)
df$positive <- abs(rnorm(100, 10, 2))
model_gamma <- glm(positive ~ x,
                   family = Gamma(link = "log"),
                   data = df)
summary(model_gamma)

# Comparer les mod√®les avec AIC
AIC(model_pois, model_nb)`, desc: 'GLM avec diff√©rentes familles' }
                            ],
                            tips: ['Poisson suppose moyenne = variance', 'Binomiale n√©gative pour surdispersion'],
                            warnings: ['V√©rifier l\'ad√©quation de la distribution']
                        }
                    }
                ]
            },
            {
                id: 'glmnet',
                title: 'üîÄ GLMnet - R√©gularisation',
                icon: 'fa-compress-arrows-alt',
                color: 'border-l-4 border-orange-500',
                commands: [
                    {
                        cmd: 'LASSO avec validation crois√©e',
                        desc: 'S√©lection de variables et r√©gularisation L1',
                        details: {
                            explanation: 'LASSO (L1) p√©nalise la somme des valeurs absolues des coefficients, for√ßant certains √† z√©ro.',
                            syntax: 'cv.glmnet(x, y, alpha = 1)',
                            options: [
                                { flag: 'alpha = 1', desc: 'LASSO (L1)' },
                                { flag: 'alpha = 0', desc: 'Ridge (L2)' },
                                { flag: 'alpha = 0.5', desc: 'Elastic Net' },
                                { flag: 'lambda.min', desc: 'Lambda minimisant l\'erreur' },
                                { flag: 'lambda.1se', desc: 'Lambda √† 1 SE (plus parcimonieux)' }
                            ],
                            examples: [
                                { code: `library(glmnet)
set.seed(42)

# Pr√©parer les donn√©es (glmnet veut des matrices)
x <- as.matrix(mtcars[, c("wt", "hp", "drat", "qsec")])
y <- mtcars$mpg

# LASSO avec validation crois√©e (10-fold)
cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10)

# Visualiser le chemin de r√©gularisation
plot(cv_lasso)

# Meilleurs lambdas
cv_lasso$lambda.min   # Minimum CV error
cv_lasso$lambda.1se   # 1 SE rule (plus simple)

# Coefficients au lambda optimal
coef(cv_lasso, s = "lambda.min")
coef(cv_lasso, s = "lambda.1se")  # Plus parcimonieux

# Pr√©diction
pred <- predict(cv_lasso, newx = x, s = "lambda.1se")

# Performance
sqrt(mean((y - pred)^2))  # RMSE`, desc: 'LASSO r√©gression' }
                            ],
                            tips: ['lambda.1se donne un mod√®le plus simple et g√©n√©ralisable', 'LASSO fait de la s√©lection de variables automatique'],
                            warnings: ['Standardiser les pr√©dicteurs (fait par d√©faut)']
                        }
                    },
                    {
                        cmd: 'Ridge regression',
                        desc: 'R√©gularisation L2 sans s√©lection',
                        details: {
                            explanation: 'Ridge (L2) p√©nalise la somme des carr√©s des coefficients, les r√©duisant mais sans les annuler.',
                            syntax: 'cv.glmnet(x, y, alpha = 0)',
                            options: [
                                { flag: 'alpha = 0', desc: 'Ridge (L2)' }
                            ],
                            examples: [
                                { code: `library(glmnet)
set.seed(42)

x <- as.matrix(mtcars[, c("wt", "hp", "drat", "qsec")])
y <- mtcars$mpg

# Ridge avec validation crois√©e
cv_ridge <- cv.glmnet(x, y, alpha = 0, nfolds = 10)

plot(cv_ridge)

# Coefficients (jamais exactement 0)
coef(cv_ridge, s = "lambda.min")

# Comparer LASSO vs Ridge
cv_lasso <- cv.glmnet(x, y, alpha = 1)

# MSE au lambda optimal
min(cv_lasso$cvm)  # LASSO
min(cv_ridge$cvm)  # Ridge

# Visualiser les chemins de r√©gularisation
par(mfrow = c(1, 2))
plot(glmnet(x, y, alpha = 1), main = "LASSO")
plot(glmnet(x, y, alpha = 0), main = "Ridge")
par(mfrow = c(1, 1))`, desc: 'Ridge regression' }
                            ],
                            tips: ['Ridge garde toutes les variables', 'Meilleur que LASSO si beaucoup de pr√©dicteurs corr√©l√©s'],
                            warnings: ['Ridge ne fait pas de s√©lection de variables']
                        }
                    },
                    {
                        cmd: 'Elastic Net',
                        desc: 'Combinaison LASSO + Ridge',
                        details: {
                            explanation: 'Elastic Net combine les p√©nalit√©s L1 et L2 pour b√©n√©ficier des avantages des deux.',
                            syntax: 'cv.glmnet(x, y, alpha = 0.5)',
                            options: [
                                { flag: 'alpha', desc: 'Ratio L1/(L1+L2), entre 0 et 1' }
                            ],
                            examples: [
                                { code: `library(glmnet)
set.seed(42)

x <- as.matrix(mtcars[, c("wt", "hp", "drat", "qsec")])
y <- mtcars$mpg

# Tester diff√©rents alpha avec caret
library(caret)

# Grid search sur alpha et lambda
tune_grid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),
  lambda = 10^seq(-3, 1, length = 50)
)

ctrl <- trainControl(
  method = "cv",
  number = 10,
  verboseIter = FALSE
)

elastic_model <- train(
  mpg ~ wt + hp + drat + qsec,
  data = mtcars,
  method = "glmnet",
  tuneGrid = tune_grid,
  trControl = ctrl
)

# Meilleurs hyperparam√®tres
elastic_model$bestTune

# Visualiser
plot(elastic_model)

# Mod√®le final
coef(elastic_model$finalModel, elastic_model$bestTune$lambda)`, desc: 'Elastic Net avec tuning' }
                            ],
                            tips: ['alpha = 0.5 est un bon point de d√©part', 'Elastic Net g√®re mieux les variables corr√©l√©es que LASSO'],
                            warnings: ['Tuner √† la fois alpha et lambda']
                        }
                    },
                    {
                        cmd: 'GLMnet multinomial',
                        desc: 'Classification multi-classes r√©gularis√©e',
                        details: {
                            explanation: 'glmnet supporte la classification multinomiale avec r√©gularisation LASSO/Ridge.',
                            syntax: 'cv.glmnet(x, y, family = "multinomial")',
                            options: [
                                { flag: 'family = "multinomial"', desc: 'Classification multi-classes' },
                                { flag: 'type.multinomial = "grouped"', desc: 'M√™me lambda pour toutes les classes' }
                            ],
                            examples: [
                                { code: `library(glmnet)
set.seed(42)

# Pr√©parer les donn√©es
x <- as.matrix(mtcars[, c("wt", "hp", "drat", "qsec")])
y <- factor(mtcars$cyl)  # 3 classes: 4, 6, 8

# R√©gression multinomiale r√©gularis√©e
cv_multi <- cv.glmnet(x, y,
                      family = "multinomial",
                      alpha = 0.5,  # Elastic Net
                      type.multinomial = "grouped",
                      nfolds = 10)

plot(cv_multi)

# Coefficients par classe
coef(cv_multi, s = "lambda.1se")

# Pr√©diction des classes
pred_class <- predict(cv_multi, newx = x,
                      s = "lambda.1se",
                      type = "class")

# Pr√©diction des probabilit√©s
pred_prob <- predict(cv_multi, newx = x,
                     s = "lambda.1se",
                     type = "response")

# Matrice de confusion
table(Predicted = pred_class, Actual = y)

# Accuracy
mean(pred_class == y)

# Avec caret pour plus de contr√¥le
library(caret)
ctrl <- trainControl(method = "cv", number = 10)

model <- train(
  x = mtcars[, c("wt", "hp", "drat", "qsec")],
  y = factor(mtcars$cyl),
  method = "glmnet",
  family = "multinomial",
  trControl = ctrl,
  tuneLength = 10
)

confusionMatrix(predict(model), factor(mtcars$cyl))`, desc: 'GLMnet multinomial' }
                            ],
                            tips: ['type.multinomial = "grouped" pour s√©lection de variables consistante', 'Utiliser caret pour un workflow plus simple'],
                            warnings: ['N√©cessite suffisamment d\'observations par classe']
                        }
                    },
                    {
                        cmd: 'GLMnet logistique avec poids',
                        desc: 'G√©rer les classes d√©s√©quilibr√©es',
                        details: {
                            explanation: 'Pond√©rer les observations pour compenser le d√©s√©quilibre des classes.',
                            syntax: 'glmnet(..., weights = w)',
                            options: [
                                { flag: 'weights', desc: 'Vecteur de poids par observation' }
                            ],
                            examples: [
                                { code: `library(glmnet)
set.seed(42)

# Donn√©es d√©s√©quilibr√©es
x <- as.matrix(mtcars[, c("wt", "hp", "drat", "qsec")])
y <- factor(mtcars$am)  # 0: 19, 1: 13

# Calculer les poids inverses
class_weights <- ifelse(y == "1",
                        sum(y == "0") / sum(y == "1"),
                        1)
class_weights <- class_weights / sum(class_weights) * length(y)

# Mod√®le avec poids
cv_weighted <- cv.glmnet(x, y,
                         family = "binomial",
                         alpha = 1,
                         weights = class_weights,
                         nfolds = 10)

# Pr√©diction
pred <- predict(cv_weighted, newx = x,
                s = "lambda.1se", type = "class")

# Matrice de confusion
table(Predicted = pred, Actual = y)

# Avec caret (plus simple)
library(caret)

ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "up"  # ou "down", "smote"
)

# Renommer les niveaux pour caret
mtcars$am_f <- factor(mtcars$am, labels = c("auto", "manual"))

model <- train(
  am_f ~ wt + hp + drat + qsec,
  data = mtcars,
  method = "glmnet",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)

confusionMatrix(predict(model), mtcars$am_f)`, desc: 'Classes d√©s√©quilibr√©es' }
                            ],
                            tips: ['sampling = "smote" cr√©e des observations synth√©tiques', 'Utiliser ROC comme m√©trique plut√¥t qu\'accuracy'],
                            warnings: ['Le sur√©chantillonnage peut mener √† l\'overfitting']
                        }
                    }
                ]
            },
            {
                id: 'randomforest',
                title: 'üå≤ Random Forest',
                icon: 'fa-tree',
                color: 'border-l-4 border-green-500',
                commands: [
                    {
                        cmd: 'Random Forest pour classification',
                        desc: 'Classifier avec un ensemble d\'arbres',
                        details: {
                            explanation: 'Random Forest construit de nombreux arbres sur des sous-√©chantillons et vote pour la classe finale.',
                            syntax: 'randomForest(y ~ ., data, ntree, mtry)',
                            options: [
                                { flag: 'ntree', desc: 'Nombre d\'arbres (d√©faut: 500)' },
                                { flag: 'mtry', desc: 'Variables par split (sqrt(p) pour classif)' },
                                { flag: 'importance = TRUE', desc: 'Calculer l\'importance des variables' }
                            ],
                            examples: [
                                { code: `library(randomForest)
set.seed(42)

# Classification
mtcars$cyl_f <- factor(mtcars$cyl)

rf_model <- randomForest(
  cyl_f ~ mpg + hp + wt + drat + qsec,
  data = mtcars,
  ntree = 500,
  mtry = 2,  # sqrt(5) ‚âà 2
  importance = TRUE
)

print(rf_model)

# Erreur OOB par nombre d'arbres
plot(rf_model, main = "Erreur OOB")

# Importance des variables
importance(rf_model)
varImpPlot(rf_model)

# Pr√©diction
pred <- predict(rf_model, type = "class")
table(Predicted = pred, Actual = mtcars$cyl_f)

# Probabilit√©s
pred_prob <- predict(rf_model, type = "prob")

# Avec ranger (plus rapide)
library(ranger)
rf_ranger <- ranger(
  cyl_f ~ mpg + hp + wt + drat + qsec,
  data = mtcars,
  num.trees = 500,
  importance = "permutation"
)

rf_ranger$confusion.matrix`, desc: 'Random Forest classification' }
                            ],
                            tips: ['OOB error est une estimation honn√™te de l\'erreur', 'ranger est 10x plus rapide'],
                            warnings: ['mtry par d√©faut: sqrt(p) pour classif, p/3 pour r√©gression']
                        }
                    },
                    {
                        cmd: 'Random Forest pour r√©gression',
                        desc: 'Pr√©dire une variable continue',
                        details: {
                            explanation: 'Pour la r√©gression, RF moyenne les pr√©dictions des arbres.',
                            syntax: 'randomForest(y ~ ., data)',
                            options: [
                                { flag: 'mtry', desc: 'p/3 par d√©faut pour r√©gression' }
                            ],
                            examples: [
                                { code: `library(randomForest)
set.seed(42)

# R√©gression
rf_reg <- randomForest(
  mpg ~ hp + wt + drat + qsec,
  data = mtcars,
  ntree = 500,
  mtry = 2,
  importance = TRUE
)

print(rf_reg)

# % Variance expliqu√©e
rf_reg$rsq[500]

# Importance
varImpPlot(rf_reg)

# Pr√©diction
pred <- predict(rf_reg)
sqrt(mean((mtcars$mpg - pred)^2))  # RMSE

# Partial dependence plots
partialPlot(rf_reg, mtcars, x.var = "wt")

# Avec caret et CV
library(caret)
ctrl <- trainControl(method = "cv", number = 10)

rf_caret <- train(
  mpg ~ hp + wt + drat + qsec,
  data = mtcars,
  method = "rf",
  trControl = ctrl,
  tuneGrid = expand.grid(mtry = 1:4),
  importance = TRUE
)

print(rf_caret)
plot(rf_caret)`, desc: 'Random Forest r√©gression' }
                            ],
                            tips: ['Partial plots montrent l\'effet marginal d\'une variable', 'CV avec caret pour s√©lectionner mtry optimal'],
                            warnings: ['RF ne donne pas de coefficients interpr√©tables']
                        }
                    },
                    {
                        cmd: 'Tuning des hyperparam√®tres RF',
                        desc: 'Optimiser ntree, mtry avec caret',
                        details: {
                            explanation: 'Utiliser la validation crois√©e pour trouver les meilleurs hyperparam√®tres.',
                            syntax: 'train(method = "rf", tuneGrid = ...)',
                            options: [
                                { flag: 'ntree', desc: 'Plus d\'arbres = plus stable mais plus lent' },
                                { flag: 'mtry', desc: 'Param√®tre le plus important √† tuner' },
                                { flag: 'nodesize', desc: 'Taille minimale des feuilles' }
                            ],
                            examples: [
                                { code: `library(caret)
library(randomForest)
set.seed(42)

# Validation crois√©e
ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  verboseIter = TRUE
)

# Grid search sur mtry
tune_grid <- expand.grid(mtry = 1:5)

rf_tuned <- train(
  factor(cyl) ~ mpg + hp + wt + drat + qsec,
  data = mtcars,
  method = "rf",
  trControl = ctrl,
  tuneGrid = tune_grid,
  ntree = 500,
  importance = TRUE
)

# Meilleur mtry
print(rf_tuned)
plot(rf_tuned)

# Avec ranger pour plus d'options
library(ranger)

ctrl_ranger <- trainControl(method = "cv", number = 10)

tune_grid_ranger <- expand.grid(
  mtry = 1:5,
  splitrule = c("gini", "extratrees"),
  min.node.size = c(1, 5, 10)
)

rf_ranger_tuned <- train(
  factor(cyl) ~ mpg + hp + wt + drat + qsec,
  data = mtcars,
  method = "ranger",
  trControl = ctrl_ranger,
  tuneGrid = tune_grid_ranger,
  importance = "permutation"
)

print(rf_ranger_tuned)`, desc: 'Tuning Random Forest' }
                            ],
                            tips: ['500-1000 arbres suffisent g√©n√©ralement', 'mtry optimal souvent proche de sqrt(p)'],
                            warnings: ['Trop d\'arbres = plus lent sans gain']
                        }
                    },
                    {
                        cmd: 'Importance des variables',
                        desc: 'Identifier les pr√©dicteurs les plus importants',
                        details: {
                            explanation: 'Deux mesures: Mean Decrease Accuracy (permutation) et Mean Decrease Gini (impuret√©).',
                            syntax: 'importance(rf_model) / varImpPlot(rf_model)',
                            options: [
                                { flag: 'MeanDecreaseAccuracy', desc: 'Perte d\'accuracy si variable permut√©e' },
                                { flag: 'MeanDecreaseGini', desc: 'Diminution de l\'impuret√© Gini' }
                            ],
                            examples: [
                                { code: `library(randomForest)
set.seed(42)

rf_model <- randomForest(
  factor(cyl) ~ mpg + hp + wt + drat + qsec,
  data = mtcars,
  importance = TRUE
)

# Importance brute
importance(rf_model)

# Visualisation
varImpPlot(rf_model, main = "Importance des variables")

# Importance par classe
importance(rf_model, type = 1)  # Accuracy
importance(rf_model, type = 2)  # Gini

# Extraire et trier
imp <- importance(rf_model)
imp_df <- data.frame(
  variable = rownames(imp),
  importance = imp[, "MeanDecreaseAccuracy"]
) %>%
  arrange(desc(importance))

# ggplot
library(ggplot2)
ggplot(imp_df, aes(x = reorder(variable, importance), y = importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(x = "Variable", y = "Mean Decrease Accuracy")

# SHAP values (plus pr√©cis)
# install.packages("iml")
library(iml)
predictor <- Predictor$new(rf_model, data = mtcars, type = "prob")
shapley <- Shapley$new(predictor, x.interest = mtcars[1, ])
plot(shapley)`, desc: 'Importance des variables' }
                            ],
                            tips: ['MeanDecreaseAccuracy est plus fiable', 'SHAP values pour interpr√©tation locale'],
                            warnings: ['Gini est biais√© vers les variables √† beaucoup de cat√©gories']
                        }
                    }
                ]
            },
            {
                id: 'xgboost',
                title: '‚ö° XGBoost',
                icon: 'fa-bolt',
                color: 'border-l-4 border-yellow-500',
                commands: [
                    {
                        cmd: 'XGBoost pour classification',
                        desc: 'Gradient Boosting optimis√©',
                        details: {
                            explanation: 'XGBoost construit des arbres s√©quentiellement, chacun corrigeant les erreurs du pr√©c√©dent.',
                            syntax: 'xgboost(data = dtrain, nrounds, objective)',
                            options: [
                                { flag: 'nrounds', desc: 'Nombre d\'it√©rations (arbres)' },
                                { flag: 'max_depth', desc: 'Profondeur max des arbres (3-10)' },
                                { flag: 'eta', desc: 'Learning rate (0.01-0.3)' },
                                { flag: 'objective', desc: '"binary:logistic", "multi:softmax"' }
                            ],
                            examples: [
                                { code: `library(xgboost)
set.seed(42)

# Pr√©parer les donn√©es
x <- as.matrix(mtcars[, c("mpg", "hp", "wt", "drat", "qsec")])
y <- as.numeric(factor(mtcars$cyl)) - 1  # 0, 1, 2

# Cr√©er DMatrix
dtrain <- xgb.DMatrix(data = x, label = y)

# Param√®tres
params <- list(
  objective = "multi:softmax",
  num_class = 3,
  max_depth = 3,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Entra√Ænement avec CV
cv_result <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 5,
  early_stopping_rounds = 10,
  verbose = TRUE
)

# Meilleur nombre d'it√©rations
best_nrounds <- cv_result$best_iteration

# Mod√®le final
xgb_model <- xgboost(
  data = dtrain,
  params = params,
  nrounds = best_nrounds,
  verbose = 0
)

# Pr√©diction
pred <- predict(xgb_model, x)
table(Predicted = pred, Actual = y)

# Importance des variables
importance <- xgb.importance(model = xgb_model)
xgb.plot.importance(importance)`, desc: 'XGBoost classification' }
                            ],
                            tips: ['early_stopping √©vite l\'overfitting', 'eta faible + plus de rounds = meilleur'],
                            warnings: ['XGBoost est sensible aux hyperparam√®tres']
                        }
                    },
                    {
                        cmd: 'XGBoost pour r√©gression',
                        desc: 'Pr√©dire une variable continue',
                        details: {
                            explanation: 'XGBoost peut aussi faire de la r√©gression avec objective = "reg:squarederror".',
                            syntax: 'xgboost(objective = "reg:squarederror")',
                            options: [
                                { flag: 'reg:squarederror', desc: 'Erreur quadratique (MSE)' },
                                { flag: 'reg:absoluteerror', desc: 'Erreur absolue (MAE)' }
                            ],
                            examples: [
                                { code: `library(xgboost)
set.seed(42)

x <- as.matrix(mtcars[, c("hp", "wt", "drat", "qsec")])
y <- mtcars$mpg

dtrain <- xgb.DMatrix(data = x, label = y)

params <- list(
  objective = "reg:squarederror",
  max_depth = 4,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# CV pour trouver nrounds optimal
cv_result <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 200,
  nfold = 10,
  metrics = "rmse",
  early_stopping_rounds = 20,
  verbose = FALSE
)

# Entra√Æner le mod√®le final
xgb_reg <- xgboost(
  data = dtrain,
  params = params,
  nrounds = cv_result$best_iteration,
  verbose = 0
)

# Pr√©diction et RMSE
pred <- predict(xgb_reg, x)
sqrt(mean((y - pred)^2))

# Partial dependence
# install.packages("pdp")
library(pdp)
partial(xgb_reg, pred.var = "wt",
        train = x,
        plot = TRUE)`, desc: 'XGBoost r√©gression' }
                            ],
                            tips: ['RMSE dans cv_result pour suivre la performance', 'pdp pour interpr√©ter les effets'],
                            warnings: ['Attention √† l\'overfitting avec max_depth √©lev√©']
                        }
                    },
                    {
                        cmd: 'Tuning XGBoost avec caret',
                        desc: 'Grid search des hyperparam√®tres',
                        details: {
                            explanation: 'caret simplifie le tuning de XGBoost avec validation crois√©e.',
                            syntax: 'train(method = "xgbTree", tuneGrid = ...)',
                            options: [
                                { flag: 'nrounds', desc: 'Nombre de boosting rounds' },
                                { flag: 'max_depth', desc: 'Profondeur (3-10)' },
                                { flag: 'eta', desc: 'Learning rate (0.01-0.3)' },
                                { flag: 'gamma', desc: 'R√©gularisation (0+)' },
                                { flag: 'subsample', desc: 'Ratio √©chantillons (0.5-1)' },
                                { flag: 'colsample_bytree', desc: 'Ratio features (0.5-1)' }
                            ],
                            examples: [
                                { code: `library(caret)
library(xgboost)
set.seed(42)

# Contr√¥le de la CV
ctrl <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  allowParallel = TRUE
)

# Grille d'hyperparam√®tres
tune_grid <- expand.grid(
  nrounds = c(50, 100, 150),
  max_depth = c(3, 4, 5),
  eta = c(0.05, 0.1, 0.2),
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

# Entra√Ænement
xgb_tuned <- train(
  mpg ~ hp + wt + drat + qsec,
  data = mtcars,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = tune_grid,
  verbose = FALSE
)

# Meilleurs param√®tres
print(xgb_tuned$bestTune)
plot(xgb_tuned)

# Importance des variables
varImp(xgb_tuned)
plot(varImp(xgb_tuned))`, desc: 'Tuning XGBoost' }
                            ],
                            tips: ['Commencer avec peu de combinaisons puis affiner', 'Parall√©liser avec doParallel'],
                            warnings: ['Grid search peut √™tre tr√®s long']
                        }
                    },
                    {
                        cmd: 'XGBoost avec classes d√©s√©quilibr√©es',
                        desc: 'scale_pos_weight pour d√©s√©quilibre',
                        details: {
                            explanation: 'scale_pos_weight compense le d√©s√©quilibre en pond√©rant la classe minoritaire.',
                            syntax: 'scale_pos_weight = sum(negative) / sum(positive)',
                            options: [
                                { flag: 'scale_pos_weight', desc: 'Ratio neg/pos' }
                            ],
                            examples: [
                                { code: `library(xgboost)
set.seed(42)

# Donn√©es binaires d√©s√©quilibr√©es
x <- as.matrix(mtcars[, c("mpg", "hp", "wt", "drat")])
y <- mtcars$am  # 19 automatic (0), 13 manual (1)

# Calculer le poids
scale_weight <- sum(y == 0) / sum(y == 1)

params <- list(
  objective = "binary:logistic",
  max_depth = 3,
  eta = 0.1,
  scale_pos_weight = scale_weight,
  eval_metric = "auc"  # AUC plut√¥t qu'accuracy
)

dtrain <- xgb.DMatrix(data = x, label = y)

# CV
cv_result <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 5,
  early_stopping_rounds = 10,
  verbose = FALSE
)

# Mod√®le final
xgb_model <- xgboost(
  data = dtrain,
  params = params,
  nrounds = cv_result$best_iteration,
  verbose = 0
)

# Pr√©diction de probabilit√©s
pred_prob <- predict(xgb_model, x)

# Trouver le seuil optimal (F1 ou autre)
library(pROC)
roc_obj <- roc(y, pred_prob)
plot(roc_obj, main = paste("AUC =", round(auc(roc_obj), 3)))

# Seuil optimal (Youden)
coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"))`, desc: 'Classes d√©s√©quilibr√©es' }
                            ],
                            tips: ['Utiliser AUC ou F1 comme m√©trique', 'Optimiser le seuil de classification'],
                            warnings: ['Ne pas se fier √† l\'accuracy avec des classes d√©s√©quilibr√©es']
                        }
                    }
                ]
            },
            {
                id: 'lca',
                title: 'üî¨ LCA - Classes Latentes',
                icon: 'fa-layer-group',
                color: 'border-l-4 border-pink-500',
                commands: [
                    {
                        cmd: 'LCA avec poLCA',
                        desc: 'Identifier des sous-groupes cach√©s',
                        details: {
                            explanation: 'Latent Class Analysis identifie des groupes non observ√©s √† partir de patterns de r√©ponses.',
                            syntax: 'poLCA(formula, data, nclass)',
                            options: [
                                { flag: 'nclass', desc: 'Nombre de classes latentes' },
                                { flag: 'maxiter', desc: 'It√©rations max (d√©faut 1000)' },
                                { flag: 'nrep', desc: 'R√©p√©titions avec starts diff√©rents' },
                                { flag: 'graphs', desc: 'Afficher les graphiques' }
                            ],
                            examples: [
                                { code: `library(poLCA)
set.seed(42)

# Exemple avec donn√©es simul√©es
# Les variables doivent √™tre des entiers 1, 2, 3...
data(values)  # Dataset inclus dans poLCA

# Formule: toutes les variables manifestes
f <- cbind(A, B, C, D) ~ 1

# LCA avec 2 classes
lca_2 <- poLCA(f, data = values, nclass = 2,
               maxiter = 1000, nrep = 10, verbose = FALSE)

# R√©sultats
print(lca_2)

# Probabilit√©s conditionnelles par classe
lca_2$probs

# Taille des classes
lca_2$P

# Classification des individus
lca_2$predclass

# Probabilit√©s d'appartenance
lca_2$posterior

# Comparer diff√©rents nombres de classes
results <- data.frame(
  nclass = 2:5,
  AIC = NA,
  BIC = NA,
  loglik = NA
)

for (i in 2:5) {
  lca <- poLCA(f, data = values, nclass = i,
               maxiter = 1000, nrep = 5, verbose = FALSE)
  results$AIC[i-1] <- lca$aic
  results$BIC[i-1] <- lca$bic
  results$loglik[i-1] <- lca$llik
}

print(results)  # Choisir le mod√®le avec BIC le plus bas`, desc: 'LCA basique' }
                            ],
                            tips: ['BIC est le crit√®re de choix pour le nombre de classes', 'nrep > 1 pour √©viter les optima locaux'],
                            warnings: ['Les variables doivent √™tre cod√©es 1, 2, 3... (pas 0)']
                        }
                    },
                    {
                        cmd: 'Choisir le nombre de classes',
                        desc: 'Comparer AIC, BIC, entropie',
                        details: {
                            explanation: 'Plusieurs crit√®res aident √† d√©terminer le nombre optimal de classes latentes.',
                            syntax: 'Comparer AIC, BIC, entropy pour k = 2, 3, 4...',
                            options: [
                                { flag: 'AIC', desc: 'Akaike Information Criterion' },
                                { flag: 'BIC', desc: 'Bayesian IC (p√©nalise plus)' },
                                { flag: 'Entropy', desc: 'Qualit√© de la classification (> 0.8)' },
                                { flag: 'LMR-LRT', desc: 'Test de vraisemblance' }
                            ],
                            examples: [
                                { code: `library(poLCA)
set.seed(42)

data(values)
f <- cbind(A, B, C, D) ~ 1

# Fonction pour comparer les mod√®les
compare_lca <- function(data, formula, max_class = 6, nrep = 10) {
  results <- data.frame()

  for (k in 2:max_class) {
    lca <- poLCA(formula, data, nclass = k,
                 maxiter = 1000, nrep = nrep, verbose = FALSE)

    # Calculer l'entropie
    entropy <- -sum(lca$posterior * log(lca$posterior + 1e-10)) / nrow(data)
    rel_entropy <- 1 - (entropy / log(k))

    results <- rbind(results, data.frame(
      nclass = k,
      AIC = lca$aic,
      BIC = lca$bic,
      loglik = lca$llik,
      entropy = rel_entropy,
      n_params = lca$npar
    ))
  }
  return(results)
}

comparison <- compare_lca(values, f, max_class = 5)
print(comparison)

# Visualiser
library(ggplot2)
comparison %>%
  pivot_longer(c(AIC, BIC)) %>%
  ggplot(aes(x = nclass, y = value, color = name)) +
  geom_line() +
  geom_point() +
  labs(title = "Crit√®res de s√©lection LCA",
       x = "Nombre de classes", y = "Valeur")`, desc: 'S√©lection du nombre de classes' }
                            ],
                            tips: ['BIC est g√©n√©ralement le meilleur crit√®re', 'Entropie > 0.8 = bonne classification'],
                            warnings: ['Tester plusieurs valeurs de nrep pour la stabilit√©']
                        }
                    },
                    {
                        cmd: 'LCA avec covariables',
                        desc: 'Pr√©dire l\'appartenance aux classes',
                        details: {
                            explanation: 'Les covariables pr√©disent la probabilit√© d\'appartenir √† chaque classe latente.',
                            syntax: 'poLCA(manifest ~ covariate, data, nclass)',
                            options: [
                                { flag: '~ covariate', desc: 'Ajouter des pr√©dicteurs d\'appartenance' }
                            ],
                            examples: [
                                { code: `library(poLCA)
set.seed(42)

# Cr√©er des donn√©es avec covariable
data(values)
values$age <- sample(1:3, nrow(values), replace = TRUE)

# LCA avec covariable
f_cov <- cbind(A, B, C, D) ~ age

lca_cov <- poLCA(f_cov, data = values, nclass = 3,
                 maxiter = 1000, nrep = 10, verbose = FALSE)

# Coefficients de r√©gression multinomiale
lca_cov$coeff

# Odds ratios
exp(lca_cov$coeff)

# Effet de la covariable sur l'appartenance aux classes
# R√©f√©rence = derni√®re classe
print(lca_cov)`, desc: 'LCA avec covariables' }
                            ],
                            tips: ['Les covariables pr√©disent l\'appartenance, pas les r√©ponses', 'La derni√®re classe est la r√©f√©rence'],
                            warnings: ['Trop de covariables peut rendre le mod√®le instable']
                        }
                    },
                    {
                        cmd: 'Gaussian Mixture avec mclust',
                        desc: 'LCA pour variables continues',
                        details: {
                            explanation: 'mclust fait du clustering bas√© sur des mod√®les de m√©lange gaussien.',
                            syntax: 'Mclust(data, G = 1:9)',
                            options: [
                                { flag: 'G', desc: 'Nombre de composantes √† tester' },
                                { flag: 'modelNames', desc: 'Types de mod√®les de covariance' }
                            ],
                            examples: [
                                { code: `library(mclust)
set.seed(42)

# Donn√©es continues
data <- mtcars[, c("mpg", "hp", "wt")]

# Mclust s√©lectionne automatiquement le nombre de clusters
mc <- Mclust(data, G = 1:9)

# R√©sum√©
summary(mc)

# Nombre optimal de clusters
mc$G

# BIC par mod√®le et nombre de clusters
plot(mc, what = "BIC")

# Classification
mc$classification

# Probabilit√©s d'appartenance
mc$z

# Visualisation
plot(mc, what = "classification")
plot(mc, what = "uncertainty")

# Ajouter au dataset
mtcars$cluster <- mc$classification

# LPA avec tidyLPA (interface tidy)
# install.packages("tidyLPA")
library(tidyLPA)

lpa_result <- data %>%
  estimate_profiles(1:5) %>%
  compare_solutions(statistics = c("AIC", "BIC", "Entropy"))

print(lpa_result)`, desc: 'Gaussian Mixture / LPA' }
                            ],
                            tips: ['mclust teste diff√©rents mod√®les de covariance', 'tidyLPA offre une interface plus simple'],
                            warnings: ['N√©cessite suffisamment d\'observations']
                        }
                    }
                ]
            },
            {
                id: 'evaluation',
                title: 'üéØ √âvaluation & G√©n√©ralisation',
                icon: 'fa-bullseye',
                color: 'border-l-4 border-red-500',
                commands: [
                    {
                        cmd: 'Validation crois√©e K-fold',
                        desc: 'Estimer la performance sur donn√©es nouvelles',
                        details: {
                            explanation: 'La CV divise les donn√©es en K parties, entra√Æne sur K-1 et teste sur la partie restante.',
                            syntax: 'trainControl(method = "cv", number = 10)',
                            options: [
                                { flag: 'cv', desc: 'K-fold cross-validation' },
                                { flag: 'repeatedcv', desc: 'CV r√©p√©t√©e plusieurs fois' },
                                { flag: 'LOOCV', desc: 'Leave-one-out (K = n)' },
                                { flag: 'boot', desc: 'Bootstrap' }
                            ],
                            examples: [
                                { code: `library(caret)
set.seed(42)

# Configuration CV
ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,          # 10 folds
  repeats = 3,          # R√©p√©ter 3 fois
  savePredictions = "final",
  classProbs = TRUE,    # Pour classification
  summaryFunction = twoClassSummary  # AUC, Sens, Spec
)

# Entra√Ænement avec CV
mtcars$am_f <- factor(mtcars$am, labels = c("auto", "manual"))

model_cv <- train(
  am_f ~ mpg + hp + wt,
  data = mtcars,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)

# R√©sultats CV
print(model_cv)
model_cv$results

# Pr√©dictions CV (out-of-fold)
head(model_cv$pred)

# Matrice de confusion sur pr√©dictions CV
confusionMatrix(model_cv$pred$pred, model_cv$pred$obs)`, desc: 'Cross-validation avec caret' }
                            ],
                            tips: ['10-fold CV est un bon compromis biais-variance', 'repeatedcv r√©duit la variance de l\'estimation'],
                            warnings: ['LOOCV peut √™tre tr√®s lent sur gros datasets']
                        }
                    },
                    {
                        cmd: 'M√©triques de classification',
                        desc: 'Accuracy, Precision, Recall, F1, AUC',
                        details: {
                            explanation: 'Diff√©rentes m√©triques capturent diff√©rents aspects de la performance.',
                            syntax: 'confusionMatrix(pred, actual)',
                            options: [
                                { flag: 'Accuracy', desc: '(TP+TN)/Total' },
                                { flag: 'Sensitivity (Recall)', desc: 'TP/(TP+FN)' },
                                { flag: 'Specificity', desc: 'TN/(TN+FP)' },
                                { flag: 'Precision (PPV)', desc: 'TP/(TP+FP)' },
                                { flag: 'F1', desc: '2*(Prec*Rec)/(Prec+Rec)' },
                                { flag: 'AUC', desc: 'Aire sous la courbe ROC' }
                            ],
                            examples: [
                                { code: `library(caret)
library(pROC)

# Pr√©dictions
pred_class <- factor(c(1,1,0,1,0,0,1,0))
pred_prob <- c(0.9, 0.8, 0.3, 0.7, 0.4, 0.2, 0.6, 0.35)
actual <- factor(c(1,1,0,0,0,0,1,1))

# Matrice de confusion
cm <- confusionMatrix(pred_class, actual, positive = "1")
print(cm)

# Extraire les m√©triques
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]  # Recall
cm$byClass["Specificity"]
cm$byClass["Precision"]    # PPV
cm$byClass["F1"]

# Courbe ROC et AUC
roc_obj <- roc(actual, pred_prob)
plot(roc_obj, main = paste("AUC =", round(auc(roc_obj), 3)))

# Seuil optimal (Youden)
best_thresh <- coords(roc_obj, "best", ret = c("threshold", "sens", "spec"))
print(best_thresh)

# Comparaison de mod√®les avec courbes ROC
# roc_list <- list(model1 = roc_obj1, model2 = roc_obj2)
# ggroc(roc_list)`, desc: 'M√©triques de classification' }
                            ],
                            tips: ['F1 est utile pour classes d√©s√©quilibr√©es', 'AUC > 0.8 = bonne discrimination'],
                            warnings: ['Accuracy trompeuse si classes d√©s√©quilibr√©es']
                        }
                    },
                    {
                        cmd: 'M√©triques de r√©gression',
                        desc: 'RMSE, MAE, R¬≤',
                        details: {
                            explanation: 'Mesurer l\'erreur de pr√©diction pour des mod√®les de r√©gression.',
                            syntax: 'RMSE = sqrt(mean((y - pred)^2))',
                            options: [
                                { flag: 'RMSE', desc: 'Root Mean Square Error' },
                                { flag: 'MAE', desc: 'Mean Absolute Error' },
                                { flag: 'R¬≤', desc: 'Variance expliqu√©e' },
                                { flag: 'MAPE', desc: 'Mean Absolute Percentage Error' }
                            ],
                            examples: [
                                { code: `library(caret)

# Pr√©dictions
y_actual <- c(3.5, 2.1, 4.8, 3.2, 5.1)
y_pred <- c(3.2, 2.5, 4.5, 3.0, 4.8)

# RMSE
rmse <- sqrt(mean((y_actual - y_pred)^2))
print(paste("RMSE:", round(rmse, 3)))

# MAE
mae <- mean(abs(y_actual - y_pred))
print(paste("MAE:", round(mae, 3)))

# R¬≤ (corr√©lation au carr√©)
r2 <- cor(y_actual, y_pred)^2
print(paste("R¬≤:", round(r2, 3)))

# R¬≤ ajust√© (pour mod√®les)
n <- length(y_actual)
p <- 3  # nombre de pr√©dicteurs
r2_adj <- 1 - (1 - r2) * (n - 1) / (n - p - 1)

# MAPE (attention aux valeurs proches de 0)
mape <- mean(abs((y_actual - y_pred) / y_actual)) * 100
print(paste("MAPE:", round(mape, 1), "%"))

# Avec caret
postResample(pred = y_pred, obs = y_actual)

# Visualiser
library(ggplot2)
data.frame(actual = y_actual, pred = y_pred) %>%
  ggplot(aes(x = actual, y = pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Actual vs Predicted")`, desc: 'M√©triques de r√©gression' }
                            ],
                            tips: ['MAE est plus robuste aux outliers que RMSE', 'R¬≤ peut √™tre trompeur avec peu de donn√©es'],
                            warnings: ['MAPE ind√©fini si y contient des z√©ros']
                        }
                    },
                    {
                        cmd: 'Comparer plusieurs mod√®les',
                        desc: 'S√©lectionner le meilleur mod√®le',
                        details: {
                            explanation: 'Comparer objectivement la performance de diff√©rents algorithmes.',
                            syntax: 'resamples() pour comparer des mod√®les caret',
                            options: [
                                { flag: 'resamples()', desc: 'Comparer les CV de plusieurs mod√®les' },
                                { flag: 'diff()', desc: 'Tester la significativit√© des diff√©rences' }
                            ],
                            examples: [
                                { code: `library(caret)
set.seed(42)

# M√™me configuration CV pour tous les mod√®les
ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  savePredictions = "final"
)

# Entra√Æner plusieurs mod√®les
model_lm <- train(mpg ~ ., data = mtcars, method = "lm", trControl = ctrl)
model_rf <- train(mpg ~ ., data = mtcars, method = "rf", trControl = ctrl)
model_xgb <- train(mpg ~ ., data = mtcars, method = "xgbTree", trControl = ctrl)
model_glmnet <- train(mpg ~ ., data = mtcars, method = "glmnet", trControl = ctrl)

# Comparer
results <- resamples(list(
  LM = model_lm,
  RF = model_rf,
  XGB = model_xgb,
  GLMNET = model_glmnet
))

# R√©sum√©
summary(results)

# Visualisations
bwplot(results)
dotplot(results)

# Test statistique des diff√©rences
diff_results <- diff(results)
summary(diff_results)

# S√©lectionner le meilleur
best_model <- model_rf  # Celui avec le meilleur RMSE CV

# Performance finale
print(best_model)`, desc: 'Comparaison de mod√®les' }
                            ],
                            tips: ['Utiliser la m√™me CV pour tous les mod√®les', 'diff() teste si les diff√©rences sont significatives'],
                            warnings: ['Plus de mod√®les = plus de chances de "gagner" par hasard']
                        }
                    },
                    {
                        cmd: 'Train/Test split et holdout',
                        desc: '√âvaluation finale sur donn√©es non vues',
                        details: {
                            explanation: 'R√©server un jeu de test non touch√© pendant le d√©veloppement pour l\'√©valuation finale.',
                            syntax: 'createDataPartition(y, p = 0.8)',
                            options: [
                                { flag: 'p', desc: 'Proportion pour l\'entra√Ænement' },
                                { flag: 'list', desc: 'FALSE retourne un vecteur d\'indices' }
                            ],
                            examples: [
                                { code: `library(caret)
set.seed(42)

# Split stratifi√© (conserve les proportions)
train_idx <- createDataPartition(
  mtcars$mpg,
  p = 0.8,
  list = FALSE
)

train_data <- mtcars[train_idx, ]
test_data <- mtcars[-train_idx, ]

# V√©rifier les proportions
nrow(train_data)
nrow(test_data)

# Entra√Æner sur train avec CV
ctrl <- trainControl(method = "cv", number = 10)

model <- train(
  mpg ~ hp + wt + drat,
  data = train_data,
  method = "rf",
  trControl = ctrl
)

# Performance CV (d√©veloppement)
print(model)

# √âvaluation finale sur TEST (jamais vu)
pred_test <- predict(model, test_data)
postResample(pred_test, test_data$mpg)

# Workflow complet recommand√©:
# 1. Split train/test (80/20)
# 2. D√©velopper avec CV sur train uniquement
# 3. Tuning hyperparam√®tres avec CV sur train
# 4. √âvaluation finale UNE SEULE FOIS sur test`, desc: 'Train/Test split' }
                            ],
                            tips: ['Ne JAMAIS toucher au test set pendant le d√©veloppement', 'createDataPartition fait un split stratifi√©'],
                            warnings: ['√âvaluer sur le test set une seule fois √† la fin']
                        }
                    },
                    {
                        cmd: 'Nested Cross-Validation',
                        desc: 'Estimation non biais√©e avec tuning inclus',
                        details: {
                            explanation: 'La nested CV utilise une boucle externe pour estimer la performance et une boucle interne pour le tuning. √âvite l\'optimisme biais.',
                            syntax: 'Boucle externe (√©valuation) + Boucle interne (tuning)',
                            options: [
                                { flag: 'Outer loop', desc: 'K-fold pour estimation de performance' },
                                { flag: 'Inner loop', desc: 'K-fold pour s√©lection d\'hyperparam√®tres' }
                            ],
                            examples: [
                                { code: `library(caret)
library(foreach)
library(doParallel)
set.seed(42)

# Parall√©lisation
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# Donn√©es
data <- mtcars
data$am_f <- factor(data$am, labels = c("auto", "manual"))

# === NESTED CV MANUELLE ===
outer_folds <- createFolds(data$am_f, k = 5, returnTrain = TRUE)

nested_results <- foreach(i = 1:5, .combine = rbind,
                          .packages = c("caret")) %dopar% {
  # Donn√©es outer fold
  train_outer <- data[outer_folds[[i]], ]
  test_outer <- data[-outer_folds[[i]], ]

  # Inner CV pour tuning
  inner_ctrl <- trainControl(
    method = "cv",
    number = 5,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )

  # Tuning dans inner loop
  model <- train(
    am_f ~ mpg + hp + wt,
    data = train_outer,
    method = "rf",
    trControl = inner_ctrl,
    tuneGrid = expand.grid(mtry = 1:3),
    metric = "ROC"
  )

  # √âvaluation sur outer fold (non vu pendant tuning)
  pred <- predict(model, test_outer)
  pred_prob <- predict(model, test_outer, type = "prob")

  # M√©triques
  cm <- confusionMatrix(pred, test_outer$am_f)

  data.frame(
    fold = i,
    accuracy = cm$overall["Accuracy"],
    best_mtry = model$bestTune$mtry
  )
}

stopCluster(cl)

# R√©sultats nested CV
print(nested_results)
mean(nested_results$accuracy)  # Estimation non biais√©e`, desc: 'Nested CV manuelle' }
                            ],
                            tips: ['Nested CV donne une estimation honn√™te m√™me avec tuning', 'Outer = 5-10 folds, Inner = 5 folds'],
                            warnings: ['Tr√®s co√ªteux en calcul (K_outer √ó K_inner √ó n_hyperparams)']
                        }
                    },
                    {
                        cmd: 'Tuning Bay√©sien des hyperparam√®tres',
                        desc: 'Optimisation intelligente avec moins d\'essais',
                        details: {
                            explanation: 'Le tuning bay√©sien utilise un mod√®le probabiliste pour choisir intelligemment les prochains hyperparam√®tres √† tester.',
                            syntax: 'ParBayesianOptimization() ou mlr3mbo',
                            options: [
                                { flag: 'ParBayesianOptimization', desc: 'Package simple pour Bayes opt' },
                                { flag: 'mlr3mbo', desc: 'Framework avanc√©' },
                                { flag: 'Expected Improvement', desc: 'Crit√®re d\'acquisition' }
                            ],
                            examples: [
                                { code: `# install.packages("ParBayesianOptimization")
library(ParBayesianOptimization)
library(xgboost)
library(caret)
set.seed(42)

# Pr√©parer les donn√©es
x <- as.matrix(mtcars[, c("hp", "wt", "drat", "qsec")])
y <- mtcars$mpg

# Fonction objectif √† maximiser (retourne score)
xgb_cv_bayes <- function(max_depth, eta, subsample, colsample_bytree) {
  dtrain <- xgb.DMatrix(data = x, label = y)

  params <- list(
    objective = "reg:squarederror",
    max_depth = max_depth,
    eta = eta,
    subsample = subsample,
    colsample_bytree = colsample_bytree
  )

  cv <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 100,
    nfold = 5,
    early_stopping_rounds = 10,
    verbose = 0
  )

  # Retourner -RMSE (on veut maximiser)
  return(list(Score = -min(cv$evaluation_log$test_rmse_mean),
              nrounds = cv$best_iteration))
}

# Bornes des hyperparam√®tres
bounds <- list(
  max_depth = c(2L, 8L),
  eta = c(0.01, 0.3),
  subsample = c(0.5, 1),
  colsample_bytree = c(0.5, 1)
)

# Optimisation bay√©sienne
bayes_result <- bayesOpt(
  FUN = xgb_cv_bayes,
  bounds = bounds,
  initPoints = 5,     # Points initiaux al√©atoires
  iters.n = 20,       # It√©rations bay√©siennes
  iters.k = 1,        # Points par it√©ration
  acq = "ei",         # Expected Improvement
  verbose = TRUE
)

# Meilleurs hyperparam√®tres
getBestPars(bayes_result)

# Historique
bayes_result$scoreSummary`, desc: 'Tuning Bay√©sien avec XGBoost' }
                            ],
                            tips: ['20-50 it√©rations suffisent souvent vs 1000+ pour grid search', 'initPoints = 5-10 pour exploration initiale'],
                            warnings: ['Fixer le seed pour reproductibilit√©']
                        }
                    }
                ]
            },
            {
                id: 'workflows',
                title: 'üîÑ Workflows ML Complets',
                icon: 'fa-diagram-project',
                color: 'border-l-4 border-indigo-500',
                commands: [
                    {
                        cmd: 'Pipeline Classification Complet',
                        desc: 'De la pr√©paration √† l\'√©valuation finale',
                        details: {
                            explanation: 'Workflow complet pour un projet de classification : EDA, pr√©traitement, mod√©lisation, √©valuation.',
                            syntax: '√âtapes: EDA ‚Üí Pr√©traitement ‚Üí Split ‚Üí Mod√©lisation ‚Üí √âvaluation',
                            options: [
                                { flag: '1. EDA', desc: 'Explorer et comprendre les donn√©es' },
                                { flag: '2. Pr√©traitement', desc: 'Nettoyage, encodage, scaling' },
                                { flag: '3. Split', desc: 'Train/Test stratifi√©' },
                                { flag: '4. Mod√©lisation', desc: 'CV + Tuning sur train' },
                                { flag: '5. √âvaluation', desc: 'Test final une seule fois' }
                            ],
                            examples: [
                                { code: `# ============================================
# PIPELINE CLASSIFICATION COMPLET
# ============================================
library(tidyverse)
library(caret)
library(pROC)
set.seed(42)

# === 1. CHARGEMENT ET EDA ===
data <- iris
data$Species <- factor(ifelse(data$Species == "setosa", "setosa", "other"))

# R√©sum√©
summary(data)
table(data$Species)  # V√©rifier √©quilibre des classes

# Visualisation
data %>%
  pivot_longer(-Species) %>%
  ggplot(aes(x = value, fill = Species)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~name, scales = "free")

# === 2. PR√âTRAITEMENT ===
# Cr√©er recipe (si besoin)
# preProcess peut normaliser, imputer, etc.

# === 3. SPLIT TRAIN/TEST (80/20) ===
train_idx <- createDataPartition(data$Species, p = 0.8, list = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

cat("Train:", nrow(train_data), "| Test:", nrow(test_data), "\\n")
cat("Train balance:", prop.table(table(train_data$Species)), "\\n")

# === 4. MOD√âLISATION AVEC CV ===
ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Plusieurs mod√®les
model_glm <- train(Species ~ ., data = train_data, method = "glm",
                   family = "binomial", trControl = ctrl, metric = "ROC")

model_rf <- train(Species ~ ., data = train_data, method = "rf",
                  trControl = ctrl, metric = "ROC",
                  tuneGrid = expand.grid(mtry = 1:4))

model_xgb <- train(Species ~ ., data = train_data, method = "xgbTree",
                   trControl = ctrl, metric = "ROC")

# Comparer
results <- resamples(list(GLM = model_glm, RF = model_rf, XGB = model_xgb))
summary(results)
dotplot(results, metric = "ROC")

# === 5. √âVALUATION FINALE SUR TEST ===
best_model <- model_rf  # Choisir le meilleur

# Pr√©dictions
pred_class <- predict(best_model, test_data)
pred_prob <- predict(best_model, test_data, type = "prob")

# Matrice de confusion
cm <- confusionMatrix(pred_class, test_data$Species, positive = "setosa")
print(cm)

# Courbe ROC
roc_obj <- roc(test_data$Species, pred_prob[, "setosa"])
plot(roc_obj, main = paste("Test AUC =", round(auc(roc_obj), 3)))

cat("\\n=== R√âSULTATS FINAUX ===\\n")
cat("Accuracy:", round(cm$overall["Accuracy"], 3), "\\n")
cat("Sensitivity:", round(cm$byClass["Sensitivity"], 3), "\\n")
cat("Specificity:", round(cm$byClass["Specificity"], 3), "\\n")
cat("AUC:", round(auc(roc_obj), 3), "\\n")`, desc: 'Pipeline classification complet' }
                            ],
                            tips: ['Toujours s√©parer test avant toute exploration', 'Utiliser la m√™me m√©trique pour comparer'],
                            warnings: ['Ne jamais toucher au test set pendant le d√©veloppement']
                        }
                    },
                    {
                        cmd: 'Pipeline R√©gression Complet',
                        desc: 'Workflow complet pour pr√©diction continue',
                        details: {
                            explanation: 'Pipeline de r√©gression avec pr√©traitement, feature engineering, et comparaison de mod√®les.',
                            syntax: 'EDA ‚Üí Feature Engineering ‚Üí Modeling ‚Üí Evaluation',
                            options: [
                                { flag: 'preProcess', desc: 'center, scale, YeoJohnson, pca' },
                                { flag: 'Feature selection', desc: 'RFE, Boruta, LASSO' }
                            ],
                            examples: [
                                { code: `# ============================================
# PIPELINE R√âGRESSION COMPLET
# ============================================
library(tidyverse)
library(caret)
set.seed(42)

# === 1. DONN√âES ===
data <- mtcars

# === 2. EDA ===
# Corr√©lations avec la cible
cor(data)[, "mpg"] %>% sort(decreasing = TRUE)

# Distribution de la cible
hist(data$mpg, main = "Distribution MPG", breaks = 15)

# === 3. SPLIT ===
train_idx <- createDataPartition(data$mpg, p = 0.8, list = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

# === 4. PR√âTRAITEMENT ===
# D√©finir le pr√©traitement sur TRAIN seulement
preProc <- preProcess(train_data[, -1],
                      method = c("center", "scale", "YeoJohnson"))

train_processed <- predict(preProc, train_data)
test_processed <- predict(preProc, test_data)

# === 5. FEATURE SELECTION (optionnel) ===
# Recursive Feature Elimination
rfe_ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfe_result <- rfe(train_data[, -1], train_data$mpg,
                  sizes = c(2, 4, 6, 8),
                  rfeControl = rfe_ctrl)
print(rfe_result)
predictors(rfe_result)  # Meilleures features

# === 6. MOD√âLISATION ===
ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  savePredictions = "final"
)

# Plusieurs mod√®les
model_lm <- train(mpg ~ ., data = train_processed, method = "lm",
                  trControl = ctrl)

model_glmnet <- train(mpg ~ ., data = train_processed, method = "glmnet",
                      trControl = ctrl,
                      tuneLength = 10)

model_rf <- train(mpg ~ ., data = train_processed, method = "rf",
                  trControl = ctrl,
                  tuneGrid = expand.grid(mtry = 2:6))

model_xgb <- train(mpg ~ ., data = train_processed, method = "xgbTree",
                   trControl = ctrl)

# === 7. COMPARAISON ===
results <- resamples(list(
  LM = model_lm,
  GLMNET = model_glmnet,
  RF = model_rf,
  XGB = model_xgb
))

summary(results)
dotplot(results, metric = "RMSE")

# Test diff√©rences significatives
diff_results <- diff(results)
summary(diff_results)

# === 8. √âVALUATION FINALE ===
best_model <- model_rf

pred_test <- predict(best_model, test_processed)
final_metrics <- postResample(pred_test, test_processed$mpg)

cat("\\n=== M√âTRIQUES TEST FINAL ===\\n")
print(final_metrics)

# Visualiser
data.frame(actual = test_processed$mpg, predicted = pred_test) %>%
  ggplot(aes(x = actual, y = predicted)) +
  geom_point(size = 3) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Predicted (Test Set)")`, desc: 'Pipeline r√©gression complet' }
                            ],
                            tips: ['Appliquer preProcess sur train, puis transformer test', 'RFE aide √† identifier les features importantes'],
                            warnings: ['Ne pas faire de feature selection sur tout le dataset']
                        }
                    },
                    {
                        cmd: 'Pipeline avec Pr√©traitement Avanc√©',
                        desc: 'recipes pour preprocessing reproductible',
                        details: {
                            explanation: 'Le package recipes permet de d√©finir des √©tapes de pr√©traitement de mani√®re d√©clarative et reproductible.',
                            syntax: 'recipe() %>% step_*() %>% prep() %>% bake()',
                            options: [
                                { flag: 'step_normalize()', desc: 'Centrer et r√©duire' },
                                { flag: 'step_dummy()', desc: 'One-hot encoding' },
                                { flag: 'step_impute_*()', desc: 'Imputation des NA' },
                                { flag: 'step_pca()', desc: 'R√©duction de dimension' }
                            ],
                            examples: [
                                { code: `library(tidyverse)
library(recipes)
library(caret)
set.seed(42)

# Donn√©es avec NA et cat√©gorielles
data <- data.frame(
  y = rnorm(100),
  x1 = rnorm(100),
  x2 = c(rnorm(95), rep(NA, 5)),  # Avec NA
  x3 = sample(c("A", "B", "C"), 100, replace = TRUE),
  x4 = rnorm(100)
)

# Split
train_idx <- createDataPartition(data$y, p = 0.8, list = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

# === RECIPE ===
rec <- recipe(y ~ ., data = train_data) %>%
  # Imputer les NA avec la m√©diane
  step_impute_median(all_numeric_predictors()) %>%
  # One-hot encoding des cat√©gorielles
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  # Normaliser
  step_normalize(all_numeric_predictors()) %>%
  # Supprimer les colonnes √† variance nulle
  step_zv(all_predictors()) %>%
  # Supprimer les colonnes corr√©l√©es > 0.9
  step_corr(all_numeric_predictors(), threshold = 0.9)

# Pr√©parer la recipe sur TRAIN
rec_prep <- prep(rec, training = train_data)

# Appliquer
train_baked <- bake(rec_prep, new_data = train_data)
test_baked <- bake(rec_prep, new_data = test_data)

# Voir les transformations
rec_prep

# === INT√âGRATION AVEC CARET ===
# Avec recipes dans trainControl
ctrl <- trainControl(method = "cv", number = 5)

model <- train(
  rec,  # Passer la recipe directement
  data = train_data,
  method = "glmnet",
  trControl = ctrl
)

print(model)

# Pr√©diction automatique avec preprocessing
pred <- predict(model, test_data)
postResample(pred, test_data$y)`, desc: 'Pr√©traitement avec recipes' }
                            ],
                            tips: ['recipes garantit que test est trait√© identiquement √† train', 'step_unknown() pour g√©rer nouvelles cat√©gories'],
                            warnings: ['Toujours prep() sur train seulement']
                        }
                    },
                    {
                        cmd: 'Pipeline avec tidymodels',
                        desc: 'Framework moderne unifi√©',
                        details: {
                            explanation: 'tidymodels est le successeur moderne de caret, avec une syntaxe tidy et des workflows.',
                            syntax: 'workflow() %>% add_recipe() %>% add_model() %>% fit()',
                            options: [
                                { flag: 'parsnip', desc: 'Sp√©cification de mod√®les' },
                                { flag: 'recipes', desc: 'Pr√©traitement' },
                                { flag: 'workflows', desc: 'Combiner recipe + model' },
                                { flag: 'tune', desc: 'Tuning avec CV' }
                            ],
                            examples: [
                                { code: `# install.packages("tidymodels")
library(tidymodels)
set.seed(42)

# === DONN√âES ===
data <- mtcars %>%
  mutate(cyl = factor(cyl))

# === SPLIT ===
split <- initial_split(data, prop = 0.8, strata = cyl)
train_data <- training(split)
test_data <- testing(split)

# === RECIPE ===
rec <- recipe(cyl ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# === MOD√àLE (sp√©cification) ===
rf_spec <- rand_forest(
  mtry = tune(),      # √Ä tuner
  trees = 500,
  min_n = tune()      # √Ä tuner
) %>%
  set_engine("ranger") %>%
  set_mode("classification")

# === WORKFLOW ===
wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_spec)

# === TUNING AVEC CV ===
cv_folds <- vfold_cv(train_data, v = 10, strata = cyl)

# Grille d'hyperparam√®tres
rf_grid <- grid_regular(
  mtry(range = c(2, 8)),
  min_n(range = c(2, 10)),
  levels = 5
)

# Tuning
tune_results <- tune_grid(
  wf,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = metric_set(accuracy, roc_auc)
)

# Meilleurs hyperparam√®tres
show_best(tune_results, metric = "accuracy")
autoplot(tune_results)

# Finaliser le workflow
best_params <- select_best(tune_results, metric = "accuracy")
final_wf <- finalize_workflow(wf, best_params)

# === √âVALUATION FINALE ===
final_fit <- last_fit(final_wf, split)

# M√©triques sur test
collect_metrics(final_fit)

# Pr√©dictions
collect_predictions(final_fit) %>%
  conf_mat(cyl, .pred_class)`, desc: 'Pipeline tidymodels complet' }
                            ],
                            tips: ['tidymodels est plus moderne et flexible que caret', 'last_fit() fait l\'√©valuation finale automatiquement'],
                            warnings: ['Syntaxe diff√©rente de caret']
                        }
                    },
                    {
                        cmd: 'Rapport de mod√©lisation automatique',
                        desc: 'G√©n√©rer un rapport complet avec performance',
                        details: {
                            explanation: 'Cr√©er un rapport structur√© avec toutes les m√©triques et visualisations.',
                            syntax: 'Fonction personnalis√©e de reporting',
                            options: [
                                { flag: 'M√©triques', desc: 'CV et Test metrics' },
                                { flag: 'Visualisations', desc: 'ROC, Importance, Residuals' },
                                { flag: 'Export', desc: 'HTML, PDF, CSV' }
                            ],
                            examples: [
                                { code: `library(tidyverse)
library(caret)
library(pROC)
library(knitr)

# === FONCTION DE RAPPORT ===
generate_model_report <- function(model, test_data, target_col,
                                  task = "classification") {

  cat("\\n", paste(rep("=", 50), collapse = ""), "\\n")
  cat("RAPPORT DE MOD√âLISATION\\n")
  cat(paste(rep("=", 50), collapse = ""), "\\n\\n")

  # Info mod√®le
  cat("Mod√®le:", model$method, "\\n")
  cat("Date:", format(Sys.time(), "%Y-%m-%d %H:%M"), "\\n\\n")

  # === PERFORMANCE CV ===
  cat("--- PERFORMANCE CV (TRAIN) ---\\n")
  print(model$results[which.min(model$results$RMSE), ])
  cat("\\n")

  if (!is.null(model$bestTune)) {
    cat("Meilleurs hyperparam√®tres:\\n")
    print(model$bestTune)
    cat("\\n")
  }

  # === √âVALUATION TEST ===
  cat("--- PERFORMANCE TEST ---\\n")
  pred <- predict(model, test_data)

  if (task == "classification") {
    # M√©triques classification
    cm <- confusionMatrix(pred, test_data[[target_col]])
    print(cm)

    # ROC si binaire
    if (length(levels(test_data[[target_col]])) == 2) {
      pred_prob <- predict(model, test_data, type = "prob")
      roc_obj <- roc(test_data[[target_col]], pred_prob[, 2])
      cat("\\nAUC:", round(auc(roc_obj), 4), "\\n")

      # Plot ROC
      plot(roc_obj, main = paste("ROC Curve - AUC:", round(auc(roc_obj), 3)))
    }

  } else {
    # M√©triques r√©gression
    metrics <- postResample(pred, test_data[[target_col]])
    cat("RMSE:", round(metrics["RMSE"], 4), "\\n")
    cat("R¬≤:", round(metrics["Rsquared"], 4), "\\n")
    cat("MAE:", round(metrics["MAE"], 4), "\\n")

    # Plot actual vs predicted
    plot(test_data[[target_col]], pred,
         main = "Actual vs Predicted",
         xlab = "Actual", ylab = "Predicted")
    abline(0, 1, col = "red")
  }

  # === IMPORTANCE ===
  cat("\\n--- IMPORTANCE DES VARIABLES ---\\n")
  imp <- varImp(model)
  print(imp)
  plot(imp, top = 10)

  cat("\\n", paste(rep("=", 50), collapse = ""), "\\n")
}

# === UTILISATION ===
# Entra√Æner un mod√®le
data <- iris
data$Species <- factor(ifelse(data$Species == "setosa", "setosa", "other"))
train_idx <- createDataPartition(data$Species, p = 0.8, list = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

ctrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)
model <- train(Species ~ ., data = train_data, method = "rf", trControl = ctrl)

# G√©n√©rer le rapport
generate_model_report(model, test_data, "Species", task = "classification")`, desc: 'Rapport de mod√©lisation' }
                            ],
                            tips: ['Automatiser les rapports pour reproductibilit√©', 'Exporter en RMarkdown pour PDF/HTML'],
                            warnings: ['Adapter la fonction selon vos besoins']
                        }
                    }
                ]
            },
            {
                id: 'explainable-ai',
                title: 'üîç Explainable AI',
                icon: 'fa-search',
                color: 'border-l-4 border-yellow-500',
                commands: [
                    {
                        cmd: 'Creer un explainer DALEX',
                        desc: 'explain(), model, data, y',
                        details: {
                            explanation: 'DALEX (Descriptive mAchine Learning EXplanations) est un framework unifie pour expliquer tout type de modele ML en R.',
                            syntax: 'explain(model, data = X, y = y, label = "Model Name")',
                            options: [
                                { flag: 'model', desc: 'Modele entraine (rf, xgb, glm, etc.)' },
                                { flag: 'data', desc: 'Donnees sans la cible' },
                                { flag: 'y', desc: 'Variable cible (vecteur)' },
                                { flag: 'label', desc: 'Nom pour les graphiques' }
                            ],
                            examples: [
                                { code: `# Installation
install.packages("DALEX")
library(DALEX)
library(randomForest)

# Donnees et modele
data(titanic_imputed, package = "DALEX")
df <- titanic_imputed

model_rf <- randomForest(survived ~ ., data = df, ntree = 100)

# Creer explainer DALEX
explainer_rf <- explain(
  model = model_rf,
  data = df[, -8],  # Sans la cible
  y = as.numeric(df$survived) - 1,  # 0/1
  label = "Random Forest",
  verbose = FALSE
)

# Afficher resume
explainer_rf`, desc: 'Creer explainer DALEX' }
                            ],
                            tips: ['DALEX fonctionne avec tous les modeles ML', 'explain() cree un objet reutilisable'],
                            warnings: ['La cible y doit etre numerique pour regression/binaire']
                        }
                    },
                    {
                        cmd: 'Analyser importance globale',
                        desc: 'model_parts(), feature_importance',
                        details: {
                            explanation: 'Calculer importance globale des features par permutation (model-agnostic).',
                            syntax: 'model_parts(explainer, type = "variable_importance")',
                            options: [
                                { flag: 'type', desc: '"variable_importance" ou "ratio"' },
                                { flag: 'N', desc: 'Nombre de permutations' },
                                { flag: 'B', desc: 'Nombre de repetitions' }
                            ],
                            examples: [
                                { code: `library(DALEX)

# Importance des variables par permutation
vi <- model_parts(explainer_rf)

# Visualiser
plot(vi)

# Importance avec intervalles de confiance
vi_detailed <- model_parts(
  explainer_rf,
  type = "variable_importance",
  B = 10  # Repetitions pour IC
)
plot(vi_detailed, show_boxplots = TRUE)

# Comparer plusieurs modeles
# explainer_glm <- explain(model_glm, ...)
# vi_comparison <- model_parts(explainer_rf, explainer_glm)
# plot(vi_comparison)`, desc: 'Importance globale' }
                            ],
                            tips: ['Permutation importance est model-agnostic', 'Comparer plusieurs modeles sur le meme plot'],
                            warnings: ['Peut etre lent sur grands datasets']
                        }
                    },
                    {
                        cmd: 'Expliquer une prediction locale',
                        desc: 'predict_parts(), break_down, shap',
                        details: {
                            explanation: 'Expliquer pourquoi le modele a fait une prediction specifique pour une observation.',
                            syntax: 'predict_parts(explainer, new_observation, type = "break_down")',
                            options: [
                                { flag: 'type = "break_down"', desc: 'Decomposition additive' },
                                { flag: 'type = "shap"', desc: 'Valeurs SHAP' },
                                { flag: 'type = "oscillations"', desc: 'Importance locale' }
                            ],
                            examples: [
                                { code: `library(DALEX)

# Observation a expliquer
new_obs <- df[1, ]

# Break-down (decomposition)
bd <- predict_parts(
  explainer_rf,
  new_observation = new_obs,
  type = "break_down"
)
plot(bd)

# Valeurs SHAP locales
shap <- predict_parts(
  explainer_rf,
  new_observation = new_obs,
  type = "shap",
  B = 25  # Nombre de permutations
)
plot(shap)

# Comparer Break-down vs SHAP
plot(bd, shap)

# Pour plusieurs observations
for (i in 1:3) {
  pp <- predict_parts(explainer_rf, df[i, ], type = "break_down")
  print(plot(pp) + ggtitle(paste("Observation", i)))
}`, desc: 'Explications locales' }
                            ],
                            tips: ['Break-down est deterministe, SHAP moyenne sur permutations', 'SHAP plus robuste mais plus lent'],
                            warnings: ['SHAP necessite B > 20 pour stabilite']
                        }
                    },
                    {
                        cmd: 'Profiler les features',
                        desc: 'model_profile(), partial_dependence',
                        details: {
                            explanation: 'Visualiser comment une feature affecte la prediction en moyenne (Partial Dependence Plots).',
                            syntax: 'model_profile(explainer, variables = "feature")',
                            options: [
                                { flag: 'type = "partial"', desc: 'PDP classique' },
                                { flag: 'type = "accumulated"', desc: 'ALE plot' },
                                { flag: 'groups', desc: 'PDP par groupe' }
                            ],
                            examples: [
                                { code: `library(DALEX)

# Partial Dependence Plot
pdp <- model_profile(
  explainer_rf,
  variables = c("age", "fare")
)
plot(pdp)

# Accumulated Local Effects (robuste aux correlations)
ale <- model_profile(
  explainer_rf,
  variables = "age",
  type = "accumulated"
)
plot(ale)

# PDP par groupe
pdp_groups <- model_profile(
  explainer_rf,
  variables = "age",
  groups = "gender"  # Separer par genre
)
plot(pdp_groups)

# Interaction 2D
pdp_2d <- model_profile(
  explainer_rf,
  variables = c("age", "fare"),
  type = "partial"
)`, desc: 'Profile des features' }
                            ],
                            tips: ['ALE plus robuste que PDP si features correlees', 'groups pour interactions simples'],
                            warnings: ['PDP assume independance des features']
                        }
                    },
                    {
                        cmd: 'Utiliser LIME pour explications',
                        desc: 'lime::explain(), lime::lime()',
                        details: {
                            explanation: 'LIME explique localement en approximant par un modele simple (lineaire) autour de observation.',
                            syntax: 'lime(training_data, model) puis explain(new_obs)',
                            options: [
                                { flag: 'n_features', desc: 'Nombre de features a afficher' },
                                { flag: 'n_permutations', desc: 'Nombre de perturbations' },
                                { flag: 'kernel_width', desc: 'Largeur du kernel' }
                            ],
                            examples: [
                                { code: `# Installation
install.packages("lime")
library(lime)
library(randomForest)

# Modele
model_rf <- randomForest(Species ~ ., data = iris[1:100, ])

# Creer explainer LIME
lime_explainer <- lime(
  iris[1:100, 1:4],  # Training data (features only)
  model_rf
)

# Expliquer nouvelles observations
new_obs <- iris[101:103, 1:4]

explanation <- explain(
  new_obs,
  lime_explainer,
  n_features = 4,
  n_permutations = 1000
)

# Visualiser
plot_features(explanation)

# Texte
plot_explanations(explanation)

# Pour un seul cas
explanation_single <- explain(
  iris[101, 1:4],
  lime_explainer,
  n_features = 4
)
plot_features(explanation_single)`, desc: 'LIME en R' }
                            ],
                            tips: ['LIME est model-agnostic', 'Interpreter les poids comme importance locale'],
                            warnings: ['Explications peuvent varier entre runs']
                        }
                    },
                    {
                        cmd: 'Calculer Shapley avec iml',
                        desc: 'iml::Shapley, iml::FeatureEffects',
                        details: {
                            explanation: 'Le package iml offre une interface unifiee pour interpretabilite incluant Shapley et effets des features.',
                            syntax: 'Shapley$new(predictor, x.interest)',
                            options: [
                                { flag: 'Shapley', desc: 'Valeurs Shapley locales' },
                                { flag: 'FeatureEffects', desc: 'PDP et ICE plots' },
                                { flag: 'FeatureImp', desc: 'Importance par permutation' }
                            ],
                            examples: [
                                { code: `# Installation
install.packages("iml")
library(iml)
library(randomForest)

# Modele
model_rf <- randomForest(mpg ~ ., data = mtcars)

# Creer Predictor wrapper
predictor <- Predictor$new(
  model_rf,
  data = mtcars[, -1],
  y = mtcars$mpg
)

# Importance globale
imp <- FeatureImp$new(predictor, loss = "mse")
plot(imp)

# Shapley pour une observation
shapley <- Shapley$new(
  predictor,
  x.interest = mtcars[1, -1]
)
plot(shapley)

# Feature Effects (PDP + ICE)
effects <- FeatureEffects$new(predictor, features = c("hp", "wt"))
plot(effects)

# ICE plots individuels
ice <- FeatureEffect$new(predictor, feature = "hp", method = "ice")
plot(ice)`, desc: 'iml pour XAI' }
                            ],
                            tips: ['iml integre plusieurs methodes XAI', 'FeatureImp = permutation importance'],
                            warnings: ['Shapley peut etre lent (exponentiel en features)']
                        }
                    },
                    {
                        cmd: 'Visualiser avec flashlight',
                        desc: 'flashlight(), light_importance()',
                        details: {
                            explanation: 'flashlight offre des outils XAI model-agnostic avec une API simple et coherente.',
                            syntax: 'flashlight(model, data, y, predict_function)',
                            options: [
                                { flag: 'light_importance', desc: 'Importance des features' },
                                { flag: 'light_ice', desc: 'ICE plots' },
                                { flag: 'light_breakdown', desc: 'SHAP-like breakdown' }
                            ],
                            examples: [
                                { code: `# Installation
install.packages("flashlight")
library(flashlight)
library(randomForest)

# Modele
model_rf <- randomForest(mpg ~ ., data = mtcars)

# Creer flashlight
fl <- flashlight(
  model = model_rf,
  data = mtcars,
  y = "mpg",
  label = "Random Forest",
  predict_function = function(m, X) predict(m, X)
)

# Importance permutation
imp <- light_importance(fl, m_repetitions = 10)
plot(imp)

# ICE et PDP
ice <- light_ice(fl, v = "hp")
plot(ice)

# Effets avec interaction
effects <- light_effects(fl, v = "hp", by = "am")
plot(effects)

# Breakdown pour observation
bd <- light_breakdown(fl, new_obs = mtcars[1, ])
plot(bd)

# Comparer modeles
# fl_glm <- flashlight(model_glm, ...)
# fls <- multiflashlight(list(fl, fl_glm))
# plot(light_importance(fls))`, desc: 'flashlight XAI' }
                            ],
                            tips: ['flashlight ideal pour comparaison de modeles', 'API coherente pour toutes les methodes'],
                            warnings: ['Definir predict_function pour modeles personnalises']
                        }
                    }
                ]
            }
        ];
    </script>
    <script src="../js/cheatsheet.js"></script>
</body>
</html>
